{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd41e9a8",
   "metadata": {},
   "source": [
    "<font size=6>Downloading required libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q numpy\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip3 install torch torchsummary\n",
    "!pip3 install -q pretty_midi\n",
    "!pip3 install -q gensim\n",
    "!pip3 install -q nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4068e6c",
   "metadata": {},
   "source": [
    "<font size=6>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4174b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:\n",
    "# 1. Add <unknown> and change eol to <eol>\n",
    "# 2. Modify the model since it's basically copy pasted which isn't ideal.\n",
    "# 3. Modify the training for the same reason.\n",
    "# 4. Add a way to make sure that eol isn't selected as often.\n",
    "# 5. When generating lyrics, make sure the generation is logical, e.g. start with a word, build context window and move it when reaching token limit.\n",
    "# 6. Make the one-hot encoding make more sense, meaning that a an example gets a single number instead of a 225 dim vector. DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import gdown\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "import csv\n",
    "import string\n",
    "from pretty_midi import PrettyMIDI, Note\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359ed8",
   "metadata": {},
   "source": [
    "<font size=6>Constants</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60457569",
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRIC_TRAIN_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_train_set.csv')\n",
    "LYRIC_TEST_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_test_set.csv')\n",
    "MIDI_FILE_PATH: str = os.path.join(os.getcwd(), 'data', 'midi_files')\n",
    "PICKLING_PATH: str = os.path.join(os.getcwd(), 'loaded_midi_files.pkl') # Path to save/load pickled MIDI files, for faster loading.\n",
    "EPSILON: float = 1e-9\n",
    "SEQUENCE_LENGTH: int = 10  # Number of words in the input sequence\n",
    "BATCH_SIZE: int = 128\n",
    "LSTM_LAYERS: int = 2\n",
    "DROPOUT: float = 0.3\n",
    "RANDOM_LOADER_SEED: int = 42\n",
    "VALIDATION_SPLIT: float = 0.1\n",
    "LEARNING_RATE: float = 0.001\n",
    "MAX_EPOCHS: int = 50\n",
    "NUMBER_OF_EXTRACT_MIDI_FEATURES: int = 33\n",
    "PATIANCE_FACTOR: float = 0.001\n",
    "PATIANCE_EPOCHS: int = 10\n",
    "UNK_ID: int = 0\n",
    "MIN_LINE_LENGTH: int = 5\n",
    "MAX_LINE_LENGTH: int = SEQUENCE_LENGTH\n",
    "SONG_STRUCTURE_PENALTY: float = 10.0\n",
    "EOL_STRING: str = '<eol>'\n",
    "UNK_STRING: str = '<unk>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1947ce",
   "metadata": {},
   "source": [
    "<font size=6>Midi Feature extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083e542",
   "metadata": {},
   "source": [
    "Auxlliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_time_signature(changes: tuple[int, int]) -> tuple[int, int]:\n",
    "    if not changes: return (4, 4)\n",
    "    pairs: list[tuple[int, int]] = [(ts.numerator, ts.denominator) for ts in changes]\n",
    "    return Counter(pairs).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2526597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_weighted_pitch_stats(notes: list[Note]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute pitch statistics weighted by each note's duration.\n",
    "    Returns a dict with:\n",
    "      mean (duration-weighted),\n",
    "      std  (duration-weighted),\n",
    "      p10 / p50 / p90 (duration-weighted percentiles),\n",
    "      ambitus = p90 - p10 (robust range).\n",
    "    If `notes` is empty, returns safe defaults.\n",
    "    \"\"\"\n",
    "    # Empty guard: nothing to measure → return neutral stats.\n",
    "    if not notes:\n",
    "        return dict(mean=0.0, std=0.0, p10=-1, p50=-1, p90=-1, ambitus=0.0)\n",
    "\n",
    "    # Vectorize pitches as float for math (MIDI 0..127, but floats simplify ops).\n",
    "    pitches: np.ndarray[np.float32] = np.fromiter((n.pitch for n in notes), dtype=np.float32)\n",
    "\n",
    "    # Each note's weight = its duration in seconds; clamp tiny/negative to epsilon.\n",
    "    weights: np.ndarray[np.float32] = np.fromiter((max(EPSILON, n.end - n.start) for n in notes), dtype=np.float32)\n",
    "    total_weights: float = weights.sum()\n",
    "    duration_weight_mean: float = float((weights * pitches).sum() / total_weights)\n",
    "    duration_weighted_variance: float = float((weights * (pitches - duration_weight_mean) ** 2).sum() / total_weights)\n",
    "    weighted_std: float = duration_weighted_variance ** 0.5\n",
    "\n",
    "    # ---------- Duration-weighted percentiles ----------\n",
    "    order: np.ndarray[np.int32] = np.argsort(pitches)\n",
    "    ordered_pitches, ordered_weights = pitches[order], weights[order]\n",
    "    cumulative_weight_sum: np.ndarray[np.float32] = np.cumsum(ordered_weights)\n",
    "\n",
    "    # Weighted quantile: find the first index where cumulative weight crosses q%.\n",
    "    def weighted_quantile(quantile: float) -> float:\n",
    "        # Target cumulative weight at quantile q (0..100).\n",
    "        target: float = (quantile / 100.0) * cumulative_weight_sum[-1]\n",
    "        # Index where cumulative_weight_sum >= target; take leftmost to be consistent.\n",
    "        idx: int = np.searchsorted(cumulative_weight_sum, target, side=\"left\")\n",
    "        return float(ordered_pitches[min(idx, len(ordered_pitches) - 1)])\n",
    "\n",
    "    # 10th / 50th (median) / 90th percentiles, duration-weighted.\n",
    "    percentile_10, percentile_50, percentile_90 = weighted_quantile(10), weighted_quantile(50), weighted_quantile(90)\n",
    "\n",
    "    # Ambitus = robust spread (p90 - p10), less sensitive than raw max - min.\n",
    "    return dict(mean=duration_weight_mean, \n",
    "                std=weighted_std,\n",
    "                p10=percentile_10, \n",
    "                p50=percentile_50, \n",
    "                p90=percentile_90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66999304",
   "metadata": {},
   "source": [
    "Extracting high level features relating to the entire song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85395762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_midi_features(midi: PrettyMIDI) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return song-level features (vector+names).\n",
    "    \"\"\" \n",
    "    duration_sec: float = midi.get_end_time()                                       # total length\n",
    "    if duration_sec <= 0: raise ValueError(\"Empty/zero-length MIDI.\")        # guard\n",
    "\n",
    "    tempo_times, tempo_bpms = midi.get_tempo_changes()                               # tempo changes\n",
    "    if len(tempo_bpms) == 0:                                                     # no changes\n",
    "        tempo_times = np.array([0.0], dtype=np.float32)                          # start time\n",
    "        tempo_bpms = np.array([midi.estimate_tempo()], dtype=np.float32)         # single bpm\n",
    "    segment_ends: np.ndarray[np.float32] = np.r_[tempo_times[1:], duration_sec]                              # segment ends\n",
    "    segment_durs: np.ndarray[np.float32] = np.maximum(1e-6, segment_ends - tempo_times[:len(segment_ends)])          # segment durations\n",
    "    tempo_mean: float = float(np.dot(tempo_bpms[:len(segment_durs)], segment_durs) / np.sum(segment_durs)) # duration-weighted mean\n",
    "    tempo_std: float = float(np.std(np.repeat(\n",
    "        tempo_bpms[:len(segment_durs)],                                              # repeat bpm by\n",
    "        np.maximum(1, (segment_durs/np.sum(segment_durs)*1000).astype(int))          # rough weights\n",
    "    )))                                                                       # dispersion proxy\n",
    "    tempo_change_count: int = int(len(tempo_bpms))                                     # number of states\n",
    "\n",
    "    time_signature_numerator, time_signature_denominator = most_common_time_signature(midi.time_signature_changes)  # mode time sig\n",
    "    instrument_count: int = sum(1 for inst in midi.instruments                     # non-drum count\n",
    "                          if not inst.is_drum and inst.notes)\n",
    "\n",
    "    instruments: list = [inst for inst in midi.instruments if not inst.is_drum and inst.notes]\n",
    "    instruments_velocities: list[float] = []\n",
    "    instrument_notes: list[Note] = []\n",
    "    for instrument in instruments:                                 # melody track\n",
    "        mel_velocity = [note.velocity for note in instrument.notes]     # melody velocities\n",
    "        instruments_velocities.extend(mel_velocity)\n",
    "        instrument_notes.extend(instrument.notes)\n",
    "\n",
    "    instrument_velocities_min: float = min(instruments_velocities)      # min pitch\n",
    "    instrument_velocities_max: float = max(instruments_velocities)      # max pitch\n",
    "    instrument_velocities_mean: float = np.mean(instruments_velocities)    # mean pitch\n",
    "    instrument_velocities_std: float = np.std(instruments_velocities)     # std pitch\n",
    "\n",
    "    duration_weight_pitch_stats_dict: dict = get_duration_weighted_pitch_stats(instrument_notes)\n",
    "    instrument_pitch_10_percentile: float = duration_weight_pitch_stats_dict['p10']\n",
    "    instrument_pitch_50_percentile: float = duration_weight_pitch_stats_dict['p50']\n",
    "    instrument_pitch_90_percentile: float = duration_weight_pitch_stats_dict['p90']\n",
    "    instrument_pitch_mean: float = duration_weight_pitch_stats_dict['mean']\n",
    "    instrument_pitch_std: float = duration_weight_pitch_stats_dict['std']\n",
    "    instrument_pitch_range_by_percentiles: float = instrument_pitch_90_percentile - instrument_pitch_10_percentile\n",
    "\n",
    "    note_durations: list[float] = [note.end - note.start for note in instrument_notes]\n",
    "    note_durations_mean: float = np.mean(note_durations) if note_durations else 0.0\n",
    "    note_durations_std: float = np.std(note_durations) if note_durations else 0.0\n",
    "    note_durations_range: float = max(note_durations) - min(note_durations) if note_durations else 0.0\n",
    "\n",
    "    note_density: float = float(len(instrument.notes) / max(EPSILON, duration_sec))           # notes/sec\n",
    "\n",
    "    chroma_global = midi.get_pitch_class_histogram(use_duration=True)        # 12-bin chroma\n",
    "    chroma_global = chroma_global / (np.sum(chroma_global) + EPSILON)           # normalize\n",
    "    names = [                                                                # feature names\n",
    "        \"duration_sec\",\n",
    "        \"tempo_mean_bpm\", \n",
    "        \"tempo_std_bpm\", \n",
    "        \"tempo_change_count\",\n",
    "        \"time_sig_num\", \n",
    "        \"time_sig_den\",\n",
    "        \"instrument_count\",\n",
    "        \"instrument_velocities_min\", \n",
    "        \"instrument_velocities_max\", \n",
    "        \"instrument_velocities_mean\", \n",
    "        \"instrument_velocities_std\", \n",
    "        \"instrument_pitch_10_percentile\",\n",
    "        \"instrument_pitch_50_percentile\",\n",
    "        \"instrument_pitch_90_percentile\",\n",
    "        \"instrument_pitch_mean\",\n",
    "        \"instrument_pitch_std\",\n",
    "        \"instrument_pitch_range_by_percentiles\",\n",
    "        \"note_durations_mean\",\n",
    "        \"note_durations_std\",\n",
    "        \"note_durations_range\",\n",
    "        \"melody_note_density_per_sec\",\n",
    "    ] + [f\"chroma_{i}\" for i in range(12)]                                   # chroma names\n",
    "    vec = np.array([                                                         # feature vector\n",
    "        duration_sec, \n",
    "        tempo_mean, \n",
    "        tempo_std, \n",
    "        tempo_change_count,\n",
    "        time_signature_numerator, \n",
    "        time_signature_denominator,   \n",
    "        instrument_count,\n",
    "        instrument_velocities_min, \n",
    "        instrument_velocities_max, \n",
    "        instrument_velocities_mean, \n",
    "        instrument_velocities_std, \n",
    "        instrument_pitch_10_percentile,\n",
    "        instrument_pitch_50_percentile,\n",
    "        instrument_pitch_90_percentile,\n",
    "        instrument_pitch_mean,\n",
    "        instrument_pitch_std,\n",
    "        instrument_pitch_range_by_percentiles,\n",
    "        note_durations_mean,\n",
    "        note_durations_std,\n",
    "        note_durations_range,   \n",
    "        note_density,\n",
    "        *chroma_global.tolist()\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return {\"vector\": vec, \"names\": names} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c68e2f",
   "metadata": {},
   "source": [
    "<font size=6>Auxlilliary Data Structures</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f7e0b",
   "metadata": {},
   "source": [
    "Auxilliary functions for creation of word sequences and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ba93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_sequences_with_targets(tokenized_lyrics: list[str], sequence_length: int = SEQUENCE_LENGTH) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given tokenized lyrics as a list of strings, create sequences of word indices and their corresponding target word indices.\n",
    "    Each sequence is of length `sequence_length`, and the target is the next word following the sequence.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Create sequences and targets\n",
    "    for i in range(len(tokenized_lyrics) - sequence_length):\n",
    "        seq = tokenized_lyrics[i:i + sequence_length]\n",
    "        target = tokenized_lyrics[i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return sequences,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongData:\n",
    "    def __init__(self, song_data_cell: list[str] = None, midi_file: PrettyMIDI = None):\n",
    "        if len(song_data_cell) != 3:\n",
    "            raise ValueError(\"song_data_cell must have exactly three elements: [artist, title, lyrics]\")\n",
    "        self.artist = song_data_cell[0]\n",
    "        self.title = song_data_cell[1]\n",
    "        self.lyrics = song_data_cell[2]\n",
    "        self.midi_data = midi_file\n",
    "        self._midi_features: Optional[dict[str, np.ndarray]] = None\n",
    "\n",
    "    @property\n",
    "    def midi_features(self):\n",
    "        if self._midi_features is None:\n",
    "            self._midi_features = extract_midi_features(self.midi_data)\n",
    "        return self._midi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8efdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataset(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 songs_data: list[SongData],\n",
    "                 word_embeddings: dict[str, np.ndarray],\n",
    "                 artist_to_index: dict[str, int]):\n",
    "        self.midi_features: list[np.ndarray] = list()\n",
    "        self.artists: list[str] = list()\n",
    "        self.sequence_artists: list[str] = list()\n",
    "        self.word_sequences: list[str] = list()\n",
    "        self.sequences_targets: list[str] = list()\n",
    "        self.sequence_to_midi: list[int] = list() # Maps each sequence to its corresponding MIDI feature index \n",
    "        self.sequence_to_artist: list[int] = list() # Maps each sequence to its corresponding artist embedding index\n",
    "        self.word_embeddings: dict[str, np.ndarray] = word_embeddings\n",
    "        self.artist_to_index: dict[str, np.ndarray] = artist_to_index\n",
    "        # Instead of saving each sequence's MIDI features, we save the index of the MIDI features in the midi_features list to save space.\n",
    "        for idx, song in enumerate(songs_data):\n",
    "            sequences, targets = create_word_sequences_with_targets(song.lyrics)\n",
    "            self.word_sequences.extend(sequences)\n",
    "            self.sequences_targets.extend(targets)\n",
    "            self.midi_features.append(song.midi_features['vector']) # Creates a mapping of the features to the sequences.\n",
    "            self.sequence_artists.append(song.artist)\n",
    "            self.sequence_to_midi.extend([idx] * len(sequences))\n",
    "            self.sequence_to_artist.extend([idx] * len(sequences))\n",
    "        print(f'Dataset has: {len(self.word_sequences)} sequences and {len(self.sequences_targets)} targets')\n",
    "\n",
    "    \n",
    "    def word_vec(self, tok: str) -> np.ndarray:\n",
    "        # helper to get word vector, or zeros if OOV\n",
    "        v = self.word_embeddings.get(tok)\n",
    "        if v is None:\n",
    "            # OOV → zeros with same dim as any known word vector\n",
    "            sample = next(iter(self.word_embeddings.values()))\n",
    "            v = np.zeros_like(sample, dtype=np.float32)\n",
    "        return v.astype(np.float32, copy=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_sequences)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        word_sequence = self.word_sequences[idx]          \n",
    "        target_word = self.sequences_targets[idx]\n",
    "        midi_feature: np.ndarray = self.midi_features[self.sequence_to_midi[idx]]\n",
    "        artist_name: str = self.sequence_artists[self.sequence_to_artist[idx]]\n",
    "        artist_index: int = self.artist_to_index[artist_name]\n",
    "        stacked_word_sequence = np.stack([self.word_vec(tok) for tok in word_sequence], axis=0)\n",
    "        stacked_word_sequence_with_midi_features = np.broadcast_to(midi_feature, (stacked_word_sequence.shape[0], midi_feature.shape[0]))\n",
    "        stacked_word_sequence_with_artist = np.broadcast_to(artist_index, (stacked_word_sequence.shape[0], 1)) # simple index.\n",
    "        concatenated_features = np.concatenate((stacked_word_sequence, stacked_word_sequence_with_midi_features, stacked_word_sequence_with_artist), axis=1)\n",
    "        line_length = 0\n",
    "        for w in word_sequence:\n",
    "            if w == f\"{EOL_STRING}\":\n",
    "                line_length = 0\n",
    "            else:\n",
    "                line_length += 1\n",
    "        # If no <eol> yet, line_length is length of sequence\n",
    "        return concatenated_features, target_word, line_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec874e9",
   "metadata": {},
   "source": [
    "<font size=6>Reading CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1959fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LYRIC_TRAIN_SET_CSV_PATH, mode='r', encoding='utf-8') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    lyric_train_data = list(reader)\n",
    "\n",
    "with open(LYRIC_TEST_SET_CSV_PATH, mode='r', encoding='utf-8') as test_file:\n",
    "    reader = csv.reader(test_file)\n",
    "    lyric_test_data = list(reader)\n",
    "\n",
    "if len(lyric_train_data) < 1 or len(lyric_test_data) < 1:\n",
    "    raise Exception(\"CSV files are empty or not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996afc4a",
   "metadata": {},
   "source": [
    "<font size=6>Parsing CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_data(raw_csv_data: list[list[str]]) -> list[tuple[str, str, list[str]]]:\n",
    "    returned_cleaned_csv_data: list[tuple[str, str, list[str]]] = []\n",
    "    for row in raw_csv_data:\n",
    "        artist = row[0].strip()\n",
    "        title_index = 1\n",
    "        lyrics_index = 2\n",
    "        while lyrics_index < len(row):\n",
    "            title = row[title_index].strip()\n",
    "            title = title.removesuffix('-2') # Remove '-2' suffix if present, relevant in 1 case.\n",
    "            title = row[title_index].strip()\n",
    "            lyrics = row[lyrics_index].strip()\n",
    "            lyrics = lyrics.lower()\n",
    "            lyrics = re.sub(f\"[{re.escape('&')}]\", f\" {EOL_STRING} \", lyrics) # Changing ampersands to eol to indicate end of line.\n",
    "            lyrics = re.sub(f\"[{re.escape('\\'')}]\", \"\", lyrics) # Removing apostrophes.\n",
    "            lyrics = re.sub(f\"[{re.escape('-')}]\", \" \", lyrics) # Removing hyphens.\n",
    "            lyrics = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", lyrics) # Removing punctuation.\n",
    "            lyrics = lyrics.split(' ') # Tokenzing each word by space.\n",
    "            lyrics = [word.strip() for word in lyrics if word] # Removing empty strings.\n",
    "            lyrics.append('<eos>') # Adding end of song token.\n",
    "            if len(title) > 0 and len(lyrics) > 0:\n",
    "                returned_cleaned_csv_data.append((artist, title, lyrics))\n",
    "            title_index += 2\n",
    "            lyrics_index += 2\n",
    "    return returned_cleaned_csv_data\n",
    "\n",
    "cleaned_lyric_train_data = clean_csv_data(lyric_train_data)\n",
    "cleaned_lyric_test_data = clean_csv_data(lyric_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique words in the lyrics\n",
    "def get_word_frequencies(lyrics_data: list[tuple[str, str, list[str]]]) -> dict[str, int]:\n",
    "    words_frequency = defaultdict(int)\n",
    "    for _, _, lyrics in lyrics_data:\n",
    "        for word in lyrics:\n",
    "            words_frequency[word] += 1\n",
    "    return words_frequency    \n",
    "word_frequencies_training: dict[str, int] = get_word_frequencies(cleaned_lyric_train_data)\n",
    "word_frequencies_test: dict[str, int] = get_word_frequencies(cleaned_lyric_test_data)\n",
    "print(f\"Number of unique words in training set: {len(word_frequencies_training)}\")\n",
    "print(f\"Number of unique words in test set: {len(word_frequencies_test)}\")\n",
    "\n",
    "d_sorted_by_val = sorted(word_frequencies_training.items(), key=lambda kv: kv[1], reverse=True)\n",
    "d_sorted_by_val[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141709c5",
   "metadata": {},
   "source": [
    "<font size=6>Reading MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80433ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_files(midi_files_location: str, pickling_path: Optional[str] = None, failed_loads_path: Optional[str] = None) -> \\\n",
    "                    tuple[dict[str, dict[str, PrettyMIDI]], dict[str, set[str]]]: # artist -> title -> PrettyMIDI, failed loads[artist, song_set]\n",
    "    failed_loads = dict()\n",
    "    if failed_loads_path is not None and os.path.isfile(failed_loads_path):\n",
    "        with open(failed_loads_path, \"rb\") as f:\n",
    "            failed_loads = pickle.load(f)\n",
    "        print(f\"Loaded failed MIDI loads from pickled file {failed_loads_path}.\")\n",
    "    if pickling_path is not None and os.path.isfile(pickling_path):\n",
    "        with open(pickling_path, \"rb\") as f:\n",
    "            loaded_midi_files = pickle.load(f)\n",
    "        print(f\"Loaded MIDI files from pickled file {pickling_path}.\")\n",
    "        print(f'Loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.')\n",
    "        return loaded_midi_files, failed_loads\n",
    "    if not os.path.isdir(midi_files_location):\n",
    "        raise ValueError(f\"MIDI file path {midi_files_location} is not a valid directory.\")\n",
    "\n",
    "    # Traversing over all files and attempt to load them with pretty_midi:\n",
    "    loaded_midi_files: dict[str, dict[str, PrettyMIDI]] = defaultdict(dict) # artist -> title -> PrettyMIDI\n",
    "    failed_loads: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    for file in os.listdir(midi_files_location):\n",
    "        if file.endswith('.mid') or file.endswith('.midi'):\n",
    "            file_path = os.path.join(midi_files_location, file)\n",
    "            file = file.removesuffix('.mid')\n",
    "            splitted_artist_and_title = file.split('_-_')\n",
    "            artist = splitted_artist_and_title[0]\n",
    "            title = splitted_artist_and_title[1]\n",
    "            if len(splitted_artist_and_title) > 2:\n",
    "                print(f\"Warning: file {file} has more than one '_-_' separator, ignoring the rest after second \\\"_-_\\\".\")\n",
    "            artist = artist.replace('_', ' ').strip().lower()\n",
    "            title = title.replace('_', ' ').strip().lower()\n",
    "            try:\n",
    "                midi_data = PrettyMIDI(file_path)\n",
    "                loaded_midi_files[artist][title] = midi_data\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file}: {e}\")\n",
    "                failed_loads[artist].add(title)\n",
    "\n",
    "\n",
    "\n",
    "    if failed_loads:\n",
    "        print(\"Failed to load the following artist and lyric midi files:\")\n",
    "        for artist, lyrics in failed_loads.items():\n",
    "            print(f\"{artist} - [{', '.join(lyrics)}]\")\n",
    "\n",
    "    if pickling_path is not None:\n",
    "        with open(pickling_path, \"wb\") as f:\n",
    "            pickle.dump(loaded_midi_files, f)\n",
    "            print(f\"Pickled loaded MIDI files to {pickling_path}.\")\n",
    "    if failed_loads_path is not None:\n",
    "        with open(failed_loads_path, \"wb\") as f:\n",
    "            pickle.dump(failed_loads, f)\n",
    "            print(f\"Pickled failed MIDI loads to {failed_loads_path}.\")\n",
    "\n",
    "    print(f\"Successfully loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.\")\n",
    "    return loaded_midi_files, failed_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a752d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_midi_files, failed_midi_loads = load_midi_files(MIDI_FILE_PATH, PICKLING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ffe14",
   "metadata": {},
   "source": [
    "<font size=6>Mapping CSV data to MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_data_to_songdata_list(csv_data: list[list[str]], \n",
    "                              failed_midi_load: dict[str, set[str]], \n",
    "                              midi_files_dict: dict[str, dict[str, PrettyMIDI]]) -> list[SongData]:\n",
    "    song_data_list: list[SongData] = list()\n",
    "    missing_midi_count = 0\n",
    "    for row in csv_data:\n",
    "        artist = row[0]\n",
    "        title = row[1]\n",
    "        if artist in failed_midi_load and title in failed_midi_load[artist]:\n",
    "            print(f\"Skipping {artist} - {title} due to previous MIDI load failure.\")\n",
    "            continue\n",
    "        if artist in midi_files_dict and title in midi_files_dict[artist]:\n",
    "            midi_file = midi_files_dict[artist][title]\n",
    "            song_data = SongData(row, midi_file)\n",
    "            song_data_list.append(song_data)\n",
    "        else:\n",
    "            missing_midi_count += 1\n",
    "            print(f\"Missing MIDI file for artist '{artist}' and title '{title}'\")\n",
    "    print(f\"Total songs with missing MIDI files: {missing_midi_count}\")\n",
    "    return song_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f061cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_train_data, failed_midi_loads, loaded_midi_files)\n",
    "test_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_test_data, failed_midi_loads, loaded_midi_files)\n",
    "print(f\"Total training songs with MIDI data: {len(train_midi_data)}\")\n",
    "print(f\"Total test songs with MIDI data: {len(test_midi_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0f1d",
   "metadata": {},
   "source": [
    "<font size=6>Handling word embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95f984",
   "metadata": {},
   "source": [
    "Downloading pretrained word2vec, containing 300 dims, trained on news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0482d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bebb8",
   "metadata": {},
   "source": [
    "Extracting the vocabulary from the lyrics.\n",
    "Getting the data from the test set aswell since the vocbulary needs to be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_vocabulary: set[str] = set()\n",
    "# Getting the data from the test set aswell since the vocbulary needs to be known\n",
    "for song in train_midi_data + test_midi_data:\n",
    "    for word in song.lyrics:\n",
    "        lyrics_vocabulary.add(word)\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305abc9",
   "metadata": {},
   "source": [
    "Creating unified embedding.\n",
    "Extracting embeddings from word2vec and using random embeddings for words not found in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_embeddings: dict[str, np.ndarray] = dict()\n",
    "existing_words_in_pretrained = 0\n",
    "not_existing_in_pretrained = 0\n",
    "added_stopwords = 0\n",
    "for word in list(lyrics_vocabulary):\n",
    "    if word in pretrained_word2vec:\n",
    "        unified_embeddings[word] = pretrained_word2vec[word]\n",
    "        existing_words_in_pretrained += 1\n",
    "    else:\n",
    "        unified_embeddings[word] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,)) # Random init for unknown words.  \n",
    "        not_existing_in_pretrained += 1\n",
    "    # Adding stopwords as well, since they are common and should be in the vocabulary.\n",
    "for stopword in stopwords.words('english'):\n",
    "    cleaned_stopword = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", stopword.strip().lower()) # Cleaning the stopword, since it contains punctuation.\n",
    "    if cleaned_stopword not in unified_embeddings:\n",
    "        unified_embeddings[cleaned_stopword] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "        added_stopwords += 1\n",
    "\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")\n",
    "print(f\"Existing words in pretrained embeddings: {existing_words_in_pretrained}\")\n",
    "print(f\"Not existing in pretrained embeddings (randomly initialized): {not_existing_in_pretrained}\")\n",
    "print(f\"Added stopwords (randomly initialized): {added_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b23e9d",
   "metadata": {},
   "source": [
    "<font size=6>Handling artist embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f40eba",
   "metadata": {},
   "source": [
    "Using simple indexing for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_artists = [song.artist for song in train_midi_data]\n",
    "test_artists = [song.artist for song in test_midi_data]\n",
    "artist_set: set = (set(train_artists).union(set(test_artists)))\n",
    "artist_to_index: dict[str, int] = dict()\n",
    "index_to_artist: dict[int, str] = dict()\n",
    "for index, artist in enumerate(artist_set):\n",
    "    artist_to_index[artist] = index\n",
    "    index_to_artist[index] = artist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df1ccb",
   "metadata": {},
   "source": [
    "<font size=6>Load dataset and dataloader</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163de739",
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata_train_dataset = SongDataset(train_midi_data, unified_embeddings, artist_to_index)\n",
    "songdata_test_dataset = SongDataset(train_midi_data, unified_embeddings, artist_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee962c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = train_test_split(songdata_train_dataset, test_size=VALIDATION_SPLIT, random_state=RANDOM_LOADER_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = data.DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_data_loader = data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_data_loader = data.DataLoader(songdata_test_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word_in_vocab: index_of_word for index_of_word, word_in_vocab in enumerate(songdata_train_dataset.word_embeddings.keys())} \n",
    "id_to_word = {index_of_word: word_in_vocab for word_in_vocab, index_of_word in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfa020",
   "metadata": {},
   "source": [
    "<font size=6>Model 1: Simple concatenation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration Method 1: Simple Concatenation - Melody features are concatenated to each word embedding\n",
    "class LyricsGenerator_Concatenation(nn.Module):\n",
    "  def __init__(self, vocab_size, \n",
    "               input_size: int, \n",
    "               hidden_layer_dim: int, \n",
    "               num_layers: int = LSTM_LAYERS, \n",
    "               dropout_rate: float = DROPOUT\n",
    "               ):\n",
    "    super(LyricsGenerator_Concatenation, self).__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # Input dimension is embedding + melody features\n",
    "\n",
    "    # LSTM processes concatenated features\n",
    "    self.lstm = nn.LSTM(input_size, hidden_layer_dim, num_layers,\n",
    "                        batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "    # Dropout and output layers\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    self.fc = nn.Linear(hidden_layer_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x is already concatenated [batch_size, seq_len, embedding_dim + melody_dim]\n",
    "\n",
    "    # LSTM forward pass\n",
    "    lstm_out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "    # Use the last output for prediction\n",
    "    last_output = lstm_out[:, -1, :]  # [batch_size, hidden_dim]\n",
    "\n",
    "    # Apply dropout and final linear layer\n",
    "    output = self.dropout(last_output)\n",
    "    output = self.fc(output)  # [batch_size, vocab_size]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02346871",
   "metadata": {},
   "source": [
    "<font size=6>Running the models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f210285",
   "metadata": {},
   "source": [
    "<font size=5>Running model 1: Concatenation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb53ce",
   "metadata": {},
   "source": [
    "Add a custom loss to enforce the creation of words that look like actual lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyric_generation_loss(outputs, \n",
    "                          targets, \n",
    "                          base_criterion,\n",
    "                          word_to_id,\n",
    "                          min_line_length=MIN_LINE_LENGTH, \n",
    "                          max_line_length=MAX_LINE_LENGTH,\n",
    "                          current_line_length=None, \n",
    "                          penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    outputs: logits from model [batch_size, vocab_size]\n",
    "    targets: tensor of target indices [batch_size]\n",
    "    current_line_length: list[int] or tensor [batch_size] (length of current line for each sample)\n",
    "    current_num_lines: list[int] or tensor [batch_size] (number of lines so far for each sample)\n",
    "    \"\"\"\n",
    "    base_loss = base_criterion(outputs, targets)\n",
    "    eol_id = word_to_id.get(f\"{EOL_STRING}\")\n",
    "    penalty = 0.0\n",
    "    if eol_id is not None and current_line_length is not None:\n",
    "        # Penalize if eol is selected and line is too short\n",
    "        eol_selected = (targets == eol_id)\n",
    "        short_line = (torch.tensor(current_line_length) < min_line_length)\n",
    "        penalty += penalty_weight * torch.sum(eol_selected & short_line).float() / outputs.size(0)\n",
    "\n",
    "        # Penalize if eol is not selected and line is too long\n",
    "        long_line = (torch.tensor(current_line_length) > max_line_length)\n",
    "        penalty += penalty_weight * torch.sum(eol_selected & long_line).float() / outputs.size(0)\n",
    "    \n",
    "\n",
    "    return base_loss + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02754ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: data.DataLoader, \n",
    "                val_loader: data.DataLoader, \n",
    "                test_loader: data.DataLoader,\n",
    "                word_to_id_dict: dict[str, int],\n",
    "                num_epochs: int = MAX_EPOCHS, \n",
    "                learning_rate: float = LEARNING_RATE,\n",
    "                patiance_factor: float = PATIANCE_FACTOR,\n",
    "                patiance_epochs: int = PATIANCE_EPOCHS):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "    unk_id = word_to_id_dict.get(\"<unk>\", 0)\n",
    "    train_losses: list[float] = list()\n",
    "    val_losses: list[float] = list()\n",
    "    test_losses: list[float] = list()\n",
    "    best_validation_loss: float = 10000.0\n",
    "    epochs_with_no_improvements: int = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        current_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets, line_lengths in train_loader:\n",
    "            inputs = inputs.to(dtype=torch.float32)\n",
    "            inputs = inputs.to(device)\n",
    "            targets_indices = torch.tensor(\n",
    "                [word_to_id.get(t, unk_id) for t in targets],\n",
    "                dtype=torch.long, device=device\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = lyric_generation_loss(\n",
    "                outputs,\n",
    "                targets_indices,\n",
    "                base_criterion=criterion,\n",
    "                word_to_id=word_to_id,\n",
    "                min_line_length=MIN_LINE_LENGTH,\n",
    "                current_line_length=line_lengths,\n",
    "                penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        validation_running_loss = 0.0\n",
    "        test_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_line_length in val_loader:\n",
    "                val_inputs = val_inputs.to(dtype=torch.float32)\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets_indices = torch.tensor(\n",
    "                    [word_to_id.get(t, unk_id) for t in val_targets],\n",
    "                    dtype=torch.long, device=device\n",
    "                )\n",
    "                validation_outputs = model(val_inputs)\n",
    "                validation_loss = lyric_generation_loss(\n",
    "                    validation_outputs,\n",
    "                    val_targets_indices,\n",
    "                    base_criterion=criterion,\n",
    "                    word_to_id=word_to_id,\n",
    "                    min_line_length=MIN_LINE_LENGTH,\n",
    "                    current_line_length=val_line_length,\n",
    "                    penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "                )\n",
    "                validation_running_loss += validation_loss.item() * val_inputs.size(0)\n",
    "\n",
    "            for test_inputs, test_targets, test_line_length in test_loader:\n",
    "                test_inputs = test_inputs.to(dtype=torch.float32)\n",
    "                test_inputs = test_inputs.to(device)\n",
    "                test_targets_indices = torch.tensor(\n",
    "                    [word_to_id.get(t, unk_id) for t in test_targets],\n",
    "                    dtype=torch.long, device=device\n",
    "                )\n",
    "                test_outputs = model(test_inputs)                  # logits (B, |V|)\n",
    "                test_loss = criterion(test_outputs, test_targets_indices)\n",
    "                test_loss = lyric_generation_loss(\n",
    "                    test_outputs,\n",
    "                    test_targets_indices,\n",
    "                    base_criterion=criterion,\n",
    "                    word_to_id=word_to_id,\n",
    "                    min_line_length=MIN_LINE_LENGTH,\n",
    "                    current_line_length=test_line_length,\n",
    "                    penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "                )\n",
    "                test_running_loss += test_loss.item() * test_inputs.size(0)\n",
    "\n",
    "        val_epoch_loss = validation_running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        test_epoch_loss = test_running_loss / len(test_loader.dataset)\n",
    "        test_losses.append(test_epoch_loss)\n",
    "        if  best_validation_loss - val_epoch_loss >= patiance_factor:\n",
    "            epochs_with_no_improvements = 0\n",
    "            best_validation_loss = val_epoch_loss\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_with_no_improvements += 1\n",
    "            print(f'No improvement in epoch. Patiance: {epochs_with_no_improvements}\\\\{patiance_epochs}')\n",
    "        scheduler.step()\n",
    "        finish_time = time.time() - current_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, Test Loss: {test_epoch_loss:.4f}, time: {finish_time}')\n",
    "        if epochs_with_no_improvements >= patiance_epochs:\n",
    "            print(f'Training ended prematurely due to lack of improvement.')\n",
    "            break\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    model.eval()\n",
    "    return model, train_losses, val_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b319f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LyricsGenerator_Concatenation(\n",
    "    vocab_size=len(songdata_train_dataset.word_embeddings),\n",
    "    input_size=pretrained_word2vec.vector_size + NUMBER_OF_EXTRACT_MIDI_FEATURES + 1, # word embedding + melody features + artist index\n",
    "    hidden_layer_dim=256,\n",
    "    num_layers=LSTM_LAYERS,\n",
    "    dropout_rate=DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_loss, validation_loss, test_loss = train_model(model, \n",
    "            training_data_loader, \n",
    "            validation_data_loader, \n",
    "            test_data_loader,\n",
    "            word_to_id_dict=word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd43df",
   "metadata": {},
   "source": [
    "<font size=6>Generating Lyrics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587df720",
   "metadata": {},
   "source": [
    "Moved the embedding to the gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf15b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_top5_next_words(\n",
    "    model,\n",
    "    word_sequence: list[str],\n",
    "    artist_index: int,\n",
    "    melody_vec,\n",
    "    word_to_id: dict[str, int],\n",
    "    id_to_word: dict[int, str],\n",
    "    embedding_weight: torch.Tensor,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    model.to(device).eval()\n",
    "    if embedding_weight.device.type != device:\n",
    "        embedding_weight = embedding_weight.to(device)\n",
    "\n",
    "    # melody -> torch tensor on device\n",
    "    if not torch.is_tensor(melody_vec):\n",
    "        melody_vec = torch.as_tensor(melody_vec, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        melody_vec = melody_vec.to(device, dtype=torch.float32)\n",
    "\n",
    "    # Prepare sequence embeddings\n",
    "    unk_id = word_to_id.get(\"<unk>\", UNK_ID)\n",
    "    seq_ids = [word_to_id.get(w, unk_id) for w in word_sequence]\n",
    "    seq_embs = embedding_weight[torch.tensor(seq_ids, device=device)]  # [seq_len, emb_dim]\n",
    "\n",
    "    # Broadcast melody and artist features\n",
    "    melody_broadcast = melody_vec.expand(len(word_sequence), -1)  # [seq_len, melody_dim]\n",
    "    artist_broadcast = torch.full((len(word_sequence), 1), artist_index, dtype=torch.float32, device=device)  # [seq_len, 1]\n",
    "\n",
    "    # Concatenate features\n",
    "    x = torch.cat([seq_embs, melody_broadcast, artist_broadcast], dim=1).unsqueeze(0)  # [1, seq_len, input_size]\n",
    "\n",
    "    logits = model(x)  # [1, vocab]\n",
    "    probs = F.softmax(logits, dim=-1).squeeze(0)  # [vocab]\n",
    "\n",
    "    vals, idxs = probs.topk(5)\n",
    "    top5 = [(id_to_word[i], float(v)) for i, v in zip(idxs.tolist(), vals.tolist())]\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unified_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acea986",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_melody_tensor = train_midi_data[0].midi_features['vector']\n",
    "embedding_weight = torch.from_numpy(\n",
    "    np.stack([unified_embeddings[w].astype(np.float32) for w in word_to_id], axis=0)\n",
    ").to(device)   # torch.FloatTensor on GPU\n",
    "word_sequence = ['i', 'dont', 'have', 'you', 'eol']\n",
    "asdasd = predict_top5_next_words(\n",
    "    model,\n",
    "    artist_index=3,\n",
    "    word_sequence=word_sequence,\n",
    "    melody_vec=my_melody_tensor,          # [melody_dim]\n",
    "    word_to_id=word_to_id,\n",
    "    id_to_word=id_to_word,\n",
    "    embedding_weight=embedding_weight,    # or your embedding layer’s .weight.data\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "for item in asdasd:\n",
    "    print(item)\n",
    "\n",
    "print('--------------------------------')\n",
    "asdasd = predict_top5_next_words(\n",
    "    model,\n",
    "    artist_index=34,\n",
    "    word_sequence=word_sequence,\n",
    "    melody_vec= train_midi_data[9].midi_features['vector'],          # [melody_dim]\n",
    "    word_to_id=word_to_id,\n",
    "    id_to_word=id_to_word,\n",
    "    embedding_weight=embedding_weight,    # or your embedding layer’s .weight.data\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "for item in asdasd:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
