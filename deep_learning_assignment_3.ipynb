{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd41e9a8",
   "metadata": {},
   "source": [
    "<font size=6>Downloading required libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q numpy\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip3 install torch torchsummary\n",
    "!pip3 install torch tensorboard\n",
    "!pip3 install -q pretty_midi\n",
    "!pip3 install -q gensim\n",
    "!pip3 install -q nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4068e6c",
   "metadata": {},
   "source": [
    "<font size=6>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from torchsummary import summary\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "import csv\n",
    "import string\n",
    "from pretty_midi import PrettyMIDI, Note\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359ed8",
   "metadata": {},
   "source": [
    "<font size=6>Constants</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60457569",
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRIC_TRAIN_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_train_set.csv')\n",
    "LYRIC_TEST_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_test_set.csv')\n",
    "MIDI_FILE_PATH: str = os.path.join(os.getcwd(), 'data', 'midi_files')\n",
    "PICKLING_PATH: str = os.path.join(os.getcwd(), 'loaded_midi_files.pkl') # Path to save/load pickled MIDI files, for faster loading.\n",
    "EPSILON: float = 1e-9\n",
    "SEQUENCE_LENGTH: int = 10  # Number of words in the input sequence\n",
    "BATCH_SIZE: int = 128\n",
    "LSTM_LAYERS: int = 2\n",
    "ATTENTION_LAYERS: int = 2\n",
    "DROPOUT: float = 0.3\n",
    "ATTENTION_DROPOUT: float = 0.2\n",
    "ATTENTION_LAYERS: int = 2\n",
    "RANDOM_LOADER_SEED: int = 42\n",
    "VALIDATION_SPLIT: float = 0.1\n",
    "LEARNING_RATE: float = 0.001\n",
    "MAX_EPOCHS: int = 50\n",
    "NUMBER_OF_EXTRACT_MIDI_FEATURES: int = 33\n",
    "WORD_EMBEDDING_SIZE: int = 300\n",
    "SIZE_OF_ARTIST_INDEX: int = 1\n",
    "PATIANCE_FACTOR: float = 0.001\n",
    "PATIANCE_EPOCHS: int = 10\n",
    "UNK_ID: int = 0\n",
    "MIN_LINE_LENGTH: int = 5\n",
    "MAX_LINE_LENGTH: int = SEQUENCE_LENGTH\n",
    "EOL_STRING: str = '<eol>'\n",
    "UNK_STRING: str = '<unk>'\n",
    "EOS_STRING: str = '<eos>'\n",
    "SONG_BEGINNING_STRING: str = '<bos>'\n",
    "TOP_K_WORDS_TO_PREDICT: int = 20\n",
    "MAX_SONG_LENGTH_WORDS: int = 80\n",
    "HIDDEN_LAYER_DIM: int = 256\n",
    "HIDDEN_LAYER_DIM_ATTENTION: int = 256\n",
    "SEED = 42\n",
    "VERBOSE: str = True\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1947ce",
   "metadata": {},
   "source": [
    "<font size=6>Midi Feature extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083e542",
   "metadata": {},
   "source": [
    "Auxlliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_time_signature(changes: tuple[int, int]) -> tuple[int, int]:\n",
    "    if not changes: return (4, 4)\n",
    "    pairs: list[tuple[int, int]] = [(ts.numerator, ts.denominator) for ts in changes]\n",
    "    return Counter(pairs).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2526597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_weighted_pitch_stats(notes: list[Note]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute pitch statistics weighted by each note's duration.\n",
    "    Returns a dict with:\n",
    "      mean (duration-weighted),\n",
    "      std  (duration-weighted),\n",
    "      p10 / p50 / p90 (duration-weighted percentiles),\n",
    "      ambitus = p90 - p10 (robust range).\n",
    "    If `notes` is empty, returns safe defaults.\n",
    "    \"\"\"\n",
    "    # Empty guard: nothing to measure â†’ return neutral stats.\n",
    "    if not notes:\n",
    "        return dict(mean=0.0, std=0.0, p10=-1, p50=-1, p90=-1, ambitus=0.0)\n",
    "\n",
    "    # Vectorize pitches as float for math (MIDI 0..127, but floats simplify ops).\n",
    "    pitches: np.ndarray[np.float32] = np.fromiter((n.pitch for n in notes), dtype=np.float32)\n",
    "\n",
    "    # Each note's weight = its duration in seconds; clamp tiny/negative to epsilon.\n",
    "    weights: np.ndarray[np.float32] = np.fromiter((max(EPSILON, n.end - n.start) for n in notes), dtype=np.float32)\n",
    "    total_weights: float = weights.sum()\n",
    "    duration_weight_mean: float = float((weights * pitches).sum() / total_weights)\n",
    "    duration_weighted_variance: float = float((weights * (pitches - duration_weight_mean) ** 2).sum() / total_weights)\n",
    "    weighted_std: float = duration_weighted_variance ** 0.5\n",
    "\n",
    "    # ---------- Duration-weighted percentiles ----------\n",
    "    order: np.ndarray[np.int32] = np.argsort(pitches)\n",
    "    ordered_pitches, ordered_weights = pitches[order], weights[order]\n",
    "    cumulative_weight_sum: np.ndarray[np.float32] = np.cumsum(ordered_weights)\n",
    "\n",
    "    # Weighted quantile: find the first index where cumulative weight crosses q%.\n",
    "    def weighted_quantile(quantile: float) -> float:\n",
    "        # Target cumulative weight at quantile q (0..100).\n",
    "        target: float = (quantile / 100.0) * cumulative_weight_sum[-1]\n",
    "        # Index where cumulative_weight_sum >= target; take leftmost to be consistent.\n",
    "        idx: int = np.searchsorted(cumulative_weight_sum, target, side=\"left\")\n",
    "        return float(ordered_pitches[min(idx, len(ordered_pitches) - 1)])\n",
    "\n",
    "    # 10th / 50th (median) / 90th percentiles, duration-weighted.\n",
    "    percentile_10, percentile_50, percentile_90 = weighted_quantile(10), weighted_quantile(50), weighted_quantile(90)\n",
    "\n",
    "    # Ambitus = robust spread (p90 - p10), less sensitive than raw max - min.\n",
    "    return dict(mean=duration_weight_mean, \n",
    "                std=weighted_std,\n",
    "                p10=percentile_10, \n",
    "                p50=percentile_50, \n",
    "                p90=percentile_90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66999304",
   "metadata": {},
   "source": [
    "Extracting high level features relating to the entire song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85395762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_midi_features(midi: PrettyMIDI) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return song-level features (vector+names).\n",
    "    \"\"\" \n",
    "    duration_sec: float = midi.get_end_time()                                       # total length\n",
    "    if duration_sec <= 0: raise ValueError(\"Empty/zero-length MIDI.\")        # guard\n",
    "\n",
    "    tempo_times, tempo_bpms = midi.get_tempo_changes()                               # tempo changes\n",
    "    if len(tempo_bpms) == 0:                                                     # no changes\n",
    "        tempo_times = np.array([0.0], dtype=np.float32)                          # start time\n",
    "        tempo_bpms = np.array([midi.estimate_tempo()], dtype=np.float32)         # single bpm\n",
    "    segment_ends: np.ndarray[np.float32] = np.r_[tempo_times[1:], duration_sec]                              # segment ends\n",
    "    segment_durs: np.ndarray[np.float32] = np.maximum(1e-6, segment_ends - tempo_times[:len(segment_ends)])          # segment durations\n",
    "    tempo_mean: float = float(np.dot(tempo_bpms[:len(segment_durs)], segment_durs) / np.sum(segment_durs)) # duration-weighted mean\n",
    "    tempo_std: float = float(np.std(np.repeat(\n",
    "        tempo_bpms[:len(segment_durs)],                                              # repeat bpm by\n",
    "        np.maximum(1, (segment_durs/np.sum(segment_durs)*1000).astype(int))          # rough weights\n",
    "    )))                                                                       # dispersion proxy\n",
    "    tempo_change_count: int = int(len(tempo_bpms))                                     # number of states\n",
    "\n",
    "    time_signature_numerator, time_signature_denominator = most_common_time_signature(midi.time_signature_changes)  # mode time sig\n",
    "    instrument_count: int = sum(1 for inst in midi.instruments                     # non-drum count\n",
    "                          if not inst.is_drum and inst.notes)\n",
    "\n",
    "    instruments: list = [inst for inst in midi.instruments if not inst.is_drum and inst.notes]\n",
    "    instruments_velocities: list[float] = []\n",
    "    instrument_notes: list[Note] = []\n",
    "    for instrument in instruments:                                 # melody track\n",
    "        mel_velocity = [note.velocity for note in instrument.notes]     # melody velocities\n",
    "        instruments_velocities.extend(mel_velocity)\n",
    "        instrument_notes.extend(instrument.notes)\n",
    "\n",
    "    instrument_velocities_min: float = min(instruments_velocities)      # min pitch\n",
    "    instrument_velocities_max: float = max(instruments_velocities)      # max pitch\n",
    "    instrument_velocities_mean: float = np.mean(instruments_velocities)    # mean pitch\n",
    "    instrument_velocities_std: float = np.std(instruments_velocities)     # std pitch\n",
    "\n",
    "    duration_weight_pitch_stats_dict: dict = get_duration_weighted_pitch_stats(instrument_notes)\n",
    "    instrument_pitch_10_percentile: float = duration_weight_pitch_stats_dict['p10']\n",
    "    instrument_pitch_50_percentile: float = duration_weight_pitch_stats_dict['p50']\n",
    "    instrument_pitch_90_percentile: float = duration_weight_pitch_stats_dict['p90']\n",
    "    instrument_pitch_mean: float = duration_weight_pitch_stats_dict['mean']\n",
    "    instrument_pitch_std: float = duration_weight_pitch_stats_dict['std']\n",
    "    instrument_pitch_range_by_percentiles: float = instrument_pitch_90_percentile - instrument_pitch_10_percentile\n",
    "\n",
    "    note_durations: list[float] = [note.end - note.start for note in instrument_notes]\n",
    "    note_durations_mean: float = np.mean(note_durations) if note_durations else 0.0\n",
    "    note_durations_std: float = np.std(note_durations) if note_durations else 0.0\n",
    "    note_durations_range: float = max(note_durations) - min(note_durations) if note_durations else 0.0\n",
    "\n",
    "    note_density: float = float(len(instrument.notes) / max(EPSILON, duration_sec))           # notes/sec\n",
    "\n",
    "    chroma_global = midi.get_pitch_class_histogram(use_duration=True)        # 12-bin chroma\n",
    "    chroma_global = chroma_global / (np.sum(chroma_global) + EPSILON)           # normalize\n",
    "    names = [                                                                # feature names\n",
    "        \"duration_sec\",\n",
    "        \"tempo_mean_bpm\", \n",
    "        \"tempo_std_bpm\", \n",
    "        \"tempo_change_count\",\n",
    "        \"time_sig_num\", \n",
    "        \"time_sig_den\",\n",
    "        \"instrument_count\",\n",
    "        \"instrument_velocities_min\", \n",
    "        \"instrument_velocities_max\", \n",
    "        \"instrument_velocities_mean\", \n",
    "        \"instrument_velocities_std\", \n",
    "        \"instrument_pitch_10_percentile\",\n",
    "        \"instrument_pitch_50_percentile\",\n",
    "        \"instrument_pitch_90_percentile\",\n",
    "        \"instrument_pitch_mean\",\n",
    "        \"instrument_pitch_std\",\n",
    "        \"instrument_pitch_range_by_percentiles\",\n",
    "        \"note_durations_mean\",\n",
    "        \"note_durations_std\",\n",
    "        \"note_durations_range\",\n",
    "        \"melody_note_density_per_sec\",\n",
    "    ] + [f\"chroma_{i}\" for i in range(12)]                                   # chroma names\n",
    "    vec = np.array([                                                         # feature vector\n",
    "        duration_sec, \n",
    "        tempo_mean, \n",
    "        tempo_std, \n",
    "        tempo_change_count,\n",
    "        time_signature_numerator, \n",
    "        time_signature_denominator,   \n",
    "        instrument_count,\n",
    "        instrument_velocities_min, \n",
    "        instrument_velocities_max, \n",
    "        instrument_velocities_mean, \n",
    "        instrument_velocities_std, \n",
    "        instrument_pitch_10_percentile,\n",
    "        instrument_pitch_50_percentile,\n",
    "        instrument_pitch_90_percentile,\n",
    "        instrument_pitch_mean,\n",
    "        instrument_pitch_std,\n",
    "        instrument_pitch_range_by_percentiles,\n",
    "        note_durations_mean,\n",
    "        note_durations_std,\n",
    "        note_durations_range,   \n",
    "        note_density,\n",
    "        *chroma_global.tolist()\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return {\"vector\": vec, \"names\": names} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c68e2f",
   "metadata": {},
   "source": [
    "<font size=6>Auxlilliary Data Structures</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f7e0b",
   "metadata": {},
   "source": [
    "Auxilliary functions for creation of word sequences and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ba93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_sequences_with_targets(tokenized_lyrics: list[str], sequence_length: int = SEQUENCE_LENGTH) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given tokenized lyrics as a list of strings, create sequences of word indices and their corresponding target word indices.\n",
    "    Each sequence is of length `sequence_length`, and the target_sequence is a list of the next words of the sequence 1 index higher..\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Create sequences and targets\n",
    "    for i in range(len(tokenized_lyrics) - sequence_length):\n",
    "        seq = [SONG_BEGINNING_STRING] + tokenized_lyrics[i:i + sequence_length] # Padding\n",
    "        target_sequence = tokenized_lyrics[i + 1:i + sequence_length + 1]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target_sequence)\n",
    "    \n",
    "    return sequences,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongData:\n",
    "    def __init__(self, song_data_cell: list[str] = None, midi_file: PrettyMIDI = None):\n",
    "        if len(song_data_cell) != 3:\n",
    "            raise ValueError(\"song_data_cell must have exactly three elements: [artist, title, lyrics]\")\n",
    "        self.artist = song_data_cell[0]\n",
    "        self.title = song_data_cell[1]\n",
    "        self.lyrics = song_data_cell[2]\n",
    "        self.midi_data = midi_file\n",
    "        self._midi_features: Optional[dict[str, np.ndarray]] = None\n",
    "\n",
    "    @property\n",
    "    def midi_features(self):\n",
    "        if self._midi_features is None:\n",
    "            self._midi_features = extract_midi_features(self.midi_data)\n",
    "        return self._midi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8efdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataset(data.Dataset):\n",
    "    def __init__(self, \n",
    "                songs_data: list[SongData],\n",
    "                word_embeddings: dict[str, np.ndarray],\n",
    "                artist_to_index: dict[str, int],\n",
    "                word_to_id: dict[str, int]):\n",
    "        self.midi_features: list[np.ndarray] = list()\n",
    "        self.artists: list[str] = list()\n",
    "        self.sequence_artists: list[str] = list()\n",
    "        self.word_sequences: list[str] = list()\n",
    "        self.sequences_targets: list[str] = list()\n",
    "        self.sequence_to_midi: list[int] = list() # Maps each sequence to its corresponding MIDI feature index \n",
    "        self.sequence_to_artist: list[int] = list() # Maps each sequence to its corresponding artist embedding index\n",
    "        self.word_embeddings: dict[str, np.ndarray] = word_embeddings\n",
    "        self.artist_to_index: dict[str, int] = artist_to_index\n",
    "        self.word_to_id: dict[str, int] = word_to_id\n",
    "        # Instead of saving each sequence's MIDI features, we save the index of the MIDI features in the midi_features list to save space.\n",
    "        for idx, song in enumerate(songs_data):\n",
    "            sequences, targets = create_word_sequences_with_targets(song.lyrics)\n",
    "            self.word_sequences.extend(sequences)\n",
    "            self.sequences_targets.extend(targets)\n",
    "            self.midi_features.append(song.midi_features['vector']) # Creates a mapping of the features to the sequences.\n",
    "            self.sequence_artists.append(song.artist)\n",
    "            self.sequence_to_midi.extend([idx] * len(sequences))\n",
    "            self.sequence_to_artist.extend([idx] * len(sequences))\n",
    "        print(f'Dataset has: {len(self.word_sequences)} sequences and {len(self.sequences_targets)} targets')\n",
    "\n",
    "    \n",
    "    def word_vec(self, tok: str) -> np.ndarray:\n",
    "        # helper to get word vector, or zeros if OOV\n",
    "        v = self.word_embeddings.get(tok)\n",
    "        if v is None:\n",
    "            sample = next(iter(self.word_embeddings.values()))\n",
    "            v = np.zeros_like(sample, dtype=np.float32)\n",
    "        return v.astype(np.float32, copy=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_sequences)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        tokens = self.word_sequences[idx]                # list[str], len T\n",
    "        target_tokens = self.sequences_targets[idx]      # list[str], len T\n",
    "        midi = self.midi_features[self.sequence_to_midi[idx]].astype(np.float32, copy=False)\n",
    "        artist_name = self.sequence_artists[self.sequence_to_artist[idx]]\n",
    "        artist_idx = np.float32(self.artist_to_index[artist_name])\n",
    "\n",
    "        emb = np.stack([self.word_vec(tok) for tok in tokens], axis=0).astype(np.float32, copy=False)     # [T,E]\n",
    "        midi_b = np.broadcast_to(midi, (emb.shape[0], midi.shape[0])).astype(np.float32, copy=False)      # [T,M]\n",
    "        artist_b = np.full((emb.shape[0], 1), artist_idx, dtype=np.float32)                               # [T,1]\n",
    "\n",
    "        concatenated_features = np.concatenate((emb, midi_b, artist_b), axis=1).astype(np.float32, copy=False)                # [T,D]\n",
    "        target_words = np.asarray([self.word_to_id.get(target_token, self.word_to_id.get(UNK_STRING, 0)) for target_token in target_tokens],\n",
    "                   dtype=np.int64)    \n",
    "        return concatenated_features, target_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec874e9",
   "metadata": {},
   "source": [
    "<font size=6>Reading CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1959fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LYRIC_TRAIN_SET_CSV_PATH, mode='r', encoding='utf-8') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    lyric_train_data = list(reader)\n",
    "\n",
    "with open(LYRIC_TEST_SET_CSV_PATH, mode='r', encoding='utf-8') as test_file:\n",
    "    reader = csv.reader(test_file)\n",
    "    lyric_test_data = list(reader)\n",
    "\n",
    "if len(lyric_train_data) < 1 or len(lyric_test_data) < 1:\n",
    "    raise Exception(\"CSV files are empty or not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996afc4a",
   "metadata": {},
   "source": [
    "<font size=6>Parsing CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_data(raw_csv_data: list[list[str]]) -> list[tuple[str, str, list[str]]]:\n",
    "    returned_cleaned_csv_data: list[tuple[str, str, list[str]]] = []\n",
    "    for row in raw_csv_data:\n",
    "        artist = row[0].strip()\n",
    "        title_index = 1\n",
    "        lyrics_index = 2\n",
    "        while lyrics_index < len(row):\n",
    "            title = row[title_index].strip()\n",
    "            title = title.removesuffix('-2') # Remove '-2' suffix if present, relevant in 1 case.\n",
    "            title = row[title_index].strip()\n",
    "            lyrics = row[lyrics_index].strip()\n",
    "            lyrics = lyrics.lower()\n",
    "            lyrics = re.sub(f\"[{re.escape('&')}]\", f\" {EOL_STRING} \", lyrics) # Changing ampersands to eol to indicate end of line.\n",
    "            lyrics = re.sub(f\"[{re.escape('\\'')}]\", \"\", lyrics) # Removing apostrophes.\n",
    "            lyrics = re.sub(f\"[{re.escape('-')}]\", \" \", lyrics) # Removing hyphens.\n",
    "            lyrics = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", lyrics) # Removing punctuation.\n",
    "            lyrics = lyrics.split(' ') # Tokenzing each word by space.\n",
    "            lyrics = [word.strip() for word in lyrics if word] # Removing empty strings.\n",
    "            lyrics.append(EOS_STRING) # Adding end of song token.\n",
    "            if len(title) > 0 and len(lyrics) > 0:\n",
    "                returned_cleaned_csv_data.append((artist, title, lyrics))\n",
    "            title_index += 2\n",
    "            lyrics_index += 2\n",
    "    return returned_cleaned_csv_data\n",
    "\n",
    "cleaned_lyric_train_data = clean_csv_data(lyric_train_data)\n",
    "cleaned_lyric_test_data = clean_csv_data(lyric_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique words in the lyrics\n",
    "def get_word_frequencies(lyrics_data: list[tuple[str, str, list[str]]]) -> dict[str, int]:\n",
    "    words_frequency = defaultdict(int)\n",
    "    for _, _, lyrics in lyrics_data:\n",
    "        for word in lyrics:\n",
    "            words_frequency[word] += 1\n",
    "    return words_frequency    \n",
    "word_frequencies_training: dict[str, int] = get_word_frequencies(cleaned_lyric_train_data)\n",
    "word_frequencies_test: dict[str, int] = get_word_frequencies(cleaned_lyric_test_data)\n",
    "print(f\"Number of unique words in training set: {len(word_frequencies_training)}\")\n",
    "print(f\"Number of unique words in test set: {len(word_frequencies_test)}\")\n",
    "\n",
    "d_sorted_by_val = sorted(word_frequencies_training.items(), key=lambda kv: kv[1], reverse=True)\n",
    "d_sorted_by_val[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141709c5",
   "metadata": {},
   "source": [
    "<font size=6>Reading MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80433ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_files(midi_files_location: str, pickling_path: Optional[str] = None, failed_loads_path: Optional[str] = None) -> \\\n",
    "                    tuple[dict[str, dict[str, PrettyMIDI]], dict[str, set[str]]]: # artist -> title -> PrettyMIDI, failed loads[artist, song_set]\n",
    "    failed_loads = dict()\n",
    "    if failed_loads_path is not None and os.path.isfile(failed_loads_path):\n",
    "        with open(failed_loads_path, \"rb\") as f:\n",
    "            failed_loads = pickle.load(f)\n",
    "        print(f\"Loaded failed MIDI loads from pickled file {failed_loads_path}.\")\n",
    "    if pickling_path is not None and os.path.isfile(pickling_path):\n",
    "        with open(pickling_path, \"rb\") as f:\n",
    "            loaded_midi_files = pickle.load(f)\n",
    "        print(f\"Loaded MIDI files from pickled file {pickling_path}.\")\n",
    "        print(f'Loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.')\n",
    "        return loaded_midi_files, failed_loads\n",
    "    if not os.path.isdir(midi_files_location):\n",
    "        raise ValueError(f\"MIDI file path {midi_files_location} is not a valid directory.\")\n",
    "\n",
    "    # Traversing over all files and attempt to load them with pretty_midi:\n",
    "    loaded_midi_files: dict[str, dict[str, PrettyMIDI]] = defaultdict(dict) # artist -> title -> PrettyMIDI\n",
    "    failed_loads: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    for file in os.listdir(midi_files_location):\n",
    "        if file.endswith('.mid') or file.endswith('.midi'):\n",
    "            file_path = os.path.join(midi_files_location, file)\n",
    "            file = file.removesuffix('.mid')\n",
    "            splitted_artist_and_title = file.split('_-_')\n",
    "            artist = splitted_artist_and_title[0]\n",
    "            title = splitted_artist_and_title[1]\n",
    "            if len(splitted_artist_and_title) > 2:\n",
    "                print(f\"Warning: file {file} has more than one '_-_' separator, ignoring the rest after second \\\"_-_\\\".\")\n",
    "            artist = artist.replace('_', ' ').strip().lower()\n",
    "            title = title.replace('_', ' ').strip().lower()\n",
    "            try:\n",
    "                midi_data = PrettyMIDI(file_path)\n",
    "                loaded_midi_files[artist][title] = midi_data\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file}: {e}\")\n",
    "                failed_loads[artist].add(title)\n",
    "\n",
    "\n",
    "\n",
    "    if failed_loads:\n",
    "        print(\"Failed to load the following artist and lyric midi files:\")\n",
    "        for artist, lyrics in failed_loads.items():\n",
    "            print(f\"{artist} - [{', '.join(lyrics)}]\")\n",
    "\n",
    "    if pickling_path is not None:\n",
    "        with open(pickling_path, \"wb\") as f:\n",
    "            pickle.dump(loaded_midi_files, f)\n",
    "            print(f\"Pickled loaded MIDI files to {pickling_path}.\")\n",
    "    if failed_loads_path is not None:\n",
    "        with open(failed_loads_path, \"wb\") as f:\n",
    "            pickle.dump(failed_loads, f)\n",
    "            print(f\"Pickled failed MIDI loads to {failed_loads_path}.\")\n",
    "\n",
    "    print(f\"Successfully loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.\")\n",
    "    return loaded_midi_files, failed_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a752d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_midi_files, failed_midi_loads = load_midi_files(MIDI_FILE_PATH, PICKLING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ffe14",
   "metadata": {},
   "source": [
    "<font size=6>Mapping CSV data to MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_data_to_songdata_list(csv_data: list[list[str]], \n",
    "                              failed_midi_load: dict[str, set[str]], \n",
    "                              midi_files_dict: dict[str, dict[str, PrettyMIDI]]) -> list[SongData]:\n",
    "    song_data_list: list[SongData] = list()\n",
    "    missing_midi_count = 0\n",
    "    for row in csv_data:\n",
    "        artist = row[0]\n",
    "        title = row[1]\n",
    "        if artist in failed_midi_load and title in failed_midi_load[artist]:\n",
    "            print(f\"Skipping {artist} - {title} due to previous MIDI load failure.\")\n",
    "            continue\n",
    "        if artist in midi_files_dict and title in midi_files_dict[artist]:\n",
    "            midi_file = midi_files_dict[artist][title]\n",
    "            song_data = SongData(row, midi_file)\n",
    "            song_data_list.append(song_data)\n",
    "        else:\n",
    "            missing_midi_count += 1\n",
    "            print(f\"Missing MIDI file for artist '{artist}' and title '{title}'\")\n",
    "    print(f\"Total songs with missing MIDI files: {missing_midi_count}\")\n",
    "    return song_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f061cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_train_data, failed_midi_loads, loaded_midi_files)\n",
    "test_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_test_data, failed_midi_loads, loaded_midi_files)\n",
    "print(f\"Total training songs with MIDI data: {len(train_midi_data)}\")\n",
    "print(f\"Total test songs with MIDI data: {len(test_midi_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0f1d",
   "metadata": {},
   "source": [
    "<font size=6>Handling word embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95f984",
   "metadata": {},
   "source": [
    "Downloading pretrained word2vec, containing 300 dims, trained on news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0482d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bebb8",
   "metadata": {},
   "source": [
    "Extracting the vocabulary from the lyrics.\n",
    "Getting the data from the test set aswell since the vocbulary needs to be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_vocabulary: set[str] = set()\n",
    "# Getting the data from the test set aswell since the vocbulary needs to be known\n",
    "for song in train_midi_data + test_midi_data:\n",
    "    for word in song.lyrics:\n",
    "        lyrics_vocabulary.add(word)\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305abc9",
   "metadata": {},
   "source": [
    "Creating unified embedding.\n",
    "Extracting embeddings from word2vec and using random embeddings for words not found in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_embeddings: dict[str, np.ndarray] = dict()\n",
    "existing_words_in_pretrained = 0\n",
    "not_existing_in_pretrained = 0\n",
    "added_stopwords = 0\n",
    "for word in list(lyrics_vocabulary):\n",
    "    if word in pretrained_word2vec:\n",
    "        unified_embeddings[word] = pretrained_word2vec[word]\n",
    "        existing_words_in_pretrained += 1\n",
    "    else:\n",
    "        unified_embeddings[word] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,)) # Random init for unknown words.  \n",
    "        not_existing_in_pretrained += 1\n",
    "    # Adding stopwords as well, since they are common and should be in the vocabulary.\n",
    "for stopword in stopwords.words('english'):\n",
    "    cleaned_stopword = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", stopword.strip().lower()) # Cleaning the stopword, since it contains punctuation.\n",
    "    if cleaned_stopword not in unified_embeddings:\n",
    "        unified_embeddings[cleaned_stopword] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "        added_stopwords += 1\n",
    "# Adding special tokens\n",
    "unified_embeddings[EOL_STRING] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "unified_embeddings[UNK_STRING] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "unified_embeddings[EOS_STRING] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "unified_embeddings[SONG_BEGINNING_STRING] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")\n",
    "print(f\"Existing words in pretrained embeddings: {existing_words_in_pretrained}\")\n",
    "print(f\"Not existing in pretrained embeddings (randomly initialized): {not_existing_in_pretrained}\")\n",
    "print(f\"Added stopwords (randomly initialized): {added_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b23e9d",
   "metadata": {},
   "source": [
    "<font size=6>Handling artist embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f40eba",
   "metadata": {},
   "source": [
    "Using simple indexing for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_artists = [song.artist for song in train_midi_data]\n",
    "test_artists = [song.artist for song in test_midi_data]\n",
    "artist_set: set = (set(train_artists).union(set(test_artists)))\n",
    "artist_to_index: dict[str, int] = dict()\n",
    "index_to_artist: dict[int, str] = dict()\n",
    "for index, artist in enumerate(artist_set):\n",
    "    artist_to_index[artist] = index\n",
    "    index_to_artist[index] = artist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df1ccb",
   "metadata": {},
   "source": [
    "<font size=6>Load dataset and dataloader</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word_in_vocab: index_of_word for index_of_word, word_in_vocab in enumerate(unified_embeddings.keys())} \n",
    "id_to_word = {index_of_word: word_in_vocab for word_in_vocab, index_of_word in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163de739",
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata_train_dataset = SongDataset(train_midi_data, unified_embeddings, artist_to_index, word_to_id)\n",
    "songdata_test_dataset = SongDataset(train_midi_data, unified_embeddings, artist_to_index, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee962c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = train_test_split(songdata_train_dataset, test_size=VALIDATION_SPLIT, random_state=RANDOM_LOADER_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = data.DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_data_loader = data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_data_loader = data.DataLoader(songdata_test_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfa020",
   "metadata": {},
   "source": [
    "<font size=6>Model 1: Simple concatenation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration Method 1: Simple Concatenation - Melody features are concatenated to each word embedding\n",
    "class LyricsGenerator_Concatenation(nn.Module):\n",
    "  def __init__(self, \n",
    "               vocab_size: int, \n",
    "               input_size: int, \n",
    "               hidden_layer_dim: int, \n",
    "               size_of_midi_features: int = NUMBER_OF_EXTRACT_MIDI_FEATURES,\n",
    "               size_of_word_embeddings: int = WORD_EMBEDDING_SIZE,\n",
    "               num_layers: int = LSTM_LAYERS, \n",
    "               dropout_rate: float = DROPOUT\n",
    "               ):\n",
    "    super(LyricsGenerator_Concatenation, self).__init__()\n",
    "    self.vocab_size: int = vocab_size\n",
    "    self.num_layers: int = num_layers\n",
    "    self.word_embedding_size: int = size_of_word_embeddings\n",
    "    self.size_of_midi_features: int = size_of_midi_features\n",
    "    self.lstm = nn.LSTM(input_size, hidden_layer_dim, num_layers,\n",
    "                        batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    self.fc = nn.Linear(hidden_layer_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    lstm_out, _ = self.lstm(x)\n",
    "    output_post_dropout = self.dropout(lstm_out)\n",
    "    logits = self.fc(output_post_dropout)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a569db",
   "metadata": {},
   "source": [
    "<font size=6>Model 2: Attention</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea633444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsGenerator_Attention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int,\n",
    "                 hidden_layer_dim: int = HIDDEN_LAYER_DIM_ATTENTION,\n",
    "                 size_of_word_embeddings: int = WORD_EMBEDDING_SIZE,\n",
    "                 size_of_midi_features: int = NUMBER_OF_EXTRACT_MIDI_FEATURES,\n",
    "                 size_of_artist_index_dim: int = SIZE_OF_ARTIST_INDEX,\n",
    "                 num_layers: int = 1,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = size_of_word_embeddings      # E\n",
    "        self.melody_dim = size_of_midi_features        # M\n",
    "        self.size_of_artist_index_dim = size_of_artist_index_dim\n",
    "        self.hidden_layer_dim = hidden_layer_dim\n",
    "\n",
    "        # RNN sees [word_emb || midi_feats || artist_scalar]\n",
    "        self.rnn = nn.LSTM(self.embedding_dim + self.melody_dim + self.size_of_artist_index_dim,\n",
    "                           self.hidden_layer_dim, num_layers,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # Attention parts\n",
    "        self.key_proj   = nn.Linear(self.hidden_layer_dim, self.hidden_layer_dim, bias=False)\n",
    "        self.val_proj   = nn.Linear(self.hidden_layer_dim, self.hidden_layer_dim, bias=False)\n",
    "        self.query_proj = nn.Linear(self.hidden_layer_dim + self.melody_dim + self.size_of_artist_index_dim, self.hidden_layer_dim, bias=False)\n",
    "\n",
    "        self.combine_proj = nn.Linear(self.hidden_layer_dim * 2, self.hidden_layer_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(self.hidden_layer_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor):            # x: [B, T, E+M+1] = [emb || midi || artist_scalar]\n",
    "        input = input.float()\n",
    "        batch_dim, token_dim, feature_dim = input.shape\n",
    "        embedding_dim, melody_dim = self.embedding_dim, self.melody_dim\n",
    "        assert feature_dim == (embedding_dim + melody_dim + self.size_of_artist_index_dim), \"Input last dim must be E+M+1 (emb+midi+artist).\"\n",
    "\n",
    "        # Split fields\n",
    "        word_emb = input[..., :embedding_dim]                      # [B,T,E]\n",
    "        midi     = input[..., embedding_dim:embedding_dim+melody_dim]                   # [B,T,M]\n",
    "        artist   = input[..., embedding_dim+melody_dim:embedding_dim+melody_dim+self.size_of_artist_index_dim]               # [B,T,1]\n",
    "\n",
    "        # RNN input\n",
    "        rnn_in = torch.cat([word_emb, midi, artist], dim=-1)  # [B,T,E+M+1]\n",
    "        rnn_out, (h_n, _) = self.rnn(rnn_in)                  # rnn_out: [B,T,H]\n",
    "        h_last = h_n[-1]                                      # [B,H]\n",
    "\n",
    "        # Query conditioning from sequence-level side info\n",
    "        cond_midi   = midi.mean(dim=1)            # [B,M]\n",
    "        cond_artist = artist.mean(dim=1)          # [B,1]\n",
    "        q_inp = torch.cat([h_last, cond_midi, cond_artist], dim=-1)  # [B,H+M+1]\n",
    "        q = self.query_proj(q_inp).unsqueeze(1)   # [B,1,H]\n",
    "\n",
    "        # Attention over time\n",
    "        K = self.key_proj(rnn_out)                # [B,T,H]\n",
    "        V = self.val_proj(rnn_out)                # [B,T,H]\n",
    "        scores = torch.bmm(q, K.transpose(1, 2)).squeeze(1) / math.sqrt(self.hidden_layer_dim)  # [B,T]\n",
    "        attn = F.softmax(scores, dim=-1)          # [B,T]\n",
    "        context = torch.bmm(attn.unsqueeze(1), V).squeeze(1)  # [B,H]\n",
    "\n",
    "        # Output\n",
    "        combined = torch.tanh(self.combine_proj(torch.cat([context, h_last], dim=-1)))  # [B,H]\n",
    "        logits = self.fc(self.dropout(combined))  # [B,V]\n",
    "\n",
    "        return logits, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02346871",
   "metadata": {},
   "source": [
    "<font size=6>Running the models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8b7b3",
   "metadata": {},
   "source": [
    "Helper function for the attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_logits(model, x):                      # x: [B,T,D]\n",
    "    B, T, _ = x.shape\n",
    "    outs = []\n",
    "    for t in range(1, T):\n",
    "        logits_t, _ = model(x[:, :t, :])     # prefix up to t-1 predicts token t\n",
    "        outs.append(logits_t.unsqueeze(1))   # [B,1,V]\n",
    "    return torch.cat(outs, dim=1)            # [B,T-1,V]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff6e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_logits(model, model_input):  # returns [B, T-1, V] for both model types\n",
    "    if isinstance(model, LyricsGenerator_Attention):\n",
    "        return tf_logits(model, model_input)               # [B,T-1,V]\n",
    "    elif isinstance(model, LyricsGenerator_Concatenation):\n",
    "        return model(model_input)[:, :-1, :]               # [B,T-1,V] from [B,T,V]\n",
    "    else:\n",
    "        raise ValueError(f'Model not recognized: {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02754ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: data.DataLoader, \n",
    "                val_loader: data.DataLoader, \n",
    "                test_loader: data.DataLoader,\n",
    "                num_epochs: int = MAX_EPOCHS, \n",
    "                learning_rate: float = LEARNING_RATE,\n",
    "                patiance_factor: float = PATIANCE_FACTOR,\n",
    "                patiance_epochs: int = PATIANCE_EPOCHS):\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patiance_epochs)\n",
    "    best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "    train_losses: list[float] = list()\n",
    "    val_losses: list[float] = list()\n",
    "    test_losses: list[float] = list()\n",
    "    best_validation_loss: float = 10000.0\n",
    "    epochs_with_no_improvements: int = 0\n",
    "    writer = SummaryWriter()  # TensorBoard writer\n",
    "    vocabulary_size = model.vocab_size\n",
    "    for epoch in range(num_epochs):\n",
    "        current_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        batch_num: int = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device=device, dtype=torch.float32)\n",
    "            targets = targets.to(device=device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            logits = seq_logits(model, inputs)\n",
    "            loss = criterion(\n",
    "                logits.contiguous().view(-1, vocabulary_size),\n",
    "                targets[:, 1:].contiguous().view(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            batch_num += 1\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)  # TensorBoard\n",
    "        model.eval()\n",
    "        validation_running_loss = 0.0\n",
    "        test_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs  = val_inputs.to(device=device, dtype=torch.float32)\n",
    "                val_targets = val_targets.to(device=device, dtype=torch.long)\n",
    "                val_logits = seq_logits(model, val_inputs)         # [B,T-1,V]\n",
    "                val_loss = criterion(\n",
    "                    val_logits.contiguous().view(-1, vocabulary_size),\n",
    "                    val_targets[:, 1:].contiguous().view(-1)\n",
    "                )\n",
    "                validation_running_loss += val_loss.item() * val_inputs.size(0)\n",
    "            val_epoch_loss = validation_running_loss / len(val_loader.dataset)\n",
    "            val_losses.append(val_epoch_loss)\n",
    "            test_running_loss = 0.0\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                test_inputs  = test_inputs.to(device=device, dtype=torch.float32)\n",
    "                test_targets = test_targets.to(device=device, dtype=torch.long)\n",
    "\n",
    "                test_logits = seq_logits(model, test_inputs)          # [B,T-1,V]\n",
    "                test_loss = criterion(\n",
    "                    test_logits.contiguous().view(-1, vocabulary_size),\n",
    "                    test_targets[:, 1:].contiguous().view(-1)\n",
    "                )\n",
    "                test_running_loss += test_loss.item() * test_inputs.size(0)\n",
    "        scheduler.step(val_epoch_loss)\n",
    "        test_epoch_loss = test_running_loss / len(test_loader.dataset)\n",
    "        test_losses.append(test_epoch_loss)\n",
    "        if  best_validation_loss - val_epoch_loss >= patiance_factor:\n",
    "            epochs_with_no_improvements = 0\n",
    "            best_validation_loss = val_epoch_loss\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_with_no_improvements += 1\n",
    "            print(f'No improvement in epoch. Patiance: {epochs_with_no_improvements}\\\\{patiance_epochs}')\n",
    "        finish_time = time.time() - current_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, Test Loss: {test_epoch_loss:.4f}, time: {finish_time}')\n",
    "        if epochs_with_no_improvements >= patiance_epochs:\n",
    "            print(f'Training ended prematurely due to lack of improvement.')\n",
    "            break\n",
    "    writer.close()  # Close TensorBoard writer\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    model.eval()\n",
    "    return model, train_losses, val_losses, test_losses\n",
    "\n",
    "# After training, run in terminal to view TensorBoard:\n",
    "# !tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8918888",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_model = LyricsGenerator_Attention(\n",
    "    vocab_size=len(songdata_train_dataset.word_embeddings),\n",
    "    size_of_word_embeddings=WORD_EMBEDDING_SIZE,\n",
    "    size_of_midi_features=NUMBER_OF_EXTRACT_MIDI_FEATURES,\n",
    "    size_of_artist_index_dim=SIZE_OF_ARTIST_INDEX,\n",
    "    hidden_layer_dim=HIDDEN_LAYER_DIM_ATTENTION,\n",
    ")\n",
    "\n",
    "attention_model, training_loss, validation_loss, test_loss = train_model(attention_model, \n",
    "            training_data_loader, \n",
    "            validation_data_loader, \n",
    "            test_data_loader,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78fa66",
   "metadata": {},
   "source": [
    "<font size=5>Running model 1: Concatenation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b319f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LyricsGenerator_Concatenation(\n",
    "    vocab_size=len(songdata_train_dataset.word_embeddings),\n",
    "    input_size=pretrained_word2vec.vector_size + NUMBER_OF_EXTRACT_MIDI_FEATURES + 1, # word embedding + melody features + artist index\n",
    "    hidden_layer_dim=HIDDEN_LAYER_DIM,\n",
    "    num_layers=LSTM_LAYERS,\n",
    "    dropout_rate=DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_loss, validation_loss, test_loss = train_model(model, \n",
    "            training_data_loader, \n",
    "            validation_data_loader, \n",
    "            test_data_loader,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27710111",
   "metadata": {},
   "source": [
    "Displaying tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e702053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TODO, MAKE THIS WORK LOL')\n",
    "print('TODO, MAKE THIS WORK LOL')\n",
    "print('TODO, MAKE THIS WORK LOL')\n",
    "print('TODO, MAKE THIS WORK LOL')\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd43df",
   "metadata": {},
   "source": [
    "<font size=6>Generating Lyrics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6c6c5",
   "metadata": {},
   "source": [
    "A function that returns the k most likely words given the input, used for coherent lyric generatin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf15b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_next_word(\n",
    "    model: nn.Module,\n",
    "    word_sequence: list[str],\n",
    "    artist_index: int,\n",
    "    melody_vec,\n",
    "    word_to_id: dict[str, int],\n",
    "    id_to_word: dict[int, str],\n",
    "    embedding_weight: torch.Tensor,\n",
    "    device: str = \"cpu\",\n",
    "    forbidden_words: list[str] | None = None,\n",
    "    strengthened_words: list[tuple[str, float]] | None = None,\n",
    "):\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # embedding matrix on device\n",
    "    if embedding_weight.device.type != device:\n",
    "        embedding_weight = embedding_weight.to(device)\n",
    "\n",
    "    # melody -> [M]\n",
    "    melody_vec = torch.as_tensor(melody_vec, dtype=torch.float32, device=device)\n",
    "\n",
    "    # seq -> ids -> embeddings\n",
    "    unk_id = word_to_id.get(UNK_STRING, UNK_ID)\n",
    "    seq_ids = torch.tensor([word_to_id.get(w, unk_id) for w in word_sequence],\n",
    "                           device=device, dtype=torch.long)              # [T]\n",
    "    seq_embs = embedding_weight[seq_ids]                                  # [T, E]\n",
    "\n",
    "    T = seq_embs.size(0)\n",
    "    # broadcast melody + artist\n",
    "    melody_broadcast = melody_vec.expand(T, -1)                           # [T, M]\n",
    "    # NOTE: your modelâ€™s artist dim must equal size_of_artist_index_dim; here we use 1\n",
    "    artist_broadcast = torch.full((T, 1), float(artist_index),\n",
    "                                  dtype=torch.float32, device=device)     # [T, 1]\n",
    "\n",
    "    # concat â†’ [1, T, E+M+artist_dim]\n",
    "    x = torch.cat([seq_embs, melody_broadcast, artist_broadcast], dim=1).unsqueeze(0)  # [1,T,D]\n",
    "\n",
    "    # forward\n",
    "    out = model(x)\n",
    "    if isinstance(model, LyricsGenerator_Attention):\n",
    "        logits = out[0]         # attention model: [1, V]\n",
    "    elif isinstance(model, LyricsGenerator_Concatenation):\n",
    "        logits = out[:, -1, :]  # [1, V]\n",
    "    else:\n",
    "        raise ValueError(f\"Model: {model} is not recognized.\")\n",
    "    \n",
    "    logits = logits.squeeze(0)  # [V]\n",
    "    # forbid\n",
    "    if forbidden_words:\n",
    "        forb_idx = [word_to_id[w] for w in forbidden_words if w in word_to_id]\n",
    "        if forb_idx:\n",
    "            logits[torch.tensor(forb_idx, device=device, dtype=torch.long)] = float(\"-inf\")\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)  # [V]\n",
    "\n",
    "    # strengthen: set explicit probabilities for selected words\n",
    "    if strengthened_words:\n",
    "        pairs = [(w, float(p)) for (w, p) in strengthened_words if (w in word_to_id) and (p >= 0.0)]\n",
    "        if pairs:\n",
    "            idxs = torch.tensor([word_to_id[w] for (w, _) in pairs], device=device, dtype=torch.long)\n",
    "            p_desired = torch.tensor([min(p, 1.0) for (_, p) in pairs], device=device, dtype=probs.dtype)\n",
    "            # aggregate dupes\n",
    "            uniq, inv = torch.unique(idxs, return_inverse=True)\n",
    "            p_agg = torch.zeros_like(uniq, dtype=probs.dtype).scatter_add(0, inv, p_desired)\n",
    "\n",
    "            base = probs.clone()\n",
    "            base[uniq] = 0.0\n",
    "            base_sum = base.sum()\n",
    "            sum_p = p_agg.sum()\n",
    "\n",
    "            if float(sum_p) >= 1.0 - 1e-8 or base_sum <= 1e-12:\n",
    "                probs = torch.zeros_like(probs)\n",
    "                probs[uniq] = p_agg / (sum_p + 1e-12)\n",
    "            else:\n",
    "                remain = 1.0 - float(sum_p)\n",
    "                base = base * (remain / (base_sum + 1e-12))\n",
    "                probs = base\n",
    "                probs[uniq] = p_agg\n",
    "\n",
    "    idx = torch.multinomial(probs, num_samples=1, replacement=True)\n",
    "    return id_to_word[int(idx.item())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fcbb1",
   "metadata": {},
   "source": [
    "Printing the generated text and handling tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e527c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generated_lyrics(generated_lyrics: list[str]):\n",
    "    capitalize = True\n",
    "    for word in generated_lyrics:\n",
    "        if word == EOL_STRING:\n",
    "            capitalize = True\n",
    "            print()\n",
    "        if word == EOS_STRING:\n",
    "            break\n",
    "        if word != EOL_STRING:\n",
    "            if capitalize:\n",
    "                capitalize = False\n",
    "                print(word.title(), end=' ')\n",
    "            else:\n",
    "                print(word, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239b529",
   "metadata": {},
   "source": [
    "Generating the lyrics and maintaining the lyrics generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3392cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(\n",
    "        model_to_use: nn.Module,\n",
    "        initial_word: str,\n",
    "        melody_features: np.ndarray,\n",
    "        melody_title: str,\n",
    "        artist_to_use: str,\n",
    "        word_to_id: dict[str, int],\n",
    "        id_to_word: dict[int, str],\n",
    "        artist_to_index: dict[str, int],\n",
    "        word_embeddings: dict[str, np.ndarray],\n",
    "        max_song_length: int = MAX_SONG_LENGTH_WORDS,\n",
    "        sequence_length: int = SEQUENCE_LENGTH,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates lyrics word by word using the model, melody, and artist.\n",
    "    Picks next word randomly from top_k candidates according to their normalized probabilities.\n",
    "    Artificially increases probability of EOS_STRING after half of max_song_length.\n",
    "    Prints the generated lyrics with line breaks at <eol>.\n",
    "    Enforces some more grammatical rules.\n",
    "    Returns: generated_lyrics (list of str), artist, melody_title.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating song from initial word: '{initial_word}', melody: '{melody_title}', artist: '{artist_to_use}', max length: {max_song_length}\")\n",
    "    melody_vec = torch.as_tensor(melody_features['vector'], dtype=torch.float32, device=device)\n",
    "    artist_idx = artist_to_index.get(artist_to_use, -1)\n",
    "    if artist_idx == -1:\n",
    "        print(f\"Warning: Artist '{artist_to_use}' not found, using index -1.\")\n",
    "\n",
    "    embedding_weight = torch.from_numpy(\n",
    "        np.stack([word_embeddings[w].astype(np.float32) for w in word_to_id], axis=0)\n",
    "    ).to(device)\n",
    "    context: deque = deque()\n",
    "    context.extendleft([UNK_STRING for _ in range(sequence_length - 1)])\n",
    "    context.appendleft(initial_word)\n",
    "    unk_index: int = 1\n",
    "    generated_lyrics = [initial_word]\n",
    "    words_in_song: int = 0\n",
    "    current_word: str = initial_word\n",
    "    minimum_song_length = int(max_song_length / 2)\n",
    "    current_words_in_line: int = 1\n",
    "    current_word = \"\"\n",
    "    next_word = \"\"\n",
    "    words_not_to_end_lines_on: list[str] = ['the']\n",
    "    while True:\n",
    "        # Tries to enforce certain rules.\n",
    "        # Don't allow end of song before minimum amount of lines.\n",
    "        # Don't repeat the same word twice\n",
    "        # Don't allow lines that are too short.\n",
    "        # Don't end lines on words in a way that would make no sense.\n",
    "        forbidden_words = [current_word]\n",
    "        strengthened_words = list()\n",
    "        if words_in_song < int(minimum_song_length):\n",
    "            forbidden_words.append(EOS_STRING)\n",
    "        if current_word in words_not_to_end_lines_on:\n",
    "            forbidden_words.extend([EOL_STRING, EOS_STRING])\n",
    "        if current_words_in_line < MIN_LINE_LENGTH:\n",
    "            forbidden_words.append(EOL_STRING)\n",
    "        if current_words_in_line > int(MAX_LINE_LENGTH / 2):\n",
    "            probability_of_eol: float = min((current_words_in_line - int(MAX_LINE_LENGTH / 2))/ int(MAX_LINE_LENGTH/2), 1.0)\n",
    "            strengthened_words.append((EOL_STRING, probability_of_eol))\n",
    "        if words_in_song > int(MAX_SONG_LENGTH_WORDS / 2):\n",
    "            probability_of_eos: float = min((words_in_song - int(MAX_SONG_LENGTH_WORDS / 2))/ int(MAX_SONG_LENGTH_WORDS/2), 1.0)\n",
    "            strengthened_words.append((EOS_STRING, probability_of_eos))\n",
    "        next_word = predict_next_word(\n",
    "            model=model_to_use,\n",
    "            word_sequence=context,\n",
    "            artist_index=artist_idx,\n",
    "            melody_vec=melody_vec,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            embedding_weight=embedding_weight,\n",
    "            forbidden_words=forbidden_words,\n",
    "            strengethened_words=strengthened_words,\n",
    "            device=device,\n",
    "        )\n",
    "        if next_word == EOS_STRING and current_word == EOL_STRING:\n",
    "            generated_lyrics[-1] = next_word # In case an end of song comes after linebreak, just end the song instead.\n",
    "        else:\n",
    "            generated_lyrics.append(next_word)\n",
    "        current_word = next_word\n",
    "        if unk_index < sequence_length:\n",
    "            context[unk_index] = next_word\n",
    "            unk_index += 1\n",
    "        else:\n",
    "            context.popleft()\n",
    "            context.append(next_word)\n",
    "        if next_word == EOS_STRING:\n",
    "            break\n",
    "        if words_in_song >= max_song_length:\n",
    "            generated_lyrics.append(EOS_STRING)\n",
    "            break\n",
    "        if next_word != EOL_STRING:\n",
    "            words_in_song += 1\n",
    "            current_words_in_line += 1\n",
    "        else:\n",
    "            current_words_in_line = 0\n",
    "    print_generated_lyrics(generated_lyrics=generated_lyrics)\n",
    "    print(f'Number of words in lyrics: {words_in_song}')\n",
    "    print(\"\\n--- End of generated lyrics ---\")\n",
    "    return generated_lyrics, artist_to_use, melody_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acea986",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_to_use = train_midi_data[63]\n",
    "\n",
    "lyrics, artist, melody = generate_lyrics(\n",
    "    model_to_use=model,\n",
    "    initial_word=\"eyes\",\n",
    "    melody_features=song_to_use.midi_features,\n",
    "    melody_title=song_to_use.title,\n",
    "    artist_to_use='billy joel',\n",
    "    word_to_id=word_to_id,\n",
    "    id_to_word=id_to_word,\n",
    "    artist_to_index=artist_to_index,\n",
    "    word_embeddings=unified_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b6897",
   "metadata": {},
   "source": [
    "<font size=6>Section 7, testing with the testing set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949fa72",
   "metadata": {},
   "source": [
    "For each melody, the output of the architecture given the melody and the initial word of the real lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_song in test_midi_data:\n",
    "    print('--------------------------------')\n",
    "    test_song: SongData\n",
    "    lyrics, artist, melody = generate_lyrics(\n",
    "        model_to_use=model,\n",
    "        initial_word=test_song.lyrics[0],\n",
    "        melody_features=test_song.midi_features,\n",
    "        melody_title=test_song.title,\n",
    "        artist_to_use=test_song.artist,\n",
    "        word_to_id=word_to_id,\n",
    "        id_to_word=id_to_word,\n",
    "        artist_to_index=artist_to_index,\n",
    "        word_embeddings=unified_embeddings,\n",
    "        max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "    )\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92433b27",
   "metadata": {},
   "source": [
    "For each melody, the output of the architecture given the melody and different starting words. The same word should be used for all melodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0dea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_words: list[str] = ['love', 'baby', 'time']\n",
    "\n",
    "for word in starting_words:\n",
    "    print(f'-------------Initial Word Selected: {word}-------------------')\n",
    "    for test_song in test_midi_data:\n",
    "        print('--------------------------------')\n",
    "        test_song: SongData\n",
    "        lyrics, artist, melody = generate_lyrics(\n",
    "            model_to_use=model,\n",
    "            initial_word=word,\n",
    "            melody_features=test_song.midi_features,\n",
    "            melody_title=test_song.title,\n",
    "            artist_to_use=test_song.artist,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            artist_to_index=artist_to_index,\n",
    "            word_embeddings=unified_embeddings,\n",
    "            max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "        )\n",
    "        print('--------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
