{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd41e9a8",
   "metadata": {},
   "source": [
    "<font size=6>Downloading required libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q numpy\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip3 install torch torchsummary\n",
    "!pip3 install torch tensorboard\n",
    "!pip3 install -q pretty_midi\n",
    "!pip3 install -q gensim\n",
    "!pip3 install -q nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4068e6c",
   "metadata": {},
   "source": [
    "<font size=6>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import gdown\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "import csv\n",
    "import string\n",
    "from pretty_midi import PrettyMIDI, Note\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359ed8",
   "metadata": {},
   "source": [
    "<font size=6>Constants</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60457569",
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRIC_TRAIN_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_train_set.csv')\n",
    "LYRIC_TEST_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_test_set.csv')\n",
    "MIDI_FILE_PATH: str = os.path.join(os.getcwd(), 'data', 'midi_files')\n",
    "PICKLING_PATH: str = os.path.join(os.getcwd(), 'loaded_midi_files.pkl') # Path to save/load pickled MIDI files, for faster loading.\n",
    "EPSILON: float = 1e-9\n",
    "SEQUENCE_LENGTH: int = 10  # Number of words in the input sequence\n",
    "BATCH_SIZE: int = 128\n",
    "LSTM_LAYERS: int = 2\n",
    "DROPOUT: float = 0.3\n",
    "RANDOM_LOADER_SEED: int = 42\n",
    "VALIDATION_SPLIT: float = 0.1\n",
    "LEARNING_RATE: float = 0.001\n",
    "MAX_EPOCHS: int = 50\n",
    "NUMBER_OF_EXTRACT_MIDI_FEATURES: int = 33\n",
    "WORD_EMBEDDING_SIZE: int = 300\n",
    "PATIANCE_FACTOR: float = 0.001\n",
    "PATIANCE_EPOCHS: int = 10\n",
    "UNK_ID: int = 0\n",
    "MIN_LINE_LENGTH: int = 5\n",
    "MAX_LINE_LENGTH: int = SEQUENCE_LENGTH\n",
    "EOL_STRING: str = 'eol'\n",
    "UNK_STRING: str = 'unk'\n",
    "EOS_STRING: str = '<eos>'\n",
    "TOP_K_WORDS_TO_PREDICT: int = 20\n",
    "MAX_SONG_LENGTH_WORDS: int = 80\n",
    "HIDDEN_LAYER_DIM: int = 256\n",
    "DEFAULT_MIDI_SCALE: float = 4.0\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1947ce",
   "metadata": {},
   "source": [
    "<font size=6>Midi Feature extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083e542",
   "metadata": {},
   "source": [
    "Auxlliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_time_signature(changes: tuple[int, int]) -> tuple[int, int]:\n",
    "    if not changes: return (4, 4)\n",
    "    pairs: list[tuple[int, int]] = [(ts.numerator, ts.denominator) for ts in changes]\n",
    "    return Counter(pairs).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2526597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_weighted_pitch_stats(notes: list[Note]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute pitch statistics weighted by each note's duration.\n",
    "    Returns a dict with:\n",
    "      mean (duration-weighted),\n",
    "      std  (duration-weighted),\n",
    "      p10 / p50 / p90 (duration-weighted percentiles),\n",
    "      ambitus = p90 - p10 (robust range).\n",
    "    If `notes` is empty, returns safe defaults.\n",
    "    \"\"\"\n",
    "    # Empty guard: nothing to measure → return neutral stats.\n",
    "    if not notes:\n",
    "        return dict(mean=0.0, std=0.0, p10=-1, p50=-1, p90=-1, ambitus=0.0)\n",
    "\n",
    "    # Vectorize pitches as float for math (MIDI 0..127, but floats simplify ops).\n",
    "    pitches: np.ndarray[np.float32] = np.fromiter((n.pitch for n in notes), dtype=np.float32)\n",
    "\n",
    "    # Each note's weight = its duration in seconds; clamp tiny/negative to epsilon.\n",
    "    weights: np.ndarray[np.float32] = np.fromiter((max(EPSILON, n.end - n.start) for n in notes), dtype=np.float32)\n",
    "    total_weights: float = weights.sum()\n",
    "    duration_weight_mean: float = float((weights * pitches).sum() / total_weights)\n",
    "    duration_weighted_variance: float = float((weights * (pitches - duration_weight_mean) ** 2).sum() / total_weights)\n",
    "    weighted_std: float = duration_weighted_variance ** 0.5\n",
    "\n",
    "    # ---------- Duration-weighted percentiles ----------\n",
    "    order: np.ndarray[np.int32] = np.argsort(pitches)\n",
    "    ordered_pitches, ordered_weights = pitches[order], weights[order]\n",
    "    cumulative_weight_sum: np.ndarray[np.float32] = np.cumsum(ordered_weights)\n",
    "\n",
    "    # Weighted quantile: find the first index where cumulative weight crosses q%.\n",
    "    def weighted_quantile(quantile: float) -> float:\n",
    "        # Target cumulative weight at quantile q (0..100).\n",
    "        target: float = (quantile / 100.0) * cumulative_weight_sum[-1]\n",
    "        # Index where cumulative_weight_sum >= target; take leftmost to be consistent.\n",
    "        idx: int = np.searchsorted(cumulative_weight_sum, target, side=\"left\")\n",
    "        return float(ordered_pitches[min(idx, len(ordered_pitches) - 1)])\n",
    "\n",
    "    # 10th / 50th (median) / 90th percentiles, duration-weighted.\n",
    "    percentile_10, percentile_50, percentile_90 = weighted_quantile(10), weighted_quantile(50), weighted_quantile(90)\n",
    "\n",
    "    # Ambitus = robust spread (p90 - p10), less sensitive than raw max - min.\n",
    "    return dict(mean=duration_weight_mean, \n",
    "                std=weighted_std,\n",
    "                p10=percentile_10, \n",
    "                p50=percentile_50, \n",
    "                p90=percentile_90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66999304",
   "metadata": {},
   "source": [
    "Extracting high level features relating to the entire song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85395762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_midi_features(midi: PrettyMIDI) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return song-level features (vector+names).\n",
    "    \"\"\" \n",
    "    duration_sec: float = midi.get_end_time()                                       # total length\n",
    "    if duration_sec <= 0: raise ValueError(\"Empty/zero-length MIDI.\")        # guard\n",
    "\n",
    "    tempo_times, tempo_bpms = midi.get_tempo_changes()                               # tempo changes\n",
    "    if len(tempo_bpms) == 0:                                                     # no changes\n",
    "        tempo_times = np.array([0.0], dtype=np.float32)                          # start time\n",
    "        tempo_bpms = np.array([midi.estimate_tempo()], dtype=np.float32)         # single bpm\n",
    "    segment_ends: np.ndarray[np.float32] = np.r_[tempo_times[1:], duration_sec]                              # segment ends\n",
    "    segment_durs: np.ndarray[np.float32] = np.maximum(1e-6, segment_ends - tempo_times[:len(segment_ends)])          # segment durations\n",
    "    tempo_mean: float = float(np.dot(tempo_bpms[:len(segment_durs)], segment_durs) / np.sum(segment_durs)) # duration-weighted mean\n",
    "    tempo_std: float = float(np.std(np.repeat(\n",
    "        tempo_bpms[:len(segment_durs)],                                              # repeat bpm by\n",
    "        np.maximum(1, (segment_durs/np.sum(segment_durs)*1000).astype(int))          # rough weights\n",
    "    )))                                                                       # dispersion proxy\n",
    "    tempo_change_count: int = int(len(tempo_bpms))                                     # number of states\n",
    "\n",
    "    time_signature_numerator, time_signature_denominator = most_common_time_signature(midi.time_signature_changes)  # mode time sig\n",
    "    instrument_count: int = sum(1 for inst in midi.instruments                     # non-drum count\n",
    "                          if not inst.is_drum and inst.notes)\n",
    "\n",
    "    instruments: list = [inst for inst in midi.instruments if not inst.is_drum and inst.notes]\n",
    "    instruments_velocities: list[float] = []\n",
    "    instrument_notes: list[Note] = []\n",
    "    for instrument in instruments:                                 # melody track\n",
    "        mel_velocity = [note.velocity for note in instrument.notes]     # melody velocities\n",
    "        instruments_velocities.extend(mel_velocity)\n",
    "        instrument_notes.extend(instrument.notes)\n",
    "\n",
    "    instrument_velocities_min: float = min(instruments_velocities)      # min pitch\n",
    "    instrument_velocities_max: float = max(instruments_velocities)      # max pitch\n",
    "    instrument_velocities_mean: float = np.mean(instruments_velocities)    # mean pitch\n",
    "    instrument_velocities_std: float = np.std(instruments_velocities)     # std pitch\n",
    "\n",
    "    duration_weight_pitch_stats_dict: dict = get_duration_weighted_pitch_stats(instrument_notes)\n",
    "    instrument_pitch_10_percentile: float = duration_weight_pitch_stats_dict['p10']\n",
    "    instrument_pitch_50_percentile: float = duration_weight_pitch_stats_dict['p50']\n",
    "    instrument_pitch_90_percentile: float = duration_weight_pitch_stats_dict['p90']\n",
    "    instrument_pitch_mean: float = duration_weight_pitch_stats_dict['mean']\n",
    "    instrument_pitch_std: float = duration_weight_pitch_stats_dict['std']\n",
    "    instrument_pitch_range_by_percentiles: float = instrument_pitch_90_percentile - instrument_pitch_10_percentile\n",
    "\n",
    "    note_durations: list[float] = [note.end - note.start for note in instrument_notes]\n",
    "    note_durations_mean: float = np.mean(note_durations) if note_durations else 0.0\n",
    "    note_durations_std: float = np.std(note_durations) if note_durations else 0.0\n",
    "    note_durations_range: float = max(note_durations) - min(note_durations) if note_durations else 0.0\n",
    "\n",
    "    note_density: float = float(len(instrument.notes) / max(EPSILON, duration_sec))           # notes/sec\n",
    "\n",
    "    chroma_global = midi.get_pitch_class_histogram(use_duration=True)        # 12-bin chroma\n",
    "    chroma_global = chroma_global / (np.sum(chroma_global) + EPSILON)           # normalize\n",
    "    names = [                                                                # feature names\n",
    "        \"duration_sec\",\n",
    "        \"tempo_mean_bpm\", \n",
    "        \"tempo_std_bpm\", \n",
    "        \"tempo_change_count\",\n",
    "        \"time_sig_num\", \n",
    "        \"time_sig_den\",\n",
    "        \"instrument_count\",\n",
    "        \"instrument_velocities_min\", \n",
    "        \"instrument_velocities_max\", \n",
    "        \"instrument_velocities_mean\", \n",
    "        \"instrument_velocities_std\", \n",
    "        \"instrument_pitch_10_percentile\",\n",
    "        \"instrument_pitch_50_percentile\",\n",
    "        \"instrument_pitch_90_percentile\",\n",
    "        \"instrument_pitch_mean\",\n",
    "        \"instrument_pitch_std\",\n",
    "        \"instrument_pitch_range_by_percentiles\",\n",
    "        \"note_durations_mean\",\n",
    "        \"note_durations_std\",\n",
    "        \"note_durations_range\",\n",
    "        \"melody_note_density_per_sec\",\n",
    "    ] + [f\"chroma_{i}\" for i in range(12)]                                   # chroma names\n",
    "    vec = np.array([                                                         # feature vector\n",
    "        duration_sec, \n",
    "        tempo_mean, \n",
    "        tempo_std, \n",
    "        tempo_change_count,\n",
    "        time_signature_numerator, \n",
    "        time_signature_denominator,   \n",
    "        instrument_count,\n",
    "        instrument_velocities_min, \n",
    "        instrument_velocities_max, \n",
    "        instrument_velocities_mean, \n",
    "        instrument_velocities_std, \n",
    "        instrument_pitch_10_percentile,\n",
    "        instrument_pitch_50_percentile,\n",
    "        instrument_pitch_90_percentile,\n",
    "        instrument_pitch_mean,\n",
    "        instrument_pitch_std,\n",
    "        instrument_pitch_range_by_percentiles,\n",
    "        note_durations_mean,\n",
    "        note_durations_std,\n",
    "        note_durations_range,   \n",
    "        note_density,\n",
    "        *chroma_global.tolist()\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return {\"vector\": vec, \"names\": names} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c68e2f",
   "metadata": {},
   "source": [
    "<font size=6>Auxlilliary Data Structures</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f7e0b",
   "metadata": {},
   "source": [
    "Auxilliary functions for creation of word sequences and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ba93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_sequences_with_targets(tokenized_lyrics: list[str], sequence_length: int = SEQUENCE_LENGTH) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given tokenized lyrics as a list of strings, create sequences of word indices and their corresponding target word indices.\n",
    "    Each sequence is of length `sequence_length`, and the target is the next word following the sequence.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Create sequences and targets\n",
    "    for i in range(len(tokenized_lyrics) - sequence_length):\n",
    "        seq = tokenized_lyrics[i:i + sequence_length]\n",
    "        target = tokenized_lyrics[i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return sequences,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongData:\n",
    "    def __init__(self, song_data_cell: list[str] = None, midi_file: PrettyMIDI = None):\n",
    "        if len(song_data_cell) != 3:\n",
    "            raise ValueError(\"song_data_cell must have exactly three elements: [artist, title, lyrics]\")\n",
    "        self.artist = song_data_cell[0]\n",
    "        self.title = song_data_cell[1]\n",
    "        self.lyrics = song_data_cell[2]\n",
    "        self.midi_data = midi_file\n",
    "        self._midi_features: Optional[dict[str, np.ndarray]] = None\n",
    "\n",
    "    @property\n",
    "    def midi_features(self):\n",
    "        if self._midi_features is None:\n",
    "            self._midi_features = extract_midi_features(self.midi_data)\n",
    "        return self._midi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8efdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataset(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 songs_data: list[SongData],\n",
    "                 word_embeddings: dict[str, np.ndarray],\n",
    "                 artist_to_index: dict[str, int]):\n",
    "        self.midi_features: list[np.ndarray] = list()\n",
    "        self.artists: list[str] = list()\n",
    "        self.sequence_artists: list[str] = list()\n",
    "        self.word_sequences: list[str] = list()\n",
    "        self.sequences_targets: list[str] = list()\n",
    "        self.sequence_to_midi: list[int] = list() # Maps each sequence to its corresponding MIDI feature index \n",
    "        self.sequence_to_artist: list[int] = list() # Maps each sequence to its corresponding artist embedding index\n",
    "        self.word_embeddings: dict[str, np.ndarray] = word_embeddings\n",
    "        self.artist_to_index: dict[str, np.ndarray] = artist_to_index\n",
    "        # Instead of saving each sequence's MIDI features, we save the index of the MIDI features in the midi_features list to save space.\n",
    "        for idx, song in enumerate(songs_data):\n",
    "            sequences, targets = create_word_sequences_with_targets(song.lyrics)\n",
    "            self.word_sequences.extend(sequences)\n",
    "            self.sequences_targets.extend(targets)\n",
    "            self.midi_features.append(song.midi_features['vector']) # Creates a mapping of the features to the sequences.\n",
    "            self.sequence_artists.append(song.artist)\n",
    "            self.sequence_to_midi.extend([idx] * len(sequences))\n",
    "            self.sequence_to_artist.extend([idx] * len(sequences))\n",
    "        print(f'Dataset has: {len(self.word_sequences)} sequences and {len(self.sequences_targets)} targets')\n",
    "\n",
    "    \n",
    "    def word_vec(self, tok: str) -> np.ndarray:\n",
    "        # helper to get word vector, or zeros if OOV\n",
    "        v = self.word_embeddings.get(tok)\n",
    "        if v is None:\n",
    "            # OOV → zeros with same dim as any known word vector\n",
    "            sample = next(iter(self.word_embeddings.values()))\n",
    "            v = np.zeros_like(sample, dtype=np.float32)\n",
    "        return v.astype(np.float32, copy=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_sequences)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        word_sequence = self.word_sequences[idx]          \n",
    "        target_word = self.sequences_targets[idx]\n",
    "        midi_feature: np.ndarray = self.midi_features[self.sequence_to_midi[idx]]\n",
    "        artist_name: str = self.sequence_artists[self.sequence_to_artist[idx]]\n",
    "        artist_index: int = self.artist_to_index[artist_name]\n",
    "        stacked_word_sequence = np.stack([self.word_vec(tok) for tok in word_sequence], axis=0)\n",
    "        stacked_word_sequence_with_midi_features = np.broadcast_to(midi_feature, (stacked_word_sequence.shape[0], midi_feature.shape[0]))\n",
    "        stacked_word_sequence_with_artist = np.broadcast_to(artist_index, (stacked_word_sequence.shape[0], 1)) # simple index.\n",
    "        concatenated_features = np.concatenate((stacked_word_sequence, stacked_word_sequence_with_midi_features, stacked_word_sequence_with_artist), axis=1)\n",
    "        return concatenated_features, target_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec874e9",
   "metadata": {},
   "source": [
    "<font size=6>Reading CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1959fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LYRIC_TRAIN_SET_CSV_PATH, mode='r', encoding='utf-8') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    lyric_train_data = list(reader)\n",
    "\n",
    "with open(LYRIC_TEST_SET_CSV_PATH, mode='r', encoding='utf-8') as test_file:\n",
    "    reader = csv.reader(test_file)\n",
    "    lyric_test_data = list(reader)\n",
    "\n",
    "if len(lyric_train_data) < 1 or len(lyric_test_data) < 1:\n",
    "    raise Exception(\"CSV files are empty or not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996afc4a",
   "metadata": {},
   "source": [
    "<font size=6>Parsing CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_data(raw_csv_data: list[list[str]]) -> list[tuple[str, str, list[str]]]:\n",
    "    returned_cleaned_csv_data: list[tuple[str, str, list[str]]] = []\n",
    "    for row in raw_csv_data:\n",
    "        artist = row[0].strip()\n",
    "        title_index = 1\n",
    "        lyrics_index = 2\n",
    "        while lyrics_index < len(row):\n",
    "            title = row[title_index].strip()\n",
    "            title = title.removesuffix('-2') # Remove '-2' suffix if present, relevant in 1 case.\n",
    "            title = row[title_index].strip()\n",
    "            lyrics = row[lyrics_index].strip()\n",
    "            lyrics = lyrics.lower()\n",
    "            lyrics = re.sub(f\"[{re.escape('&')}]\", f\" {EOL_STRING} \", lyrics) # Changing ampersands to eol to indicate end of line.\n",
    "            lyrics = re.sub(f\"[{re.escape('\\'')}]\", \"\", lyrics) # Removing apostrophes.\n",
    "            lyrics = re.sub(f\"[{re.escape('-')}]\", \" \", lyrics) # Removing hyphens.\n",
    "            lyrics = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", lyrics) # Removing punctuation.\n",
    "            lyrics = lyrics.split(' ') # Tokenzing each word by space.\n",
    "            lyrics = [word.strip() for word in lyrics if word] # Removing empty strings.\n",
    "            lyrics.append(EOS_STRING) # Adding end of song token.\n",
    "            if len(title) > 0 and len(lyrics) > 0:\n",
    "                returned_cleaned_csv_data.append((artist, title, lyrics))\n",
    "            title_index += 2\n",
    "            lyrics_index += 2\n",
    "    return returned_cleaned_csv_data\n",
    "\n",
    "cleaned_lyric_train_data = clean_csv_data(lyric_train_data)\n",
    "cleaned_lyric_test_data = clean_csv_data(lyric_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique words in the lyrics\n",
    "def get_word_frequencies(lyrics_data: list[tuple[str, str, list[str]]]) -> dict[str, int]:\n",
    "    words_frequency = defaultdict(int)\n",
    "    for _, _, lyrics in lyrics_data:\n",
    "        for word in lyrics:\n",
    "            words_frequency[word] += 1\n",
    "    return words_frequency    \n",
    "word_frequencies_training: dict[str, int] = get_word_frequencies(cleaned_lyric_train_data)\n",
    "word_frequencies_test: dict[str, int] = get_word_frequencies(cleaned_lyric_test_data)\n",
    "print(f\"Number of unique words in training set: {len(word_frequencies_training)}\")\n",
    "print(f\"Number of unique words in test set: {len(word_frequencies_test)}\")\n",
    "\n",
    "d_sorted_by_val = sorted(word_frequencies_training.items(), key=lambda kv: kv[1], reverse=True)\n",
    "d_sorted_by_val[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141709c5",
   "metadata": {},
   "source": [
    "<font size=6>Reading MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80433ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_files(midi_files_location: str, pickling_path: Optional[str] = None, failed_loads_path: Optional[str] = None) -> \\\n",
    "                    tuple[dict[str, dict[str, PrettyMIDI]], dict[str, set[str]]]: # artist -> title -> PrettyMIDI, failed loads[artist, song_set]\n",
    "    failed_loads = dict()\n",
    "    if failed_loads_path is not None and os.path.isfile(failed_loads_path):\n",
    "        with open(failed_loads_path, \"rb\") as f:\n",
    "            failed_loads = pickle.load(f)\n",
    "        print(f\"Loaded failed MIDI loads from pickled file {failed_loads_path}.\")\n",
    "    if pickling_path is not None and os.path.isfile(pickling_path):\n",
    "        with open(pickling_path, \"rb\") as f:\n",
    "            loaded_midi_files = pickle.load(f)\n",
    "        print(f\"Loaded MIDI files from pickled file {pickling_path}.\")\n",
    "        print(f'Loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.')\n",
    "        return loaded_midi_files, failed_loads\n",
    "    if not os.path.isdir(midi_files_location):\n",
    "        raise ValueError(f\"MIDI file path {midi_files_location} is not a valid directory.\")\n",
    "\n",
    "    # Traversing over all files and attempt to load them with pretty_midi:\n",
    "    loaded_midi_files: dict[str, dict[str, PrettyMIDI]] = defaultdict(dict) # artist -> title -> PrettyMIDI\n",
    "    failed_loads: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    for file in os.listdir(midi_files_location):\n",
    "        if file.endswith('.mid') or file.endswith('.midi'):\n",
    "            file_path = os.path.join(midi_files_location, file)\n",
    "            file = file.removesuffix('.mid')\n",
    "            splitted_artist_and_title = file.split('_-_')\n",
    "            artist = splitted_artist_and_title[0]\n",
    "            title = splitted_artist_and_title[1]\n",
    "            if len(splitted_artist_and_title) > 2:\n",
    "                print(f\"Warning: file {file} has more than one '_-_' separator, ignoring the rest after second \\\"_-_\\\".\")\n",
    "            artist = artist.replace('_', ' ').strip().lower()\n",
    "            title = title.replace('_', ' ').strip().lower()\n",
    "            try:\n",
    "                midi_data = PrettyMIDI(file_path)\n",
    "                loaded_midi_files[artist][title] = midi_data\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file}: {e}\")\n",
    "                failed_loads[artist].add(title)\n",
    "\n",
    "\n",
    "\n",
    "    if failed_loads:\n",
    "        print(\"Failed to load the following artist and lyric midi files:\")\n",
    "        for artist, lyrics in failed_loads.items():\n",
    "            print(f\"{artist} - [{', '.join(lyrics)}]\")\n",
    "\n",
    "    if pickling_path is not None:\n",
    "        with open(pickling_path, \"wb\") as f:\n",
    "            pickle.dump(loaded_midi_files, f)\n",
    "            print(f\"Pickled loaded MIDI files to {pickling_path}.\")\n",
    "    if failed_loads_path is not None:\n",
    "        with open(failed_loads_path, \"wb\") as f:\n",
    "            pickle.dump(failed_loads, f)\n",
    "            print(f\"Pickled failed MIDI loads to {failed_loads_path}.\")\n",
    "\n",
    "    print(f\"Successfully loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.\")\n",
    "    return loaded_midi_files, failed_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a752d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_midi_files, failed_midi_loads = load_midi_files(MIDI_FILE_PATH, PICKLING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ffe14",
   "metadata": {},
   "source": [
    "<font size=6>Mapping CSV data to MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_data_to_songdata_list(csv_data: list[list[str]], \n",
    "                              failed_midi_load: dict[str, set[str]], \n",
    "                              midi_files_dict: dict[str, dict[str, PrettyMIDI]]) -> list[SongData]:\n",
    "    song_data_list: list[SongData] = list()\n",
    "    missing_midi_count = 0\n",
    "    for row in csv_data:\n",
    "        artist = row[0]\n",
    "        title = row[1]\n",
    "        if artist in failed_midi_load and title in failed_midi_load[artist]:\n",
    "            print(f\"Skipping {artist} - {title} due to previous MIDI load failure.\")\n",
    "            continue\n",
    "        if artist in midi_files_dict and title in midi_files_dict[artist]:\n",
    "            midi_file = midi_files_dict[artist][title]\n",
    "            song_data = SongData(row, midi_file)\n",
    "            song_data_list.append(song_data)\n",
    "        else:\n",
    "            missing_midi_count += 1\n",
    "            print(f\"Missing MIDI file for artist '{artist}' and title '{title}'\")\n",
    "    print(f\"Total songs with missing MIDI files: {missing_midi_count}\")\n",
    "    return song_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f061cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_train_data, failed_midi_loads, loaded_midi_files)\n",
    "test_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_test_data, failed_midi_loads, loaded_midi_files)\n",
    "print(f\"Total training songs with MIDI data: {len(train_midi_data)}\")\n",
    "print(f\"Total test songs with MIDI data: {len(test_midi_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0f1d",
   "metadata": {},
   "source": [
    "<font size=6>Handling word embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95f984",
   "metadata": {},
   "source": [
    "Downloading pretrained word2vec, containing 300 dims, trained on news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0482d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bebb8",
   "metadata": {},
   "source": [
    "Extracting the vocabulary from the lyrics.\n",
    "Getting the data from the test set aswell since the vocbulary needs to be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_vocabulary: set[str] = set()\n",
    "# Getting the data from the test set aswell since the vocbulary needs to be known\n",
    "for song in train_midi_data + test_midi_data:\n",
    "    for word in song.lyrics:\n",
    "        lyrics_vocabulary.add(word)\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305abc9",
   "metadata": {},
   "source": [
    "Creating unified embedding.\n",
    "Extracting embeddings from word2vec and using random embeddings for words not found in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_embeddings: dict[str, np.ndarray] = dict()\n",
    "existing_words_in_pretrained = 0\n",
    "not_existing_in_pretrained = 0\n",
    "added_stopwords = 0\n",
    "for word in list(lyrics_vocabulary):\n",
    "    if word in pretrained_word2vec:\n",
    "        unified_embeddings[word] = pretrained_word2vec[word]\n",
    "        existing_words_in_pretrained += 1\n",
    "    else:\n",
    "        unified_embeddings[word] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,)) # Random init for unknown words.  \n",
    "        not_existing_in_pretrained += 1\n",
    "    # Adding stopwords as well, since they are common and should be in the vocabulary.\n",
    "for stopword in stopwords.words('english'):\n",
    "    cleaned_stopword = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", stopword.strip().lower()) # Cleaning the stopword, since it contains punctuation.\n",
    "    if cleaned_stopword not in unified_embeddings:\n",
    "        unified_embeddings[cleaned_stopword] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "        added_stopwords += 1\n",
    "\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")\n",
    "print(f\"Existing words in pretrained embeddings: {existing_words_in_pretrained}\")\n",
    "print(f\"Not existing in pretrained embeddings (randomly initialized): {not_existing_in_pretrained}\")\n",
    "print(f\"Added stopwords (randomly initialized): {added_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b23e9d",
   "metadata": {},
   "source": [
    "<font size=6>Handling artist embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f40eba",
   "metadata": {},
   "source": [
    "Using simple indexing for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_artists = [song.artist for song in train_midi_data]\n",
    "test_artists = [song.artist for song in test_midi_data]\n",
    "artist_set: set = (set(train_artists).union(set(test_artists)))\n",
    "artist_to_index: dict[str, int] = dict()\n",
    "index_to_artist: dict[int, str] = dict()\n",
    "for index, artist in enumerate(artist_set):\n",
    "    artist_to_index[artist] = index\n",
    "    index_to_artist[index] = artist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df1ccb",
   "metadata": {},
   "source": [
    "<font size=6>Load dataset and dataloader</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163de739",
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata_train_dataset = SongDataset(train_midi_data, unified_embeddings, artist_to_index)\n",
    "songdata_test_dataset = SongDataset(train_midi_data, unified_embeddings, artist_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee962c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = train_test_split(songdata_train_dataset, test_size=VALIDATION_SPLIT, random_state=RANDOM_LOADER_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = data.DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_data_loader = data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_data_loader = data.DataLoader(songdata_test_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word_in_vocab: index_of_word for index_of_word, word_in_vocab in enumerate(songdata_train_dataset.word_embeddings.keys())} \n",
    "id_to_word = {index_of_word: word_in_vocab for word_in_vocab, index_of_word in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfa020",
   "metadata": {},
   "source": [
    "<font size=6>Model 1: Simple concatenation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration Method 1: Simple Concatenation - Melody features are concatenated to each word embedding\n",
    "class LyricsGenerator_Concatenation(nn.Module):\n",
    "  def __init__(self, \n",
    "               vocab_size: int, \n",
    "               input_size: int, \n",
    "               hidden_layer_dim: int, \n",
    "               size_of_midi_features: int = NUMBER_OF_EXTRACT_MIDI_FEATURES,\n",
    "               size_of_word_embeddings: int = WORD_EMBEDDING_SIZE,\n",
    "               midi_scale: float = DEFAULT_MIDI_SCALE,\n",
    "               num_layers: int = LSTM_LAYERS, \n",
    "               dropout_rate: float = DROPOUT\n",
    "               ):\n",
    "    super(LyricsGenerator_Concatenation, self).__init__()\n",
    "    self.vocab_size: int = vocab_size\n",
    "    self.num_layers: int = num_layers\n",
    "    self.midi_scale: float = midi_scale\n",
    "    self.word_embedding_size: int = size_of_word_embeddings\n",
    "    self.size_of_midi_features: int = size_of_midi_features\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size, hidden_layer_dim, num_layers,\n",
    "                        batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    self.fc = nn.Linear(hidden_layer_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Returns the output after the midi features were made more relevant.\n",
    "    word_emb = x[..., :self.word_embedding_size]\n",
    "    midi_feat = x[..., self.word_embedding_size:self.word_embedding_size + self.size_of_midi_features] * self.midi_scale\n",
    "    artist_idx = x[..., -1:]\n",
    "    x_enhanced = torch.cat([word_emb, midi_feat, artist_idx], dim=-1)\n",
    "    lstm_out, _ = self.lstm(x_enhanced)\n",
    "    last_output = lstm_out[:, -1, :]\n",
    "    output = self.dropout(last_output)\n",
    "    logits = self.fc(output)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02346871",
   "metadata": {},
   "source": [
    "<font size=6>Running the models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f210285",
   "metadata": {},
   "source": [
    "<font size=5>Running model 1: Concatenation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb53ce",
   "metadata": {},
   "source": [
    "Add a custom loss to enforce the creation of words that look like actual lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02754ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, \n",
    "                train_loader: data.DataLoader, \n",
    "                val_loader: data.DataLoader, \n",
    "                test_loader: data.DataLoader,\n",
    "                word_to_id_dict: dict[str, int],\n",
    "                num_epochs: int = MAX_EPOCHS, \n",
    "                learning_rate: float = LEARNING_RATE,\n",
    "                patiance_factor: float = PATIANCE_FACTOR,\n",
    "                patiance_epochs: int = PATIANCE_EPOCHS):\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "    unk_id = word_to_id_dict.get(\"<unk>\", 0)\n",
    "    train_losses: list[float] = list()\n",
    "    val_losses: list[float] = list()\n",
    "    test_losses: list[float] = list()\n",
    "    best_validation_loss: float = 10000.0\n",
    "    epochs_with_no_improvements: int = 0\n",
    "    writer = SummaryWriter()  # TensorBoard writer\n",
    "    for epoch in range(num_epochs):\n",
    "        current_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(dtype=torch.float32)\n",
    "            inputs = inputs.to(device)\n",
    "            targets_indices = torch.tensor(\n",
    "                [word_to_id.get(t, unk_id) for t in targets],\n",
    "                dtype=torch.long, device=device\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets_indices)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)  # TensorBoard\n",
    "        model.eval()\n",
    "        validation_running_loss = 0.0\n",
    "        test_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs = val_inputs.to(dtype=torch.float32)\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets_indices = torch.tensor(\n",
    "                    [word_to_id.get(t, unk_id) for t in val_targets],\n",
    "                    dtype=torch.long, device=device\n",
    "                )\n",
    "                validation_outputs = model(val_inputs)\n",
    "                validation_loss = criterion(validation_outputs, val_targets_indices)\n",
    "                validation_running_loss += validation_loss.item() * val_inputs.size(0)\n",
    "\n",
    "            val_epoch_loss = validation_running_loss / len(val_loader.dataset)\n",
    "            val_losses.append(val_epoch_loss)\n",
    "            writer.add_scalar(\"Loss/val\", val_epoch_loss, epoch)  # TensorBoard\n",
    "\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                test_inputs = test_inputs.to(dtype=torch.float32)\n",
    "                test_inputs = test_inputs.to(device)\n",
    "                test_targets_indices = torch.tensor(\n",
    "                    [word_to_id.get(t, unk_id) for t in test_targets],\n",
    "                    dtype=torch.long, device=device\n",
    "                )\n",
    "                test_outputs = model(test_inputs)                  # logits (B, |V|)\n",
    "                test_loss = criterion(test_outputs, test_targets_indices)\n",
    "                test_running_loss += test_loss.item() * test_inputs.size(0)\n",
    "\n",
    "        test_epoch_loss = test_running_loss / len(test_loader.dataset)\n",
    "        test_losses.append(test_epoch_loss)\n",
    "        if  best_validation_loss - val_epoch_loss >= patiance_factor:\n",
    "            epochs_with_no_improvements = 0\n",
    "            best_validation_loss = val_epoch_loss\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_with_no_improvements += 1\n",
    "            print(f'No improvement in epoch. Patiance: {epochs_with_no_improvements}\\\\{patiance_epochs}')\n",
    "        scheduler.step()\n",
    "        finish_time = time.time() - current_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, Test Loss: {test_epoch_loss:.4f}, time: {finish_time}')\n",
    "        if epochs_with_no_improvements >= patiance_epochs:\n",
    "            print(f'Training ended prematurely due to lack of improvement.')\n",
    "            break\n",
    "    writer.close()  # Close TensorBoard writer\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    model.eval()\n",
    "    return model, train_losses, val_losses, test_losses\n",
    "\n",
    "# After training, run in terminal to view TensorBoard:\n",
    "# !tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b319f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LyricsGenerator_Concatenation(\n",
    "    vocab_size=len(songdata_train_dataset.word_embeddings),\n",
    "    input_size=pretrained_word2vec.vector_size + NUMBER_OF_EXTRACT_MIDI_FEATURES + 1, # word embedding + melody features + artist index\n",
    "    hidden_layer_dim=256,\n",
    "    num_layers=LSTM_LAYERS,\n",
    "    dropout_rate=DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_as_strings: list[str] = [target for _, target in training_set]\n",
    "targets_as_indices: list[int] = [word_to_id.get(t, 0) for t in targets_as_strings]\n",
    "\n",
    "model, training_loss, validation_loss, test_loss = train_model(model, \n",
    "            training_data_loader, \n",
    "            validation_data_loader, \n",
    "            test_data_loader,\n",
    "            word_to_id_dict=word_to_id\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27710111",
   "metadata": {},
   "source": [
    "Displaying tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e702053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TODO, MAKE THIS WORK LOL')\n",
    "print('TODO, MAKE THIS WORK LOL')\n",
    "print('TODO, MAKE THIS WORK LOL')\n",
    "print('TODO, MAKE THIS WORK LOL')\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd43df",
   "metadata": {},
   "source": [
    "<font size=6>Generating Lyrics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6c6c5",
   "metadata": {},
   "source": [
    "A function that returns the k most likely words given the input, used for coherent lyric generatin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf15b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_top_k_next_words(\n",
    "    model,\n",
    "    top_words_to_predict: int,\n",
    "    word_sequence: list[str],\n",
    "    artist_index: int,\n",
    "    melody_vec,\n",
    "    word_to_id: dict[str, int],\n",
    "    id_to_word: dict[int, str],\n",
    "    embedding_weight: torch.Tensor,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    model.to(device).eval()\n",
    "    if embedding_weight.device.type != device:\n",
    "        embedding_weight = embedding_weight.to(device)\n",
    "\n",
    "    # melody -> torch tensor on device\n",
    "    if not torch.is_tensor(melody_vec):\n",
    "        melody_vec = torch.as_tensor(melody_vec, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        melody_vec = melody_vec.to(device, dtype=torch.float32)\n",
    "\n",
    "    # Prepare sequence embeddings\n",
    "    unk_id = word_to_id.get(\"<unk>\", UNK_ID)\n",
    "    seq_ids = [word_to_id.get(w, unk_id) for w in word_sequence]\n",
    "    seq_embs = embedding_weight[torch.tensor(seq_ids, device=device)]  # [seq_len, emb_dim]\n",
    "\n",
    "    # Broadcast melody and artist features\n",
    "    melody_broadcast = melody_vec.expand(len(word_sequence), -1)  # [seq_len, melody_dim]\n",
    "    artist_broadcast = torch.full((len(word_sequence), 1), artist_index, dtype=torch.float32, device=device)  # [seq_len, 1]\n",
    "\n",
    "    # Concatenate features\n",
    "    x = torch.cat([seq_embs, melody_broadcast, artist_broadcast], dim=1).unsqueeze(0)  # [1, seq_len, input_size]\n",
    "\n",
    "    logits = model(x)  # [1, vocab]\n",
    "    probs = F.softmax(logits, dim=-1).squeeze(0)  # [vocab]\n",
    "\n",
    "    vals, idxs = probs.topk(top_words_to_predict)\n",
    "    top5 = [(id_to_word[i], float(v)) for i, v in zip(idxs.tolist(), vals.tolist())]\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fcbb1",
   "metadata": {},
   "source": [
    "Printing the generated text and handling tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e527c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generated_lyrics(generated_lyrics: list[str]):\n",
    "    capitalize = True\n",
    "    for word in generated_lyrics:\n",
    "        if word == EOL_STRING:\n",
    "            capitalize = True\n",
    "            print()\n",
    "        if word == EOS_STRING:\n",
    "            break\n",
    "        if word != EOL_STRING:\n",
    "            if capitalize:\n",
    "                capitalize = False\n",
    "                print(word.title(), end=' ')\n",
    "            else:\n",
    "                print(word, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239b529",
   "metadata": {},
   "source": [
    "Generating the lyrics and maintaining the lyrics generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3392cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(\n",
    "        model_to_use: nn.Module,\n",
    "        initial_word: str,\n",
    "        melody_features: np.ndarray,\n",
    "        melody_title: str,\n",
    "        artist_to_use: str,\n",
    "        word_to_id: dict[str, int],\n",
    "        id_to_word: dict[int, str],\n",
    "        artist_to_index: dict[str, int],\n",
    "        word_embeddings: dict[str, np.ndarray],\n",
    "        max_song_length: int = MAX_SONG_LENGTH_WORDS,\n",
    "        sequence_length: int = SEQUENCE_LENGTH,\n",
    "        top_k: int = TOP_K_WORDS_TO_PREDICT,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates lyrics word by word using the model, melody, and artist.\n",
    "    Picks next word randomly from top_k candidates according to their normalized probabilities.\n",
    "    Artificially increases probability of EOS_STRING after half of max_song_length.\n",
    "    Prints the generated lyrics with line breaks at <eol>.\n",
    "    Enforces some more grammatical rules.\n",
    "    Returns: generated_lyrics (list of str), artist, melody_title.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating song from initial word: '{initial_word}', melody: '{melody_title}', artist: '{artist_to_use}', max length: {max_song_length}\")\n",
    "    melody_vec = torch.as_tensor(melody_features['vector'], dtype=torch.float32, device=device)\n",
    "    artist_idx = artist_to_index.get(artist_to_use, -1)\n",
    "    if artist_idx == -1:\n",
    "        print(f\"Warning: Artist '{artist_to_use}' not found, using index -1.\")\n",
    "\n",
    "    embedding_weight = torch.from_numpy(\n",
    "        np.stack([word_embeddings[w].astype(np.float32) for w in word_to_id], axis=0)\n",
    "    ).to(device)\n",
    "    context: deque = deque()\n",
    "    context.extendleft([UNK_STRING for _ in range(sequence_length - 1)])\n",
    "    context.appendleft(initial_word)\n",
    "    unk_index: int = 1\n",
    "    generated_lyrics = [initial_word]\n",
    "    words_in_song: int = 0\n",
    "    current_word: str = initial_word\n",
    "    minimum_song_length = int(max_song_length / 2)\n",
    "    current_words_in_line: int = 1\n",
    "    while True:\n",
    "        top_words = predict_top_k_next_words(\n",
    "            model=model_to_use,\n",
    "            top_words_to_predict=top_k,\n",
    "            word_sequence=context,\n",
    "            artist_index=artist_idx,\n",
    "            melody_vec=melody_vec,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            embedding_weight=embedding_weight,\n",
    "            device=device\n",
    "        )\n",
    "        words, probs = zip(*top_words)\n",
    "        probs = np.array(probs, dtype=np.float32)\n",
    "        probs = probs / probs.sum()  # Normalize to sum to 1\n",
    "\n",
    "        # Artificially increase EOS probability after reaching half of max_song_length.\n",
    "        words, probs = zip(*top_words)\n",
    "        probs = np.array(probs, dtype=np.float32)\n",
    "        probs = probs / probs.sum()  # Normalize to sum to 1\n",
    "\n",
    "        # If EOS should be considered, add it to the candidates and replace the lowest probability\n",
    "        if words_in_song >= minimum_song_length and EOS_STRING not in words:\n",
    "            # Find the index of the lowest probability\n",
    "            min_prob_idx = np.argmin(probs)\n",
    "            # Add EOS to words and replace the lowest probability with eos_boost\n",
    "            eos_boost = (words_in_song - minimum_song_length + 1) / minimum_song_length\n",
    "            eos_boost = min(max(eos_boost, 0.0), 1.0)\n",
    "            words = list(words)\n",
    "            probs = list(probs)\n",
    "            words[min_prob_idx] = EOS_STRING\n",
    "            probs[min_prob_idx] = eos_boost\n",
    "            # Renormalize\n",
    "            probs = np.array(probs, dtype=np.float32)\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "        words_not_to_end_lines_on: list[str] = ['the']\n",
    "        max_tries: int = 10 # Max tries for correct word.\n",
    "        curr_try: int = 0\n",
    "        # Tries to enforce certain rules.\n",
    "        # Don't allow end of song before minimum amount of lines.\n",
    "        # Don't repeat the same word twice\n",
    "        # Don't allow lines that are too short.\n",
    "        # Don't end lines on words in a way that would make no sense.\n",
    "        while True:\n",
    "            next_word = np.random.choice(words, p=probs)  \n",
    "            curr_try += 1\n",
    "            if curr_try <= max_tries:\n",
    "                if next_word == current_word:\n",
    "                    continue\n",
    "                if next_word == EOS_STRING and words_in_song < int(minimum_song_length):\n",
    "                    continue\n",
    "                if current_word in words_not_to_end_lines_on and next_word == EOL_STRING:\n",
    "                    continue\n",
    "                if next_word == EOL_STRING and current_words_in_line < MIN_LINE_LENGTH:\n",
    "                    continue\n",
    "            break\n",
    "        if next_word == EOS_STRING and current_word == EOL_STRING:\n",
    "            generated_lyrics[-1] = next_word\n",
    "        else:\n",
    "            generated_lyrics.append(next_word)\n",
    "        current_word = next_word\n",
    "        if unk_index < sequence_length:\n",
    "            context[unk_index] = next_word\n",
    "            unk_index += 1\n",
    "        else:\n",
    "            context.popleft()\n",
    "            context.append(next_word)\n",
    "        if next_word == EOS_STRING:\n",
    "            break\n",
    "        if words_in_song >= max_song_length:\n",
    "            generated_lyrics.append(EOS_STRING)\n",
    "            break\n",
    "        if next_word != EOL_STRING:\n",
    "            words_in_song += 1\n",
    "            current_words_in_line += 1\n",
    "        else:\n",
    "            current_words_in_line = 0\n",
    "    print_generated_lyrics(generated_lyrics=generated_lyrics)\n",
    "    print(f'Number of words in lyrics: {words_in_song}')\n",
    "    print(\"\\n--- End of generated lyrics ---\")\n",
    "    return generated_lyrics, artist_to_use, melody_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acea986",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_to_use = train_midi_data[63]\n",
    "\n",
    "lyrics, artist, melody = generate_lyrics(\n",
    "    model_to_use=model,\n",
    "    initial_word=\"eyes\",\n",
    "    melody_features=song_to_use.midi_features,\n",
    "    melody_title=song_to_use.title,\n",
    "    artist_to_use='billy joel',\n",
    "    word_to_id=word_to_id,\n",
    "    id_to_word=id_to_word,\n",
    "    artist_to_index=artist_to_index,\n",
    "    word_embeddings=unified_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b6897",
   "metadata": {},
   "source": [
    "<font size=6>Section 7, testing with the testing set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949fa72",
   "metadata": {},
   "source": [
    "For each melody, the output of the architecture given the melody and the initial word of the real lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e489d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_song in test_midi_data:\n",
    "    print('--------------------------------')\n",
    "    test_song: SongData\n",
    "    lyrics, artist, melody = generate_lyrics(\n",
    "        model_to_use=model,\n",
    "        initial_word=test_song.lyrics[0],\n",
    "        melody_features=test_song.midi_features,\n",
    "        melody_title=test_song.title,\n",
    "        artist_to_use=test_song.artist,\n",
    "        word_to_id=word_to_id,\n",
    "        id_to_word=id_to_word,\n",
    "        artist_to_index=artist_to_index,\n",
    "        word_embeddings=unified_embeddings,\n",
    "        max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "    )\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92433b27",
   "metadata": {},
   "source": [
    "For each melody, the output of the architecture given the melody and different starting words. The same word should be used for all melodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0dea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_words: list[str] = ['love', 'baby', 'time']\n",
    "\n",
    "for word in starting_words:\n",
    "    print(f'-------------Initial Word Selected: {word}-------------------')\n",
    "    for test_song in test_midi_data:\n",
    "        print('--------------------------------')\n",
    "        test_song: SongData\n",
    "        lyrics, artist, melody = generate_lyrics(\n",
    "            model_to_use=model,\n",
    "            initial_word=word,\n",
    "            melody_features=test_song.midi_features,\n",
    "            melody_title=test_song.title,\n",
    "            artist_to_use=test_song.artist,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            artist_to_index=artist_to_index,\n",
    "            word_embeddings=unified_embeddings,\n",
    "            max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "        )\n",
    "        print('--------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
