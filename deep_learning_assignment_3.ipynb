{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd41e9a8",
   "metadata": {},
   "source": [
    "<font size=6>Downloading required libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q numpy\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip3 install torch torchsummary\n",
    "!pip3 install -q pretty_midi\n",
    "!pip3 install -q gensim\n",
    "!pip3 install -q nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4068e6c",
   "metadata": {},
   "source": [
    "<font size=6>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import gdown\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "import csv\n",
    "import string\n",
    "from pretty_midi import PrettyMIDI, Note\n",
    "import re\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359ed8",
   "metadata": {},
   "source": [
    "<font size=6>Constants</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60457569",
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRIC_TRAIN_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_train_set.csv')\n",
    "LYRIC_TEST_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_test_set.csv')\n",
    "MIDI_FILE_PATH: str = os.path.join(os.getcwd(), 'data', 'midi_files')\n",
    "PICKLING_PATH: str = os.path.join(os.getcwd(), 'loaded_midi_files.pkl') # Path to save/load pickled MIDI files, for faster loading.\n",
    "EPSILON: float = 1e-9\n",
    "SEQUENCE_LENGTH = 10  # Number of words in the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1947ce",
   "metadata": {},
   "source": [
    "<font size=6>Midi Feature extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083e542",
   "metadata": {},
   "source": [
    "Auxlliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_time_signature(changes: tuple[int, int]) -> tuple[int, int]:\n",
    "    if not changes: return (4, 4)\n",
    "    pairs: list[tuple[int, int]] = [(ts.numerator, ts.denominator) for ts in changes]\n",
    "    return Counter(pairs).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2526597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_weighted_pitch_stats(notes: list[Note]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute pitch statistics weighted by each note's duration.\n",
    "    Returns a dict with:\n",
    "      mean (duration-weighted),\n",
    "      std  (duration-weighted),\n",
    "      p10 / p50 / p90 (duration-weighted percentiles),\n",
    "      ambitus = p90 - p10 (robust range).\n",
    "    If `notes` is empty, returns safe defaults.\n",
    "    \"\"\"\n",
    "    # Empty guard: nothing to measure → return neutral stats.\n",
    "    if not notes:\n",
    "        return dict(mean=0.0, std=0.0, p10=-1, p50=-1, p90=-1, ambitus=0.0)\n",
    "\n",
    "    # Vectorize pitches as float for math (MIDI 0..127, but floats simplify ops).\n",
    "    pitches: np.ndarray[np.float32] = np.fromiter((n.pitch for n in notes), dtype=np.float32)\n",
    "\n",
    "    # Each note's weight = its duration in seconds; clamp tiny/negative to epsilon.\n",
    "    weights: np.ndarray[np.float32] = np.fromiter((max(EPSILON, n.end - n.start) for n in notes), dtype=np.float32)\n",
    "    total_weights: float = weights.sum()\n",
    "    duration_weight_mean: float = float((weights * pitches).sum() / total_weights)\n",
    "    duration_weighted_variance: float = float((weights * (pitches - duration_weight_mean) ** 2).sum() / total_weights)\n",
    "    weighted_std: float = duration_weighted_variance ** 0.5\n",
    "\n",
    "    # ---------- Duration-weighted percentiles ----------\n",
    "    order: np.ndarray[np.int32] = np.argsort(pitches)\n",
    "    ordered_pitches, ordered_weights = pitches[order], weights[order]\n",
    "    cumulative_weight_sum: np.ndarray[np.float32] = np.cumsum(ordered_weights)\n",
    "\n",
    "    # Weighted quantile: find the first index where cumulative weight crosses q%.\n",
    "    def weighted_quantile(quantile: float) -> float:\n",
    "        # Target cumulative weight at quantile q (0..100).\n",
    "        target: float = (quantile / 100.0) * cumulative_weight_sum[-1]\n",
    "        # Index where cumulative_weight_sum >= target; take leftmost to be consistent.\n",
    "        idx: int = np.searchsorted(cumulative_weight_sum, target, side=\"left\")\n",
    "        return float(ordered_pitches[min(idx, len(ordered_pitches) - 1)])\n",
    "\n",
    "    # 10th / 50th (median) / 90th percentiles, duration-weighted.\n",
    "    percentile_10, percentile_50, percentile_90 = weighted_quantile(10), weighted_quantile(50), weighted_quantile(90)\n",
    "\n",
    "    # Ambitus = robust spread (p90 - p10), less sensitive than raw max - min.\n",
    "    return dict(mean=duration_weight_mean, \n",
    "                std=weighted_std,\n",
    "                p10=percentile_10, \n",
    "                p50=percentile_50, \n",
    "                p90=percentile_90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66999304",
   "metadata": {},
   "source": [
    "Extracting high level features relating to the entire song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85395762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_midi_features(midi: PrettyMIDI) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return song-level features (vector+names).\n",
    "    \"\"\" \n",
    "    duration_sec: float = midi.get_end_time()                                       # total length\n",
    "    if duration_sec <= 0: raise ValueError(\"Empty/zero-length MIDI.\")        # guard\n",
    "\n",
    "    tempo_times, tempo_bpms = midi.get_tempo_changes()                               # tempo changes\n",
    "    if len(tempo_bpms) == 0:                                                     # no changes\n",
    "        tempo_times = np.array([0.0], dtype=np.float32)                          # start time\n",
    "        tempo_bpms = np.array([midi.estimate_tempo()], dtype=np.float32)         # single bpm\n",
    "    segment_ends: np.ndarray[np.float32] = np.r_[tempo_times[1:], duration_sec]                              # segment ends\n",
    "    segment_durs: np.ndarray[np.float32] = np.maximum(1e-6, segment_ends - tempo_times[:len(segment_ends)])          # segment durations\n",
    "    tempo_mean: float = float(np.dot(tempo_bpms[:len(segment_durs)], segment_durs) / np.sum(segment_durs)) # duration-weighted mean\n",
    "    tempo_std: float = float(np.std(np.repeat(\n",
    "        tempo_bpms[:len(segment_durs)],                                              # repeat bpm by\n",
    "        np.maximum(1, (segment_durs/np.sum(segment_durs)*1000).astype(int))          # rough weights\n",
    "    )))                                                                       # dispersion proxy\n",
    "    tempo_change_count: int = int(len(tempo_bpms))                                     # number of states\n",
    "\n",
    "    time_signature_numerator, time_signature_denominator = most_common_time_signature(midi.time_signature_changes)  # mode time sig\n",
    "    instrument_count: int = sum(1 for inst in midi.instruments                     # non-drum count\n",
    "                          if not inst.is_drum and inst.notes)\n",
    "\n",
    "    instruments: list = [inst for inst in midi.instruments if not inst.is_drum and inst.notes]\n",
    "    instruments_velocities: list[float] = []\n",
    "    instrument_notes: list[Note] = []\n",
    "    for instrument in instruments:                                 # melody track\n",
    "        mel_velocity = [note.velocity for note in instrument.notes]     # melody velocities\n",
    "        instruments_velocities.extend(mel_velocity)\n",
    "        instrument_notes.extend(instrument.notes)\n",
    "\n",
    "    instrument_velocities_min: float = min(instruments_velocities)      # min pitch\n",
    "    instrument_velocities_max: float = max(instruments_velocities)      # max pitch\n",
    "    instrument_velocities_mean: float = np.mean(instruments_velocities)    # mean pitch\n",
    "    instrument_velocities_std: float = np.std(instruments_velocities)     # std pitch\n",
    "\n",
    "    duration_weight_pitch_stats_dict: dict = get_duration_weighted_pitch_stats(instrument_notes)\n",
    "    instrument_pitch_10_percentile: float = duration_weight_pitch_stats_dict['p10']\n",
    "    instrument_pitch_50_percentile: float = duration_weight_pitch_stats_dict['p50']\n",
    "    instrument_pitch_90_percentile: float = duration_weight_pitch_stats_dict['p90']\n",
    "    instrument_pitch_mean: float = duration_weight_pitch_stats_dict['mean']\n",
    "    instrument_pitch_std: float = duration_weight_pitch_stats_dict['std']\n",
    "    instrument_pitch_range_by_percentiles: float = instrument_pitch_90_percentile - instrument_pitch_10_percentile\n",
    "\n",
    "    note_durations: list[float] = [note.end - note.start for note in instrument_notes]\n",
    "    note_durations_mean: float = np.mean(note_durations) if note_durations else 0.0\n",
    "    note_durations_std: float = np.std(note_durations) if note_durations else 0.0\n",
    "    note_durations_range: float = max(note_durations) - min(note_durations) if note_durations else 0.0\n",
    "\n",
    "    note_density: float = float(len(instrument.notes) / max(EPSILON, duration_sec))           # notes/sec\n",
    "\n",
    "    chroma_global = midi.get_pitch_class_histogram(use_duration=True)        # 12-bin chroma\n",
    "    chroma_global = chroma_global / (np.sum(chroma_global) + EPSILON)           # normalize\n",
    "    names = [                                                                # feature names\n",
    "        \"duration_sec\",\n",
    "        \"tempo_mean_bpm\", \n",
    "        \"tempo_std_bpm\", \n",
    "        \"tempo_change_count\",\n",
    "        \"time_sig_num\", \n",
    "        \"time_sig_den\",\n",
    "        \"instrument_count\",\n",
    "        \"instrument_velocities_min\", \n",
    "        \"instrument_velocities_max\", \n",
    "        \"instrument_velocities_mean\", \n",
    "        \"instrument_velocities_std\", \n",
    "        \"instrument_pitch_10_percentile\",\n",
    "        \"instrument_pitch_50_percentile\",\n",
    "        \"instrument_pitch_90_percentile\",\n",
    "        \"instrument_pitch_mean\",\n",
    "        \"instrument_pitch_std\",\n",
    "        \"instrument_pitch_range_by_percentiles\",\n",
    "        \"note_durations_mean\",\n",
    "        \"note_durations_std\",\n",
    "        \"note_durations_range\",\n",
    "        \"melody_note_density_per_sec\",\n",
    "    ] + [f\"chroma_{i}\" for i in range(12)]                                   # chroma names\n",
    "    vec = np.array([                                                         # feature vector\n",
    "        duration_sec, \n",
    "        tempo_mean, \n",
    "        tempo_std, \n",
    "        tempo_change_count,\n",
    "        time_signature_numerator, \n",
    "        time_signature_denominator,   \n",
    "        instrument_count,\n",
    "        instrument_velocities_min, \n",
    "        instrument_velocities_max, \n",
    "        instrument_velocities_mean, \n",
    "        instrument_velocities_std, \n",
    "        instrument_pitch_10_percentile,\n",
    "        instrument_pitch_50_percentile,\n",
    "        instrument_pitch_90_percentile,\n",
    "        instrument_pitch_mean,\n",
    "        instrument_pitch_std,\n",
    "        instrument_pitch_range_by_percentiles,\n",
    "        note_durations_mean,\n",
    "        note_durations_std,\n",
    "        note_durations_range,   \n",
    "        note_density,\n",
    "        *chroma_global.tolist()\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return {\"vector\": vec, \"names\": names} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c68e2f",
   "metadata": {},
   "source": [
    "<font size=6>Auxlilliary Data Structures</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f7e0b",
   "metadata": {},
   "source": [
    "Auxilliary functions for creation of word sequences and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ba93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_sequences_with_targets(tokenized_lyrics: list[str], sequence_length: int = SEQUENCE_LENGTH) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given tokenized lyrics as a list of strings, create sequences of word indices and their corresponding target word indices.\n",
    "    Each sequence is of length `sequence_length`, and the target is the next word following the sequence.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Create sequences and targets\n",
    "    for i in range(len(tokenized_lyrics) - sequence_length):\n",
    "        seq = tokenized_lyrics[i:i + sequence_length]\n",
    "        target = tokenized_lyrics[i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return sequences,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongData:\n",
    "    def __init__(self, song_data_cell: list[str] = None, midi_file: PrettyMIDI = None):\n",
    "        if len(song_data_cell) != 3:\n",
    "            raise ValueError(\"song_data_cell must have exactly three elements: [artist, title, lyrics]\")\n",
    "        self.artist = song_data_cell[0]\n",
    "        self.title = song_data_cell[1]\n",
    "        self.lyrics = song_data_cell[2]\n",
    "        self.midi_data = midi_file\n",
    "        self._midi_features: Optional[dict[str, np.ndarray]] = None\n",
    "\n",
    "    @property\n",
    "    def midi_features(self):\n",
    "        if self._midi_features is None:\n",
    "            self._midi_features = extract_midi_features(self.midi_data)\n",
    "        return self._midi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8efdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataset(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 songs_data: list[SongData],\n",
    "                 word_embeddings: dict[str, np.ndarray],\n",
    "                 artist_embeddings: dict[str, np.ndarray],\n",
    "                 artist_vocab: dict[str, int]):\n",
    "        self.midi_features: list[np.ndarray] = list()\n",
    "        self.artists: list[str] = list()\n",
    "        self.sequence_artists: list[str] = list()\n",
    "        self.word_sequences: list[str] = list()\n",
    "        self.sequences_targets: list[str] = list()\n",
    "        self.sequence_to_midi: list[int] = list() # Maps each sequence to its corresponding MIDI feature index \n",
    "        self.sequence_to_artist: list[int] = list() # Maps each sequence to its corresponding artist embedding index\n",
    "        self.word_embeddings: dict[str, np.ndarray] = word_embeddings\n",
    "        self.artist_embeddings: dict[str, np.ndarray] = artist_embeddings\n",
    "        self.artist_vocab: dict[str, int] = artist_vocab\n",
    "        # Instead of saving each sequence's MIDI features, we save the index of the MIDI features in the midi_features list to save space.\n",
    "        for idx, song in enumerate(songs_data):\n",
    "            sequences, targets = create_word_sequences_with_targets(song.lyrics)\n",
    "            self.word_sequences.extend(sequences)\n",
    "            self.sequences_targets.extend(targets)\n",
    "            self.midi_features.append(song.midi_features['vector']) # Creates a mapping of the features to the sequences.\n",
    "            self.sequence_artists.append(song.artist)\n",
    "            self.sequence_to_midi.extend([idx] * len(sequences))\n",
    "            self.sequence_to_artist.extend([idx] * len(sequences))\n",
    "        print(f'Dataset has: {len(self.word_sequences)} sequences and {len(self.sequences_targets)} targets')\n",
    "\n",
    "    \n",
    "    def word_vec(self, tok: str) -> np.ndarray:\n",
    "        # helper to get word vector, or zeros if OOV\n",
    "        v = self.word_embeddings.get(tok)\n",
    "        if v is None:\n",
    "            # OOV → zeros with same dim as any known word vector\n",
    "            sample = next(iter(self.word_embeddings.values()))\n",
    "            v = np.zeros_like(sample, dtype=np.float32)\n",
    "        return v.astype(np.float32, copy=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_sequences)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        word_sequence = self.word_sequences[idx]          \n",
    "        target_word = self.sequences_targets[idx]\n",
    "        midi_feature: np.ndarray = self.midi_features[self.sequence_to_midi[idx]]\n",
    "        artist_embedding: np.ndarray = self.artist_embeddings[self.artist_vocab[self.sequence_artists[idx]]]\n",
    "        stacked_word_sequence = np.stack([self.word_vec(tok) for tok in word_sequence], axis=0)\n",
    "        stacked_word_sequence_with_midi_features = np.broadcast_to(midi_feature, (stacked_word_sequence.shape[0], midi_feature.shape[0]))\n",
    "        stacked_word_sequence_with_artist_embedding = np.broadcast_to(artist_embedding, (stacked_word_sequence.shape[0], artist_embedding.shape[0]))\n",
    "        concatenated_features = np.concatenate((stacked_word_sequence, stacked_word_sequence_with_midi_features, stacked_word_sequence_with_artist_embedding), axis=1)\n",
    "        # Debug prints to verify shapes and contents\n",
    "        print(f\"Word sequence shape: {stacked_word_sequence.shape}\")\n",
    "        print(f\"MIDI feature shape: {midi_feature.shape}\")\n",
    "        print(f\"Artist embedding shape: {artist_embedding.shape}\")\n",
    "        print(f\"Concatenated features shape: {concatenated_features.shape}\")\n",
    "        return concatenated_features, target_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec874e9",
   "metadata": {},
   "source": [
    "<font size=6>Reading CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1959fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LYRIC_TRAIN_SET_CSV_PATH, mode='r', encoding='utf-8') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    lyric_train_data = list(reader)\n",
    "\n",
    "with open(LYRIC_TEST_SET_CSV_PATH, mode='r', encoding='utf-8') as test_file:\n",
    "    reader = csv.reader(test_file)\n",
    "    lyric_test_data = list(reader)\n",
    "\n",
    "if len(lyric_train_data) < 1 or len(lyric_test_data) < 1:\n",
    "    raise Exception(\"CSV files are empty or not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996afc4a",
   "metadata": {},
   "source": [
    "<font size=6>Parsing CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_data(raw_csv_data: list[list[str]]) -> list[tuple[str, str, list[str]]]:\n",
    "    returned_cleaned_csv_data: list[tuple[str, str, list[str]]] = []\n",
    "    for row in raw_csv_data:\n",
    "        artist = row[0].strip()\n",
    "        title_index = 1\n",
    "        lyrics_index = 2\n",
    "        while lyrics_index < len(row):\n",
    "            title = row[title_index].strip()\n",
    "            title = title.removesuffix('-2') # Remove '-2' suffix if present, relevant in 1 case.\n",
    "            title = row[title_index].strip()\n",
    "            lyrics = row[lyrics_index].strip()\n",
    "            lyrics = lyrics.lower()\n",
    "            lyrics = re.sub(f\"[{re.escape('&')}]\", \" eol \", lyrics) # Changing ampersands to eol to indicate end of line.\n",
    "            lyrics = re.sub(f\"[{re.escape('\\'')}]\", \"\", lyrics) # Removing apostrophes.\n",
    "            lyrics = re.sub(f\"[{re.escape('-')}]\", \" \", lyrics) # Removing hyphens.\n",
    "            lyrics = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", lyrics) # Removing punctuation.\n",
    "            lyrics = lyrics.split(' ') # Tokenzing each word by space.\n",
    "            lyrics = [word.strip() for word in lyrics if word] # Removing empty strings.\n",
    "            lyrics.append('eos') # Adding end of song token.\n",
    "            if len(title) > 0 and len(lyrics) > 0:\n",
    "                returned_cleaned_csv_data.append((artist, title, lyrics))\n",
    "            title_index += 2\n",
    "            lyrics_index += 2\n",
    "    return returned_cleaned_csv_data\n",
    "\n",
    "cleaned_lyric_train_data = clean_csv_data(lyric_train_data)\n",
    "cleaned_lyric_test_data = clean_csv_data(lyric_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of unique words in the lyrics\n",
    "def count_unique_words(lyrics_data: list[tuple[str, str, list[str]]]) -> int:\n",
    "    unique_words = set()\n",
    "    for _, _, lyrics in lyrics_data:\n",
    "        unique_words.update(lyrics)\n",
    "    return len(unique_words)    \n",
    "print(f\"Number of unique words in training set: {count_unique_words(cleaned_lyric_train_data)}\")\n",
    "print(f\"Number of unique words in test set: {count_unique_words(cleaned_lyric_test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141709c5",
   "metadata": {},
   "source": [
    "<font size=6>Reading MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80433ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_files(midi_files_location: str, pickling_path: Optional[str] = None, failed_loads_path: Optional[str] = None) -> \\\n",
    "                    tuple[dict[str, dict[str, PrettyMIDI]], dict[str, set[str]]]: # artist -> title -> PrettyMIDI, failed loads[artist, song_set]\n",
    "    failed_loads = dict()\n",
    "    if failed_loads_path is not None and os.path.isfile(failed_loads_path):\n",
    "        with open(failed_loads_path, \"rb\") as f:\n",
    "            failed_loads = pickle.load(f)\n",
    "        print(f\"Loaded failed MIDI loads from pickled file {failed_loads_path}.\")\n",
    "    if pickling_path is not None and os.path.isfile(pickling_path):\n",
    "        with open(pickling_path, \"rb\") as f:\n",
    "            loaded_midi_files = pickle.load(f)\n",
    "        print(f\"Loaded MIDI files from pickled file {pickling_path}.\")\n",
    "        print(f'Loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.')\n",
    "        return loaded_midi_files, failed_loads\n",
    "    if not os.path.isdir(midi_files_location):\n",
    "        raise ValueError(f\"MIDI file path {midi_files_location} is not a valid directory.\")\n",
    "\n",
    "    # Traversing over all files and attempt to load them with pretty_midi:\n",
    "    loaded_midi_files: dict[str, dict[str, PrettyMIDI]] = defaultdict(dict) # artist -> title -> PrettyMIDI\n",
    "    failed_loads: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    for file in os.listdir(midi_files_location):\n",
    "        if file.endswith('.mid') or file.endswith('.midi'):\n",
    "            file_path = os.path.join(midi_files_location, file)\n",
    "            file = file.removesuffix('.mid')\n",
    "            splitted_artist_and_title = file.split('_-_')\n",
    "            artist = splitted_artist_and_title[0]\n",
    "            title = splitted_artist_and_title[1]\n",
    "            if len(splitted_artist_and_title) > 2:\n",
    "                print(f\"Warning: file {file} has more than one '_-_' separator, ignoring the rest after second \\\"_-_\\\".\")\n",
    "            artist = artist.replace('_', ' ').strip().lower()\n",
    "            title = title.replace('_', ' ').strip().lower()\n",
    "            try:\n",
    "                midi_data = PrettyMIDI(file_path)\n",
    "                loaded_midi_files[artist][title] = midi_data\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file}: {e}\")\n",
    "                failed_loads[artist].add(title)\n",
    "\n",
    "\n",
    "\n",
    "    if failed_loads:\n",
    "        print(\"Failed to load the following artist and lyric midi files:\")\n",
    "        for artist, lyrics in failed_loads.items():\n",
    "            print(f\"{artist} - [{', '.join(lyrics)}]\")\n",
    "\n",
    "    if pickling_path is not None:\n",
    "        with open(pickling_path, \"wb\") as f:\n",
    "            pickle.dump(loaded_midi_files, f)\n",
    "            print(f\"Pickled loaded MIDI files to {pickling_path}.\")\n",
    "    if failed_loads_path is not None:\n",
    "        with open(failed_loads_path, \"wb\") as f:\n",
    "            pickle.dump(failed_loads, f)\n",
    "            print(f\"Pickled failed MIDI loads to {failed_loads_path}.\")\n",
    "\n",
    "    print(f\"Successfully loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.\")\n",
    "    return loaded_midi_files, failed_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a752d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_midi_files, failed_midi_loads = load_midi_files(MIDI_FILE_PATH, PICKLING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ffe14",
   "metadata": {},
   "source": [
    "<font size=6>Mapping CSV data to MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_data_to_songdata_list(csv_data: list[list[str]], \n",
    "                              failed_midi_load: dict[str, set[str]], \n",
    "                              midi_files_dict: dict[str, dict[str, PrettyMIDI]]) -> list[SongData]:\n",
    "    song_data_list: list[SongData] = list()\n",
    "    missing_midi_count = 0\n",
    "    for row in csv_data:\n",
    "        artist = row[0]\n",
    "        title = row[1]\n",
    "        if artist in failed_midi_load and title in failed_midi_load[artist]:\n",
    "            print(f\"Skipping {artist} - {title} due to previous MIDI load failure.\")\n",
    "            continue\n",
    "        if artist in midi_files_dict and title in midi_files_dict[artist]:\n",
    "            midi_file = midi_files_dict[artist][title]\n",
    "            song_data = SongData(row, midi_file)\n",
    "            song_data_list.append(song_data)\n",
    "        else:\n",
    "            missing_midi_count += 1\n",
    "            print(f\"Missing MIDI file for artist '{artist}' and title '{title}'\")\n",
    "    print(f\"Total songs with missing MIDI files: {missing_midi_count}\")\n",
    "    return song_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f061cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_train_data, failed_midi_loads, loaded_midi_files)\n",
    "test_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_test_data, failed_midi_loads, loaded_midi_files)\n",
    "print(f\"Total training songs with MIDI data: {len(train_midi_data)}\")\n",
    "print(f\"Total test songs with MIDI data: {len(test_midi_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0f1d",
   "metadata": {},
   "source": [
    "<font size=6>Handling word embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95f984",
   "metadata": {},
   "source": [
    "Downloading pretrained word2vec, containing 300 dims, trained on news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0482d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bebb8",
   "metadata": {},
   "source": [
    "Extracting the vocabulary from the lyrics.\n",
    "Getting the data from the test set aswell since the vocbulary needs to be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_vocabulary: set[str] = set()\n",
    "# Getting the data from the test set aswell since the vocbulary needs to be known\n",
    "for song in train_midi_data + test_midi_data:\n",
    "    for word in song.lyrics:\n",
    "        lyrics_vocabulary.add(word)\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")\n",
    "print(lyrics_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305abc9",
   "metadata": {},
   "source": [
    "Creating unified embedding.\n",
    "Extracting embeddings from word2vec and using random embeddings for words not found in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_embeddings: dict[str, np.ndarray] = dict()\n",
    "existing_words_in_pretrained = 0\n",
    "not_existing_in_pretrained = 0\n",
    "added_stopwords = 0\n",
    "for word in list(lyrics_vocabulary):\n",
    "    if word in pretrained_word2vec:\n",
    "        unified_embeddings[word] = pretrained_word2vec[word]\n",
    "        existing_words_in_pretrained += 1\n",
    "    else:\n",
    "        unified_embeddings[word] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,)) # Random init for unknown words.  \n",
    "        not_existing_in_pretrained += 1\n",
    "    # Adding stopwords as well, since they are common and should be in the vocabulary.\n",
    "for stopword in stopwords.words('english'):\n",
    "    cleaned_stopword = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", stopword.strip().lower()) # Cleaning the stopword, since it contains punctuation.\n",
    "    if cleaned_stopword not in unified_embeddings:\n",
    "        unified_embeddings[cleaned_stopword] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "        added_stopwords += 1\n",
    "\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")\n",
    "print(f\"Existing words in pretrained embeddings: {existing_words_in_pretrained}\")\n",
    "print(f\"Not existing in pretrained embeddings (randomly initialized): {not_existing_in_pretrained}\")\n",
    "print(f\"Added stopwords (randomly initialized): {added_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b23e9d",
   "metadata": {},
   "source": [
    "<font size=6>Handling artist embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f40eba",
   "metadata": {},
   "source": [
    "Using simple one-hot embedding for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(artists: list[str]) -> tuple[np.ndarray, dict[str, int]]:\n",
    "    vocab = {s: i for i, s in enumerate(dict.fromkeys(artists))}\n",
    "    number_of_items, number_of_columns = len(artists), len(vocab)\n",
    "    artist_one_hot_encoding = np.zeros((number_of_items, number_of_columns), dtype=np.int8)\n",
    "    artist_one_hot_encoding[np.arange(number_of_items), [vocab[s] for s in artists]] = 1\n",
    "    return artist_one_hot_encoding, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_artists = [song.artist for song in train_midi_data]\n",
    "test_artists = [song.artist for song in test_midi_data]\n",
    "artist_one_hot_encoding, artist_vocab = one_hot_encode(list(set(train_artists).union(set(test_artists))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df1ccb",
   "metadata": {},
   "source": [
    "<font size=6>Load to datasets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163de739",
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata_dataset = SongDataset(train_midi_data, unified_embeddings, artist_one_hot_encoding, artist_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
