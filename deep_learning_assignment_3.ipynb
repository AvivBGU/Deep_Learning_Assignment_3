{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd41e9a8",
   "metadata": {},
   "source": [
    "<font size=6>Downloading required libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7a3d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.281593Z",
     "start_time": "2025-10-14T14:25:35.762638Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install -q numpy\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip3 install torch torchinfo\n",
    "!pip3 install torch tensorboard\n",
    "!pip3 install -q pretty_midi\n",
    "!pip3 install -q gensim\n",
    "!pip3 install -q nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4068e6c",
   "metadata": {},
   "source": [
    "<font size=6>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414349c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.298668Z",
     "start_time": "2025-10-14T14:25:48.288572Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from torchinfo import summary\n",
    "from typing import Optional\n",
    "import csv\n",
    "import string\n",
    "from pretty_midi import PrettyMIDI, Note\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359ed8",
   "metadata": {},
   "source": [
    "<font size=6>Constants</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60457569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.351346Z",
     "start_time": "2025-10-14T14:25:48.340260Z"
    }
   },
   "outputs": [],
   "source": [
    "LYRIC_TRAIN_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_train_set.csv')\n",
    "LYRIC_TEST_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_test_set.csv')\n",
    "MIDI_FILE_PATH: str = os.path.join(os.getcwd(), 'data', 'midi_files')\n",
    "PICKLING_PATH: str = os.path.join(os.getcwd(), 'loaded_midi_files.pkl') # Path to save/load pickled MIDI files, for faster loading.\n",
    "EPSILON: float = 1e-9\n",
    "SEQUENCE_LENGTH: int = 10  # Number of words in the input sequence\n",
    "BATCH_SIZE: int = 128\n",
    "LSTM_LAYERS: int = 2\n",
    "ATTENTION_LAYERS: int = 2\n",
    "DROPOUT: float = 0.3\n",
    "ATTENTION_DROPOUT: float = 0.2\n",
    "RANDOM_LOADER_SEED: int = 42\n",
    "VALIDATION_SPLIT: float = 0.1\n",
    "LEARNING_RATE: float = 0.001\n",
    "MAX_EPOCHS: int = 100\n",
    "NUMBER_OF_EXTRACT_MIDI_FEATURES: int = 25\n",
    "WORD_EMBEDDING_SIZE: int = 300\n",
    "SIZE_OF_ARTIST_INDEX: int = 1\n",
    "PATIENCE_FACTOR: float = 0.01\n",
    "PATIENCE_EPOCHS: int = 10\n",
    "UNK_ID: int = 0\n",
    "MAX_LINE_LENGTH: int = SEQUENCE_LENGTH\n",
    "MIN_LINE_LENGTH: int = SEQUENCE_LENGTH / 2\n",
    "EOL_STRING: str = '<eol>'\n",
    "UNK_STRING: str = '<unk>'\n",
    "PAD_STRING: str = '<pad>'\n",
    "EOS_STRING: str = '<eos>'\n",
    "TOP_K_WORDS_TO_PREDICT: int = 20\n",
    "MAX_SONG_LENGTH_WORDS: int = 80\n",
    "HIDDEN_LAYER_DIM: int = 256\n",
    "HIDDEN_LAYER_DIM_ATTENTION: int = 256\n",
    "SEED = 42\n",
    "VERBOSE: str = True\n",
    "NOTE_SAMPLING_INTERVAL_SHORT: float = 0.10\n",
    "NOTE_SAMPLING_INTERVAL_LONG: float = 0.50\n",
    "RHYTHM_FEATURES: str = 'RHYTHM_FEATURES'\n",
    "HARMONY_FEATURES: str = 'HARMONY_FEATURES'\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1947ce",
   "metadata": {},
   "source": [
    "<font size=6>Midi Feature extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083e542",
   "metadata": {},
   "source": [
    "Auxlliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216678d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.397213Z",
     "start_time": "2025-10-14T14:25:48.391697Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_common_time_signature(changes: tuple[int, int]) -> tuple[int, int]:\n",
    "    if not changes: return (4, 4)\n",
    "    pairs: list[tuple[int, int]] = [(ts.numerator, ts.denominator) for ts in changes]\n",
    "    return Counter(pairs).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2526597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.449517Z",
     "start_time": "2025-10-14T14:25:48.440642Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_duration_weighted_pitch_stats(notes: list[Note]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute pitch statistics weighted by each note's duration.\n",
    "    Returns a dict with:\n",
    "      mean (duration-weighted),\n",
    "      std  (duration-weighted),\n",
    "      p10 / p50 / p90 (duration-weighted percentiles),\n",
    "      ambitus = p90 - p10 (robust range).\n",
    "    If `notes` is empty, returns safe defaults.\n",
    "    \"\"\"\n",
    "    # Empty guard: nothing to measure â†’ return neutral stats.\n",
    "    if not notes:\n",
    "        return dict(mean=0.0, std=0.0, p10=-1, p50=-1, p90=-1, ambitus=0.0)\n",
    "\n",
    "    # Vectorize pitches as float for math (MIDI 0..127, but floats simplify ops).\n",
    "    pitches: np.ndarray[np.float32] = np.fromiter((n.pitch for n in notes), dtype=np.float32)\n",
    "\n",
    "    # Each note's weight = its duration in seconds; clamp tiny/negative to epsilon.\n",
    "    weights: np.ndarray[np.float32] = np.fromiter((max(EPSILON, n.end - n.start) for n in notes), dtype=np.float32)\n",
    "    total_weights: float = weights.sum()\n",
    "    duration_weight_mean: float = float((weights * pitches).sum() / total_weights)\n",
    "    duration_weighted_variance: float = float((weights * (pitches - duration_weight_mean) ** 2).sum() / total_weights)\n",
    "    weighted_std: float = duration_weighted_variance ** 0.5\n",
    "\n",
    "    # ---------- Duration-weighted percentiles ----------\n",
    "    order: np.ndarray[np.int32] = np.argsort(pitches)\n",
    "    ordered_pitches, ordered_weights = pitches[order], weights[order]\n",
    "    cumulative_weight_sum: np.ndarray[np.float32] = np.cumsum(ordered_weights)\n",
    "\n",
    "    # Weighted quantile: find the first index where cumulative weight crosses q%.\n",
    "    def weighted_quantile(quantile: float) -> float:\n",
    "        # Target cumulative weight at quantile q (0..100).\n",
    "        target: float = (quantile / 100.0) * cumulative_weight_sum[-1]\n",
    "        # Index where cumulative_weight_sum >= target; take leftmost to be consistent.\n",
    "        idx: int = np.searchsorted(cumulative_weight_sum, target, side=\"left\")\n",
    "        return float(ordered_pitches[min(idx, len(ordered_pitches) - 1)])\n",
    "\n",
    "    # 10th / 50th (median) / 90th percentiles, duration-weighted.\n",
    "    percentile_10, percentile_50, percentile_90 = weighted_quantile(10), weighted_quantile(50), weighted_quantile(90)\n",
    "\n",
    "    # Ambitus = robust spread (p90 - p10), less sensitive than raw max - min.\n",
    "    return dict(mean=duration_weight_mean, \n",
    "                std=weighted_std,\n",
    "                p10=percentile_10, \n",
    "                p50=percentile_50, \n",
    "                p90=percentile_90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641869c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.499535Z",
     "start_time": "2025-10-14T14:25:48.490297Z"
    }
   },
   "outputs": [],
   "source": [
    "def tempo_stats(midi: PrettyMIDI) -> tuple[float, float, int]:\n",
    "    tempo_times, tempo_bpms = midi.get_tempo_changes()\n",
    "    if len(tempo_bpms) == 0:\n",
    "        tempo_times = np.array([0.0], dtype=np.float32)\n",
    "        tempo_bpms  = np.array([midi.estimate_tempo()], dtype=np.float32)\n",
    "\n",
    "    # duration-weighted mean tempo + dispersion proxy\n",
    "    duration_sec = midi.get_end_time()\n",
    "    segment_ends: np.ndarray[np.float32] = np.r_[tempo_times[1:], duration_sec]\n",
    "    segment_durs: np.ndarray[np.float32] = np.maximum(EPSILON, segment_ends - tempo_times[:len(segment_ends)])\n",
    "    tempo_mean: float = float(np.dot(tempo_bpms[:len(segment_durs)], segment_durs) / np.sum(segment_durs))\n",
    "    tempo_std: float = float(np.std(np.repeat(\n",
    "        tempo_bpms[:len(segment_durs)],\n",
    "        np.maximum(1, (segment_durs/np.sum(segment_durs)*1000).astype(int))\n",
    "    )))\n",
    "    tempo_change_count: int = int(len(tempo_bpms))\n",
    "    return tempo_mean, tempo_std, tempo_change_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cb53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.541792Z",
     "start_time": "2025-10-14T14:25:48.535574Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_melodic_material(midi: PrettyMIDI) -> tuple[list[Note], list[int]]:\n",
    "    notes: list[Note] = []\n",
    "    velocities: list[int] = []\n",
    "    for inst in midi.instruments:\n",
    "        if inst.is_drum or not inst.notes:\n",
    "            continue\n",
    "        notes.extend(inst.notes)\n",
    "        velocities.extend([n.velocity for n in inst.notes])\n",
    "    return notes, velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6cb436",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.586377Z",
     "start_time": "2025-10-14T14:25:48.581457Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def instrument_count_non_drum_with_notes(midi: PrettyMIDI) -> int:\n",
    "    return sum(1 for inst in midi.instruments if not inst.is_drum and inst.notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c65254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.632724Z",
     "start_time": "2025-10-14T14:25:48.624202Z"
    }
   },
   "outputs": [],
   "source": [
    "def note_duration_stats(notes: list[Note]) -> tuple[float, float, float, float, float, float]:\n",
    "    if not notes:\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    durs = np.array([max(EPSILON, n.end - n.start) for n in notes], dtype=np.float32)\n",
    "    mean = float(durs.mean())\n",
    "    std  = float(durs.std())\n",
    "    rng  = float(durs.max() - durs.min())\n",
    "    cv   = float(std / max(EPSILON, mean))\n",
    "    p10  = float(np.percentile(durs, 10))\n",
    "    p90  = float(np.percentile(durs, 90))\n",
    "    return mean, std, rng, cv, p10, p90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e1bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.677391Z",
     "start_time": "2025-10-14T14:25:48.670632Z"
    }
   },
   "outputs": [],
   "source": [
    "def ioi_stats(notes: list[Note]) -> tuple[float, float, float, float, float]:\n",
    "    if not notes:\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    starts = np.array(sorted(n.start for n in notes), dtype=np.float32)\n",
    "    if len(starts) < 2:\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    iois = np.diff(starts)\n",
    "    iois = np.clip(iois, EPSILON, None)\n",
    "    mean = float(iois.mean())\n",
    "    std  = float(iois.std())\n",
    "    cv   = float(std / max(EPSILON, mean))\n",
    "    short_ratio = float((iois <= NOTE_SAMPLING_INTERVAL_SHORT).mean())\n",
    "    long_ratio  = float((iois >= NOTE_SAMPLING_INTERVAL_LONG).mean())\n",
    "    return mean, std, cv, short_ratio, long_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6ee3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.722829Z",
     "start_time": "2025-10-14T14:25:48.716972Z"
    }
   },
   "outputs": [],
   "source": [
    "def rest_ratio_extraction(notes: list[Note], duration_sec: float) -> float:\n",
    "    if not notes or duration_sec <= 0:\n",
    "        return 0.0\n",
    "    notes_sorted = sorted(notes, key=lambda n: n.start)\n",
    "    gaps = []\n",
    "    for a, b in zip(notes_sorted[:-1], notes_sorted[1:]):\n",
    "        gap = max(0.0, b.start - a.end)\n",
    "        if gap > 0:\n",
    "            gaps.append(gap)\n",
    "    return float(sum(gaps) / max(EPSILON, duration_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce984e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.768886Z",
     "start_time": "2025-10-14T14:25:48.763591Z"
    }
   },
   "outputs": [],
   "source": [
    "def velocity_stats(velocities: list[int]) -> tuple[float, float, float, float]:\n",
    "    if not velocities:\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "    arr = np.array(velocities, dtype=np.float32)\n",
    "    vmin, vmax = float(arr.min()), float(arr.max())\n",
    "    vmean, vstd = float(arr.mean()), float(arr.std())\n",
    "    vcv = float(vstd / max(EPSILON, vmean))\n",
    "    return vmin, vmax, vmean, vstd if vmean == 0 else vcv  # keep both std and cv separately elsewhere if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144a6d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.813087Z",
     "start_time": "2025-10-14T14:25:48.804658Z"
    }
   },
   "outputs": [],
   "source": [
    "def velocity_min_max_mean_std_cv(velocities: list[int]) -> tuple[float, float, float, float, float]:\n",
    "    if not velocities:\n",
    "        return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    arr = np.array(velocities, dtype=np.float32)\n",
    "    vmin = float(arr.min())\n",
    "    vmax = float(arr.max())\n",
    "    vmean = float(arr.mean())\n",
    "    vstd  = float(arr.std())\n",
    "    vcv   = float(vstd / max(EPSILON, vmean))\n",
    "    return vmin, vmax, vmean, vstd, vcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e25514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.858515Z",
     "start_time": "2025-10-14T14:25:48.852178Z"
    }
   },
   "outputs": [],
   "source": [
    "def interval_stats(notes: list[Note]) -> tuple[float, float, float]:\n",
    "    if not notes:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    ns = sorted(notes, key=lambda n: n.start)\n",
    "    if len(ns) < 2:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    pitches = np.array([n.pitch for n in ns], dtype=np.float32)\n",
    "    steps = np.diff(pitches)\n",
    "    abs_steps = np.abs(steps)\n",
    "    if abs_steps.size == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    avg_abs = float(abs_steps.mean())\n",
    "    small_step_ratio = float((abs_steps <= 2).mean())\n",
    "    large_leap_ratio = float((abs_steps >= 7).mean())\n",
    "    return avg_abs, small_step_ratio, large_leap_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d3608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.904623Z",
     "start_time": "2025-10-14T14:25:48.899129Z"
    }
   },
   "outputs": [],
   "source": [
    "def pitch_time_slope(notes: list[Note]) -> float:\n",
    "    if not notes:\n",
    "        return 0.0\n",
    "    ns = sorted(notes, key=lambda n: n.start)\n",
    "    if len(ns) < 2:\n",
    "        return 0.0\n",
    "    t = np.array([n.start for n in ns], dtype=np.float32)\n",
    "    p = np.array([n.pitch for n in ns], dtype=np.float32)\n",
    "    t = (t - t.mean()) / (t.std() + EPSILON)\n",
    "    p = (p - p.mean()) / (p.std() + EPSILON)\n",
    "    return float(np.clip(np.corrcoef(t, p)[0,1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72354a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:48.954074Z",
     "start_time": "2025-10-14T14:25:48.948798Z"
    }
   },
   "outputs": [],
   "source": [
    "def distinct_pitch_and_repetition(notes: list[Note]) -> tuple[float, float]:\n",
    "    if not notes:\n",
    "        return 0.0, 0.0\n",
    "    pitches = [n.pitch for n in notes]\n",
    "    total = len(pitches)\n",
    "    distinct_ratio = float(len(set(pitches)) / max(1, total))\n",
    "    # repetition as mode frequency / total\n",
    "    values, counts = np.unique(pitches, return_counts=True)\n",
    "    repetition_ratio = float(counts.max() / total)\n",
    "    return distinct_ratio, repetition_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b3fd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.000899Z",
     "start_time": "2025-10-14T14:25:48.990448Z"
    }
   },
   "outputs": [],
   "source": [
    "def chroma_features(midi: PrettyMIDI) -> tuple[np.ndarray, float]:\n",
    "    chroma = midi.get_pitch_class_histogram(use_duration=True)\n",
    "    chroma = chroma / (np.sum(chroma) + EPSILON)\n",
    "    # Shannon entropy in nats\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        entropy = -np.nansum(chroma * np.log(chroma + EPSILON))\n",
    "    return chroma.astype(np.float32), float(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f53dc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.094797Z",
     "start_time": "2025-10-14T14:25:49.086984Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_category_rhythm_dynamics_midi_features(midi: PrettyMIDI) -> tuple[np.ndarray, list[str]]:\n",
    "    notes, velocities = collect_melodic_material(midi=midi)\n",
    "    duration_sec = midi.get_end_time()\n",
    "    tempo_mean, tempo_std, tempo_change_count = tempo_stats(midi)\n",
    "    ts_num, ts_den = most_common_time_signature(midi.time_signature_changes)\n",
    "    instr_count = instrument_count_non_drum_with_notes(midi)\n",
    "    note_density = float(len(notes) / max(EPSILON, duration_sec))\n",
    "\n",
    "    ioi_mean, ioi_std, ioi_cv, ioi_short_ratio, ioi_long_ratio = ioi_stats(notes)\n",
    "    rest_ratio = rest_ratio_extraction(notes, duration_sec)\n",
    "\n",
    "    dur_mean, dur_std, dur_range, dur_cv, dur_p10, dur_p90 = note_duration_stats(notes)\n",
    "\n",
    "    v_min, v_max, v_mean, v_std, v_cv = velocity_min_max_mean_std_cv(velocities)\n",
    "\n",
    "    names = [\n",
    "        \"duration_sec\",\n",
    "        \"tempo_mean_bpm\",\n",
    "        \"tempo_std_bpm\",\n",
    "        \"tempo_change_count\",\n",
    "        \"time_sig_num\",\n",
    "        \"time_sig_den\",\n",
    "        \"instrument_count\",\n",
    "        \"melody_note_density_per_sec\",\n",
    "        \"ioi_mean_sec\",\n",
    "        \"ioi_std_sec\",\n",
    "        \"ioi_cv\",\n",
    "        \"rest_ratio\",\n",
    "        \"note_durations_mean\",\n",
    "        \"note_durations_std\",\n",
    "        \"note_durations_range\",\n",
    "        \"note_durations_cv\",\n",
    "        \"note_duration_p10\",\n",
    "        \"note_duration_p90\",\n",
    "        \"short_note_ratio_le_0p10s\",\n",
    "        \"long_note_ratio_ge_0p50s\",\n",
    "        \"velocity_min\",\n",
    "        \"velocity_max\",\n",
    "        \"velocity_mean\",\n",
    "        \"velocity_std\",\n",
    "        \"velocity_cv\",\n",
    "    ]\n",
    "    vec = np.array([\n",
    "        duration_sec,\n",
    "        tempo_mean,\n",
    "        tempo_std,\n",
    "        tempo_change_count,\n",
    "        ts_num,\n",
    "        ts_den,\n",
    "        instr_count,\n",
    "        note_density,\n",
    "        ioi_mean,\n",
    "        ioi_std,\n",
    "        ioi_cv,\n",
    "        rest_ratio,\n",
    "        dur_mean,\n",
    "        dur_std,\n",
    "        dur_range,\n",
    "        dur_cv,\n",
    "        dur_p10,\n",
    "        dur_p90,\n",
    "        ioi_short_ratio,\n",
    "        ioi_long_ratio,\n",
    "        v_min,\n",
    "        v_max,\n",
    "        v_mean,\n",
    "        v_std,\n",
    "        v_cv,\n",
    "    ], dtype=np.float32)\n",
    "    return vec, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4df82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.145618Z",
     "start_time": "2025-10-14T14:25:49.137586Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_category_pitch_harmony_midi_features(midi: PrettyMIDI) -> tuple[np.ndarray, list[str]]:\n",
    "    notes, _  = collect_melodic_material(midi=midi)\n",
    "    # duration-weighted pitch stats you already have\n",
    "    dwp = get_duration_weighted_pitch_stats(notes)\n",
    "    pitch_mean = float(dwp[\"mean\"])\n",
    "    pitch_std  = float(dwp[\"std\"])\n",
    "    pitch_p10  = float(dwp[\"p10\"])\n",
    "    pitch_p50  = float(dwp[\"p50\"])\n",
    "    pitch_p90  = float(dwp[\"p90\"])\n",
    "    ambitus    = float(pitch_p90 - pitch_p10)\n",
    "\n",
    "    avg_abs_interval, small_step_ratio, large_leap_ratio = interval_stats(notes)\n",
    "    slope = pitch_time_slope(notes)\n",
    "    distinct_ratio, repetition_ratio = distinct_pitch_and_repetition(notes)\n",
    "\n",
    "    chroma, chroma_entropy = chroma_features(midi)\n",
    "\n",
    "    names = [\n",
    "        \"pitch_mean_dw\",\n",
    "        \"pitch_std_dw\",\n",
    "        \"pitch_p10_dw\",\n",
    "        \"pitch_p50_dw\",\n",
    "        \"pitch_p90_dw\",\n",
    "        \"pitch_ambitus_p90_p10\",\n",
    "        \"avg_abs_interval\",\n",
    "        \"small_step_ratio_le_2st\",\n",
    "        \"large_leap_ratio_ge_7st\",\n",
    "        \"pitch_time_slope_corr\",\n",
    "        \"distinct_pitch_ratio\",\n",
    "        \"repetition_ratio_mode\",\n",
    "        \"chroma_entropy\",\n",
    "    ] + [f\"chroma_{i}\" for i in range(12)]\n",
    "\n",
    "    vec = np.array([\n",
    "        pitch_mean,\n",
    "        pitch_std,\n",
    "        pitch_p10,\n",
    "        pitch_p50,\n",
    "        pitch_p90,\n",
    "        ambitus,\n",
    "        avg_abs_interval,\n",
    "        small_step_ratio,\n",
    "        large_leap_ratio,\n",
    "        slope,\n",
    "        distinct_ratio,\n",
    "        repetition_ratio,\n",
    "        chroma_entropy,\n",
    "        *chroma.tolist(),  # 12 elements, each representing a pitch class.\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return vec, names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c68e2f",
   "metadata": {},
   "source": [
    "<font size=6>Auxlilliary Data Structures</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f7e0b",
   "metadata": {},
   "source": [
    "Auxilliary functions for creation of word sequences and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ba93f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.199240Z",
     "start_time": "2025-10-14T14:25:49.190912Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_word_sequences_with_targets(tokenized_lyrics: list[str], sequence_length: int = SEQUENCE_LENGTH) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given tokenized lyrics as a list of strings, create sequences of word indices and their corresponding target word indices.\n",
    "    Each sequence is of length `sequence_length`, and the target_sequence is a list of the next words of the sequence 1 index higher..\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Create sequences and targets\n",
    "    for i in range(len(tokenized_lyrics) - sequence_length):\n",
    "        seq = tokenized_lyrics[i:i + sequence_length] # Padding\n",
    "        target_sequence = tokenized_lyrics[i + 1: i + sequence_length + 1]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target_sequence)\n",
    "    \n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533208f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.241913Z",
     "start_time": "2025-10-14T14:25:49.236209Z"
    }
   },
   "outputs": [],
   "source": [
    "class SongData:\n",
    "    def __init__(self, song_data_cell: list[str] = None, midi_file: PrettyMIDI = None):\n",
    "        if len(song_data_cell) != 3:\n",
    "            raise ValueError(\"song_data_cell must have exactly three elements: [artist, title, lyrics]\")\n",
    "        self.artist = song_data_cell[0]\n",
    "        self.title = song_data_cell[1]\n",
    "        self.lyrics = song_data_cell[2]\n",
    "        self.raw_midi_data = midi_file\n",
    "        self._midi_harmony_and_pitch_features: Optional[dict[str, np.ndarray]] = None\n",
    "        self._midi_rhythm_and_dynamic_features: Optional[dict[str, np.ndarray]] = None\n",
    "\n",
    "    @property\n",
    "    def midi_harmony_and_pitch_features(self):\n",
    "        if self._midi_harmony_and_pitch_features is None:\n",
    "            self._midi_harmony_and_pitch_features = extract_category_pitch_harmony_midi_features(self.raw_midi_data)\n",
    "        return self._midi_harmony_and_pitch_features\n",
    "    \n",
    "    @property\n",
    "    def midi_rhythm_and_dynamic_features(self):\n",
    "        if self._midi_rhythm_and_dynamic_features is None:\n",
    "            self._midi_rhythm_and_dynamic_features = extract_category_rhythm_dynamics_midi_features(self.raw_midi_data)\n",
    "        return self._midi_rhythm_and_dynamic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec874e9",
   "metadata": {},
   "source": [
    "<font size=6>Reading CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8efdde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.295459Z",
     "start_time": "2025-10-14T14:25:49.283798Z"
    }
   },
   "outputs": [],
   "source": [
    "class SongDataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                songs_data: list[SongData],\n",
    "                word_embeddings: dict[str, np.ndarray],\n",
    "                artist_to_index_data_set: dict[str, int],\n",
    "                word_to_id_data_set: dict[str, int],\n",
    "                midi_features_to_use: str):\n",
    "        self.midi_features: list[np.ndarray] = list()\n",
    "        self.artists: list[str] = list()\n",
    "        self.sequence_artists: list[str] = list()\n",
    "        self.word_sequences: list[str] = list()\n",
    "        self.sequences_targets: list[str] = list()\n",
    "        self.sequence_to_midi: list[int] = list() # Maps each sequence to its corresponding MIDI feature index\n",
    "        self.sequence_to_artist: list[int] = list() # Maps each sequence to its corresponding artist embedding index\n",
    "        self.word_embeddings: dict[str, np.ndarray] = word_embeddings\n",
    "        self.artist_to_index: dict[str, int] = artist_to_index_data_set\n",
    "        self.word_to_id: dict[str, int] = word_to_id_data_set\n",
    "        # Instead of saving each sequence's MIDI features, we save the index of the MIDI features in the midi_features list to save space.\n",
    "        for idx, song in enumerate(songs_data):\n",
    "            sequences, targets = create_word_sequences_with_targets(song.lyrics)\n",
    "            self.word_sequences.extend(sequences)\n",
    "            self.sequences_targets.extend(targets)\n",
    "            if midi_features_to_use == HARMONY_FEATURES:\n",
    "                extracted_features, _ = song.midi_harmony_and_pitch_features\n",
    "            elif midi_features_to_use == RHYTHM_FEATURES:\n",
    "                extracted_features, _ = song.midi_rhythm_and_dynamic_features\n",
    "            else:\n",
    "                raise ValueError(f\"Type of features extraction not supported: {midi_features_to_use}\")\n",
    "            self.midi_features.append(extracted_features) # Creates a mapping of the features to the sequences.\n",
    "            self.sequence_artists.append(song.artist)\n",
    "            self.sequence_to_midi.extend([idx] * len(sequences))\n",
    "            self.sequence_to_artist.extend([idx] * len(sequences))\n",
    "        print(f'Dataset with {midi_features_to_use} has: {len(self.word_sequences)} sequences and {len(self.sequences_targets)} targets')\n",
    "\n",
    "\n",
    "    def word_vec(self, tok: str) -> np.ndarray:\n",
    "        # helper to get word vector, or zeros if no embedding\n",
    "        v = self.word_embeddings.get(tok)\n",
    "        if v is None:\n",
    "            sample = next(iter(self.word_embeddings.values())) # Get size.\n",
    "            v = np.zeros_like(sample, dtype=np.float32)\n",
    "        return v.astype(np.float32, copy=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_sequences)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        tokens = self.word_sequences[idx]                # list[str], len T\n",
    "        target_tokens = self.sequences_targets[idx]      # list[str], len T\n",
    "        midi = self.midi_features[self.sequence_to_midi[idx]].astype(np.float32, copy=False)\n",
    "        artist_name = self.sequence_artists[self.sequence_to_artist[idx]]\n",
    "        artist_idx = np.float32(self.artist_to_index[artist_name])\n",
    "\n",
    "        emb = np.stack([self.word_vec(tok) for tok in tokens], axis=0).astype(np.float32, copy=False)     # [T,E]\n",
    "        midi_b = np.broadcast_to(midi, (emb.shape[0], midi.shape[0])).astype(np.float32, copy=False)      # [T,M]\n",
    "        artist_b = np.full((emb.shape[0], 1), artist_idx, dtype=np.float32)                               # [T,1]\n",
    "\n",
    "        concatenated_features = np.concatenate((emb, midi_b, artist_b), axis=1).astype(np.float32, copy=False)                # [T,D]\n",
    "        target_words = np.asarray([self.word_to_id.get(target_token, self.word_to_id.get(UNK_STRING, 0)) for target_token in target_tokens],\n",
    "                   dtype=np.int64)\n",
    "        return concatenated_features, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1959fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.350180Z",
     "start_time": "2025-10-14T14:25:49.336700Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(LYRIC_TRAIN_SET_CSV_PATH, mode='r', encoding='utf-8') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    lyric_train_data = list(reader)\n",
    "\n",
    "with open(LYRIC_TEST_SET_CSV_PATH, mode='r', encoding='utf-8') as test_file:\n",
    "    reader = csv.reader(test_file)\n",
    "    lyric_test_data = list(reader)\n",
    "\n",
    "if len(lyric_train_data) < 1 or len(lyric_test_data) < 1:\n",
    "    raise Exception(\"CSV files are empty or not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996afc4a",
   "metadata": {},
   "source": [
    "<font size=6>Parsing CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5d31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.447417Z",
     "start_time": "2025-10-14T14:25:49.386089Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_csv_data(raw_csv_data: list[list[str]]) -> list[tuple[str, str, list[str]]]:\n",
    "    returned_cleaned_csv_data: list[tuple[str, str, list[str]]] = []\n",
    "    for row in raw_csv_data:\n",
    "        artist = row[0].strip()\n",
    "        title_index = 1\n",
    "        lyrics_index = 2\n",
    "        while lyrics_index < len(row):\n",
    "            title = row[title_index].strip()\n",
    "            title = title.removesuffix('-2') # Remove '-2' suffix if present, relevant in 1 case.\n",
    "            title = title.strip()\n",
    "            lyrics = row[lyrics_index].strip()\n",
    "            lyrics = lyrics.lower()\n",
    "            lyrics = re.sub(f\"[{re.escape('\\'')}]\", \"\", lyrics) # Removing apostrophes.\n",
    "            lyrics = re.sub(f\"[{re.escape('-')}]\", \" \", lyrics) # Removing hyphens.\n",
    "            lyrics = re.sub(f\"[{re.escape('&')}]\", f\" {'PLACEHOLDERSTRING'} \", lyrics) # Putting PLACEHOLDER to not be removed by string_punctuation.\n",
    "            lyrics = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", lyrics) # Removing punctuation.\n",
    "            lyrics = re.sub('PLACEHOLDERSTRING', EOL_STRING, lyrics) # Putting PLACEHOLDER to not be removed by string_punctuation.\n",
    "            lyrics = lyrics.split(' ') # Tokenzing each word by space.\n",
    "            lyrics = [word.strip() for word in lyrics if word] # Removing empty strings.\n",
    "            lyrics.append(EOS_STRING) # Adding end of song token.\n",
    "            if len(title) > 0 and len(lyrics) > 0:\n",
    "                returned_cleaned_csv_data.append((artist, title, lyrics))\n",
    "            title_index += 2\n",
    "            lyrics_index += 2\n",
    "    return returned_cleaned_csv_data\n",
    "\n",
    "cleaned_lyric_train_data = clean_csv_data(lyric_train_data)\n",
    "cleaned_lyric_test_data = clean_csv_data(lyric_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca9578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.540958Z",
     "start_time": "2025-10-14T14:25:49.492467Z"
    }
   },
   "outputs": [],
   "source": [
    "# count the number of unique words in the lyrics\n",
    "def get_word_frequencies(lyrics_data: list[tuple[str, str, list[str]]]) -> dict[str, int]:\n",
    "    words_frequency = defaultdict(int)\n",
    "    for _, _, lyrics in lyrics_data:\n",
    "        for word in lyrics:\n",
    "            words_frequency[word] += 1\n",
    "    return words_frequency    \n",
    "word_frequencies_training: dict[str, int] = get_word_frequencies(cleaned_lyric_train_data)\n",
    "word_frequencies_test: dict[str, int] = get_word_frequencies(cleaned_lyric_test_data)\n",
    "print(f\"Number of unique words in training set: {len(word_frequencies_training)}\")\n",
    "print(f\"Number of unique words in test set: {len(word_frequencies_test)}\")\n",
    "\n",
    "d_sorted_by_val = sorted(word_frequencies_training.items(), key=lambda kv: kv[1], reverse=True)\n",
    "d_sorted_by_val[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141709c5",
   "metadata": {},
   "source": [
    "<font size=6>Reading MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80433ac1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:49.594853Z",
     "start_time": "2025-10-14T14:25:49.584871Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_midi_files(midi_files_location: str, pickling_path: Optional[str] = None, failed_loads_path: Optional[str] = None) -> \\\n",
    "                    tuple[dict[str, dict[str, PrettyMIDI]], dict[str, set[str]]]: # artist -> title -> PrettyMIDI, failed loads[artist, song_set]\n",
    "    failed_loads = dict()\n",
    "    if failed_loads_path is not None and os.path.isfile(failed_loads_path):\n",
    "        with open(failed_loads_path, \"rb\") as f:\n",
    "            failed_loads = pickle.load(f)\n",
    "        print(f\"Loaded failed MIDI loads from pickled file {failed_loads_path}.\")\n",
    "    if pickling_path is not None and os.path.isfile(pickling_path):\n",
    "        with open(pickling_path, \"rb\") as f:\n",
    "            loaded_midi_files = pickle.load(f)\n",
    "        print(f\"Loaded MIDI files from pickled file {pickling_path}.\")\n",
    "        print(f'Loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.')\n",
    "        return loaded_midi_files, failed_loads\n",
    "    if not os.path.isdir(midi_files_location):\n",
    "        raise ValueError(f\"MIDI file path {midi_files_location} is not a valid directory.\")\n",
    "\n",
    "    # Traversing over all files and attempt to load them with pretty_midi:\n",
    "    loaded_midi_files: dict[str, dict[str, PrettyMIDI]] = defaultdict(dict) # artist -> title -> PrettyMIDI\n",
    "    failed_loads: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    for file in os.listdir(midi_files_location):\n",
    "        if file.endswith('.mid') or file.endswith('.midi'):\n",
    "            file_path = os.path.join(midi_files_location, file)\n",
    "            file = file.removesuffix('.mid')\n",
    "            splitted_artist_and_title = file.split('_-_')\n",
    "            artist = splitted_artist_and_title[0]\n",
    "            title = splitted_artist_and_title[1]\n",
    "            if len(splitted_artist_and_title) > 2:\n",
    "                print(f\"Warning: file {file} has more than one '_-_' separator, ignoring the rest after second \\\"_-_\\\".\")\n",
    "            artist = artist.replace('_', ' ').strip().lower()\n",
    "            title = title.replace('_', ' ').strip().lower()\n",
    "            try:\n",
    "                midi_data = PrettyMIDI(file_path)\n",
    "                loaded_midi_files[artist][title] = midi_data\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file}: {e}\")\n",
    "                failed_loads[artist].add(title)\n",
    "\n",
    "\n",
    "\n",
    "    if failed_loads:\n",
    "        print(\"Failed to load the following artist and lyric midi files:\")\n",
    "        for artist, lyrics in failed_loads.items():\n",
    "            print(f\"{artist} - [{', '.join(lyrics)}]\")\n",
    "\n",
    "    if pickling_path is not None:\n",
    "        with open(pickling_path, \"wb\") as f:\n",
    "            pickle.dump(loaded_midi_files, f)\n",
    "            print(f\"Pickled loaded MIDI files to {pickling_path}.\")\n",
    "    if failed_loads_path is not None:\n",
    "        with open(failed_loads_path, \"wb\") as f:\n",
    "            pickle.dump(failed_loads, f)\n",
    "            print(f\"Pickled failed MIDI loads to {failed_loads_path}.\")\n",
    "\n",
    "    print(f\"Successfully loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.\")\n",
    "    if failed_loads:\n",
    "        print(\"Failed to load the following midi files:\")\n",
    "        for failed_load in failed_loads:\n",
    "            print(failed_loads)\n",
    "    return loaded_midi_files, failed_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a752d6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:57.807474Z",
     "start_time": "2025-10-14T14:25:49.635216Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_midi_files, failed_midi_loads = load_midi_files(MIDI_FILE_PATH, PICKLING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ffe14",
   "metadata": {},
   "source": [
    "<font size=6>Mapping CSV data to MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e5010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:57.850743Z",
     "start_time": "2025-10-14T14:25:57.844698Z"
    }
   },
   "outputs": [],
   "source": [
    "def csv_data_to_songdata_list(csv_data: list[list[str]], \n",
    "                              failed_midi_load: dict[str, set[str]], \n",
    "                              midi_files_dict: dict[str, dict[str, PrettyMIDI]]) -> list[SongData]:\n",
    "    song_data_list: list[SongData] = list()\n",
    "    missing_midi_count = 0\n",
    "    for row in csv_data:\n",
    "        artist = row[0]\n",
    "        title = row[1]\n",
    "        if artist in failed_midi_load and title in failed_midi_load[artist]:\n",
    "            print(f\"Skipping {artist} - {title} due to previous MIDI load failure.\")\n",
    "            continue\n",
    "        if artist in midi_files_dict and title in midi_files_dict[artist]:\n",
    "            midi_file = midi_files_dict[artist][title]\n",
    "            song_data = SongData(row, midi_file)\n",
    "            song_data_list.append(song_data)\n",
    "        else:\n",
    "            missing_midi_count += 1\n",
    "            print(f\"Missing MIDI file for artist '{artist}' and title '{title}'\")\n",
    "    print(f\"Total songs with missing MIDI files: {missing_midi_count}\")\n",
    "    return song_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f061cd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:58.597533Z",
     "start_time": "2025-10-14T14:25:57.894160Z"
    }
   },
   "outputs": [],
   "source": [
    "train_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_train_data, failed_midi_loads, loaded_midi_files)\n",
    "test_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_test_data, failed_midi_loads, loaded_midi_files)\n",
    "print(f\"Total training songs with MIDI data: {len(train_midi_data)}\")\n",
    "print(f\"Total test songs with MIDI data: {len(test_midi_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0f1d",
   "metadata": {},
   "source": [
    "<font size=6>Handling word embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95f984",
   "metadata": {},
   "source": [
    "Downloading pretrained word2vec, containing 300 dims, trained on news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0482d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:26:55.162787Z",
     "start_time": "2025-10-14T14:25:58.641365Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bebb8",
   "metadata": {},
   "source": [
    "Extracting the vocabulary from the lyrics.\n",
    "Getting the data from the test set aswell since the vocbulary needs to be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bca59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:26:55.236292Z",
     "start_time": "2025-10-14T14:26:55.203733Z"
    }
   },
   "outputs": [],
   "source": [
    "lyrics_vocabulary: set[str] = set()\n",
    "# Getting the data from the test set aswell since the vocbulary needs to be known\n",
    "for song in train_midi_data + test_midi_data:\n",
    "    for word in song.lyrics:\n",
    "        lyrics_vocabulary.add(word)\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305abc9",
   "metadata": {},
   "source": [
    "Creating unified embedding.\n",
    "Extracting embeddings from word2vec and using random embeddings for words not found in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2575f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:26:55.306996Z",
     "start_time": "2025-10-14T14:26:55.276858Z"
    }
   },
   "outputs": [],
   "source": [
    "unified_embeddings: dict[str, np.ndarray] = dict()\n",
    "existing_words_in_pretrained = 0\n",
    "not_existing_in_pretrained = 0\n",
    "added_stopwords = 0\n",
    "for word in list(lyrics_vocabulary):\n",
    "    if word in pretrained_word2vec:\n",
    "        unified_embeddings[word] = pretrained_word2vec[word]\n",
    "        existing_words_in_pretrained += 1\n",
    "    else:\n",
    "        unified_embeddings[word] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,)) # Random init for unknown words.  \n",
    "        not_existing_in_pretrained += 1\n",
    "    # Adding stopwords as well, since they are common and should be in the vocabulary.\n",
    "for stopword in stopwords.words('english'):\n",
    "    cleaned_stopword = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", stopword.strip().lower()) # Cleaning the stopword, since it contains punctuation.\n",
    "    if cleaned_stopword not in unified_embeddings:\n",
    "        unified_embeddings[cleaned_stopword] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "        added_stopwords += 1\n",
    "# Adding special tokens\n",
    "unified_embeddings[EOL_STRING] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "unified_embeddings[UNK_STRING] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "unified_embeddings[EOS_STRING] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")\n",
    "print(f\"Existing words in pretrained embeddings: {existing_words_in_pretrained}\")\n",
    "print(f\"Not existing in pretrained embeddings (randomly initialized): {not_existing_in_pretrained}\")\n",
    "print(f\"Added stopwords (randomly initialized): {added_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b23e9d",
   "metadata": {},
   "source": [
    "<font size=6>Handling artist embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f40eba",
   "metadata": {},
   "source": [
    "Using simple indexing for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7caf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:26:55.354142Z",
     "start_time": "2025-10-14T14:26:55.346300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_artists = [song.artist for song in train_midi_data]\n",
    "test_artists = [song.artist for song in test_midi_data]\n",
    "artist_set: set = (set(train_artists).union(set(test_artists)))\n",
    "artist_to_index: dict[str, int] = dict()\n",
    "index_to_artist: dict[int, str] = dict()\n",
    "for index, artist in enumerate(artist_set):\n",
    "    artist_to_index[artist] = index\n",
    "    index_to_artist[index] = artist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfa020",
   "metadata": {},
   "source": [
    "<font size=6>Model 1: Simple concatenation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0b392",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:26:55.397809Z",
     "start_time": "2025-10-14T14:26:55.392178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Integration Method 1: Simple Concatenation - Melody features are concatenated to each word embedding\n",
    "class LyricsGenerator_Concatenation(nn.Module):\n",
    "  def __init__(self, \n",
    "               vocab_size: int, \n",
    "               input_size: int, \n",
    "               hidden_layer_dim: int, \n",
    "               size_of_midi_features: int = NUMBER_OF_EXTRACT_MIDI_FEATURES,\n",
    "               size_of_word_embeddings: int = WORD_EMBEDDING_SIZE,\n",
    "               num_layers: int = LSTM_LAYERS, \n",
    "               dropout_rate: float = DROPOUT\n",
    "               ):\n",
    "    super(LyricsGenerator_Concatenation, self).__init__()\n",
    "    self.vocab_size: int = vocab_size\n",
    "    self.num_layers: int = num_layers\n",
    "    self.word_embedding_size: int = size_of_word_embeddings\n",
    "    self.size_of_midi_features: int = size_of_midi_features\n",
    "    self.lstm = nn.LSTM(input_size, hidden_layer_dim, num_layers,\n",
    "                        batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    self.fc = nn.Linear(hidden_layer_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    lstm_out, _ = self.lstm(x)\n",
    "    output_post_dropout = self.dropout(lstm_out)\n",
    "    logits = self.fc(output_post_dropout)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a569db",
   "metadata": {},
   "source": [
    "<font size=6>Model 2: Attention</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea633444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:26:55.508077Z",
     "start_time": "2025-10-14T14:26:55.438756Z"
    }
   },
   "outputs": [],
   "source": [
    "class LyricsGenerator_Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 hidden_layer_dim: int = HIDDEN_LAYER_DIM_ATTENTION,\n",
    "                 size_of_word_embeddings: int = WORD_EMBEDDING_SIZE,\n",
    "                 size_of_midi_features: int = NUMBER_OF_EXTRACT_MIDI_FEATURES,\n",
    "                 size_of_artist_index_dim: int = SIZE_OF_ARTIST_INDEX,\n",
    "                 num_layers: int = 1,\n",
    "                 dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = size_of_word_embeddings\n",
    "        self.melody_dim = size_of_midi_features\n",
    "        self.artist_dim = size_of_artist_index_dim\n",
    "        self.hidden = hidden_layer_dim\n",
    "        self.size_of_artist_index_dim = size_of_artist_index_dim\n",
    "        self.hidden_layer_dim = hidden_layer_dim\n",
    "\n",
    "        # Separate encoders\n",
    "        self.word_rnn = nn.LSTM(self.embedding_dim, self.hidden_layer_dim, num_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "        # MIDI & Artist: static per song -> pool to a single token then project\n",
    "        self.midi_proj   = nn.Linear(self.melody_dim, self.hidden_layer_dim, bias=True)\n",
    "        self.artist_proj = nn.Linear(self.artist_dim, self.hidden_layer_dim, bias=True)\n",
    "\n",
    "        # Per-modality attention projections (keys/values); queries shared\n",
    "        self.k_word = nn.Linear(self.hidden_layer_dim, self.hidden_layer_dim, bias=False)\n",
    "        self.v_word = nn.Linear(self.hidden_layer_dim, self.hidden_layer_dim, bias=False)\n",
    "\n",
    "        self.k_midi = nn.Linear(self.hidden_layer_dim, self.hidden_layer_dim, bias=False)    # operates on pooled MIDI token\n",
    "        self.v_midi = nn.Linear(self.hidden_layer_dim, self.hidden_layer_dim, bias=False)\n",
    "\n",
    "        self.k_artist = nn.Linear(self.hidden_layer_dim, self.hidden_layer_dim, bias=False)  # operates on pooled Artist token\n",
    "        self.v_artist = nn.Linear(self.hidden_layer_dim, self.hidden_layer_dim, bias=False)\n",
    "\n",
    "        # Shared query from global summary (word_last | midi_tok | artist_tok)\n",
    "        self.q_proj = nn.Linear(3 * self.hidden_layer_dim, self.hidden_layer_dim, bias=False)\n",
    "\n",
    "        # Late fusion over 3 heads\n",
    "        self.fusion_scorer = nn.Linear(3 * self.hidden_layer_dim, 3)\n",
    "\n",
    "        # Per-step fusion of [ctx || base] used by attn_seq_logits\n",
    "        self.combine_proj_seq = nn.Linear(2 * self.hidden_layer_dim, self.hidden_layer_dim)\n",
    "\n",
    "        # Output head\n",
    "        self.combine_proj = nn.Linear(self.hidden_layer_dim + 3 * self.hidden_layer_dim, self.hidden_layer_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(self.hidden_layer_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: [B, T, E+M+A] = [word_emb || midi || artist]\n",
    "        returns:\n",
    "          logits: [B, V]\n",
    "        \"\"\"\n",
    "        x = x.float()\n",
    "        _, _, features = x.shape\n",
    "        E, M, A, H = self.embedding_dim, self.melody_dim, self.artist_dim, self.hidden\n",
    "        assert features == (E + M + A)\n",
    "\n",
    "        # Split\n",
    "        word = x[..., :E]               # [B,T,E]\n",
    "        midi = x[..., E:E+M]            # [B,T,M] (likely static over T)\n",
    "        art  = x[..., E+M:E+M+A]        # [B,T,A] (likely static over T)\n",
    "\n",
    "        # Encoders\n",
    "        word_out, (word_last, _) = self.word_rnn(word)   # word_out: [B,T,H]\n",
    "        word_last = word_last[-1]                        # [B,H]\n",
    "\n",
    "        # Pool static modalities to a single token per song\n",
    "        midi_tok   = self.midi_proj(midi.mean(dim=1))    # [B,H]\n",
    "        artist_tok = self.artist_proj(art.mean(dim=1))   # [B,H]\n",
    "\n",
    "        # Shared query\n",
    "        global_last = torch.cat([word_last, midi_tok, artist_tok], dim=-1)  # [B,3H]\n",
    "        q = self.q_proj(global_last).unsqueeze(1)                            # [B,1,H]\n",
    "\n",
    "        # --- Head 1: WORD (temporal over T) ---\n",
    "        Kw = self.k_word(word_out)                               # [B,T,H]\n",
    "        Vw = self.v_word(word_out)                               # [B,T,H]\n",
    "        scores_w = torch.bmm(q, Kw.transpose(1, 2)).squeeze(1) / math.sqrt(H)  # [B,T]\n",
    "        attn_w = F.softmax(scores_w, dim=-1)                     # [B,T]\n",
    "\n",
    "        ctx_w = torch.bmm(attn_w.unsqueeze(1), Vw).squeeze(1)    # [B,H]\n",
    "        # --- Head 2: MIDI (single token) ---\n",
    "        Vm = self.v_midi(midi_tok).unsqueeze(1)                  # [B,1,H]\n",
    "        ctx_m = Vm.squeeze(1)                                     # [B,H]\n",
    "\n",
    "        # --- Head 3: ARTIST (single token) ---\n",
    "        Va = self.v_artist(artist_tok).unsqueeze(1)              # [B,1,H]\n",
    "        ctx_a = Va.squeeze(1)                                     # [B,H]\n",
    "\n",
    "        # Late fusion across heads\n",
    "        ctx_cat = torch.cat([ctx_w, ctx_m, ctx_a], dim=-1)          # [B,3H]\n",
    "        alpha = F.softmax(self.fusion_scorer(ctx_cat), dim=-1)       # [B,3]\n",
    "        w_w, w_m, w_a = alpha.unbind(dim=-1)                         # each [B]\n",
    "        fused = (w_w.unsqueeze(-1)*ctx_w +\n",
    "                 w_m.unsqueeze(-1)*ctx_m +\n",
    "                 w_a.unsqueeze(-1)*ctx_a)                            # [B,H]\n",
    "\n",
    "        combined = torch.tanh(self.combine_proj(torch.cat([fused, global_last], dim=-1)))  # [B,H]\n",
    "        logits = self.fc(self.dropout(combined))                     # [B,V]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02346871",
   "metadata": {},
   "source": [
    "<font size=6>Running the models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01758d1d",
   "metadata": {},
   "source": [
    "Simple word-to-index and index-to-word for word generation afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187aed2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:26:55.555081Z",
     "start_time": "2025-10-14T14:26:55.543601Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_id = {word_in_vocab: index_of_word for index_of_word, word_in_vocab in enumerate(unified_embeddings.keys())} \n",
    "id_to_word = {index_of_word: word_in_vocab for word_in_vocab, index_of_word in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c7296",
   "metadata": {},
   "source": [
    "<font size=4>Preparing dataset and dataloaders</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acba32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:27:10.994805Z",
     "start_time": "2025-10-14T14:26:55.590788Z"
    }
   },
   "outputs": [],
   "source": [
    "songdata_train_dataset_harmony_midi_features = SongDataset(train_midi_data, unified_embeddings, artist_to_index, word_to_id, HARMONY_FEATURES)\n",
    "songdata_train_dataset_rhythm_midi_features = SongDataset(train_midi_data, unified_embeddings, artist_to_index, word_to_id, RHYTHM_FEATURES)\n",
    "\n",
    "songdata_test_dataset_harmony_midi_features = SongDataset(train_midi_data, unified_embeddings, artist_to_index, word_to_id, HARMONY_FEATURES)\n",
    "songdata_test_dataset_rhythm_midi_features = SongDataset(train_midi_data, unified_embeddings, artist_to_index, word_to_id, RHYTHM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d754492",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:27:30.146785Z",
     "start_time": "2025-10-14T14:27:11.030465Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------ HARMONY ------------------------------\n",
    "training_set_harmony, validation_set_harmony = train_test_split(songdata_train_dataset_harmony_midi_features, test_size=VALIDATION_SPLIT, random_state=RANDOM_LOADER_SEED, shuffle=True)\n",
    "training_data_loader_harmony = data.DataLoader(training_set_harmony, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validation_data_loader_harmony = data.DataLoader(validation_set_harmony, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_data_loader_harmony = data.DataLoader(songdata_test_dataset_harmony_midi_features, batch_size=5, shuffle=False) # That's the size of the test set.\n",
    "\n",
    "# ------------------------------ RHYTHM ------------------------------\n",
    "training_set_rhythm, validation_set_rhythm = train_test_split(songdata_train_dataset_rhythm_midi_features, test_size=VALIDATION_SPLIT, random_state=RANDOM_LOADER_SEED, shuffle=True)\n",
    "training_data_loader_rhythm = data.DataLoader(training_set_rhythm, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validation_data_loader_rhythm = data.DataLoader(validation_set_rhythm, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_data_loader_rhythm = data.DataLoader(songdata_test_dataset_rhythm_midi_features, batch_size=5, shuffle=False) # That's the size of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8b7b3",
   "metadata": {},
   "source": [
    "Helper function for the attention model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab17bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:27:30.196711Z",
     "start_time": "2025-10-14T14:27:30.189245Z"
    }
   },
   "outputs": [],
   "source": [
    "def attn_seq_logits(model: nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "    # x: [B,T,E+M+A] -> logits [B,T-1,V]\n",
    "    x = x.float()\n",
    "    B, T, _ = x.shape\n",
    "    E, M, A, H = model.embedding_dim, model.melody_dim, model.artist_dim, model.hidden_layer_dim\n",
    "\n",
    "    # split\n",
    "    word   = x[..., :E]            # [B,T,E]\n",
    "    midi   = x[..., E:E+M]         # [B,T,M]\n",
    "    artist = x[..., E+M:E+M+A]     # [B,T,A]\n",
    "\n",
    "    # word encoder\n",
    "    rnn_out, (h_n, _) = model.word_rnn(word)     # rnn_out: [B,T,H]\n",
    "    h_last = h_n[-1]                              # [B,H]\n",
    "\n",
    "    # pooled static tokens\n",
    "    midi_tok   = model.midi_proj(midi.mean(1))    # [B,H]\n",
    "    artist_tok = model.artist_proj(artist.mean(1))# [B,H]\n",
    "\n",
    "    # query from global summary\n",
    "    global_last = torch.cat([h_last, midi_tok, artist_tok], dim=-1)  # [B,3H]\n",
    "    q = model.q_proj(global_last).unsqueeze(1)                       # [B,1,H]\n",
    "\n",
    "    # attention over word timeline\n",
    "    K = model.k_word(rnn_out)                         # [B,T,H]\n",
    "    V = model.v_word(rnn_out)                         # [B,T,H]\n",
    "    scores = torch.bmm(q, K.transpose(1, 2)).squeeze(1) / math.sqrt(H)  # [B,T]\n",
    "    attn = torch.softmax(scores, dim=-1)              # [B,T]\n",
    "    context = torch.bmm(attn.unsqueeze(1), V).squeeze(1)                # [B,H]\n",
    "\n",
    "    # per-step combine with base hidden states\n",
    "    base = rnn_out[:, :-1, :]                         # [B,T-1,H]\n",
    "    ctx  = context.unsqueeze(1).expand(-1, base.size(1), -1)  # [B,T-1,H]\n",
    "    combined = torch.tanh(model.combine_proj_seq(torch.cat([ctx, base], dim=-1)))  # [B,T-1,H]\n",
    "    logits = model.fc(model.dropout(combined))        # [B,T-1,V]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff6e36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:27:30.237764Z",
     "start_time": "2025-10-14T14:27:30.232967Z"
    }
   },
   "outputs": [],
   "source": [
    "def seq_logits(model, model_input):  # returns [B, T-1, V] for both model types\n",
    "    if isinstance(model, LyricsGenerator_Attention):\n",
    "        return attn_seq_logits(model, model_input)               # [B,T-1,V]\n",
    "    elif isinstance(model, LyricsGenerator_Concatenation):\n",
    "        return model(model_input)[:, :-1, :]               # [B,T-1,V] from [B,T,V]\n",
    "    else:\n",
    "        raise ValueError(f'Model not recognized: {model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02754ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:27:30.286840Z",
     "start_time": "2025-10-14T14:27:30.273975Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,\n",
    "                train_loader: data.DataLoader,\n",
    "                val_loader: data.DataLoader,\n",
    "                test_loader: data.DataLoader,\n",
    "                num_epochs: int = MAX_EPOCHS,\n",
    "                learning_rate: float = LEARNING_RATE,\n",
    "                patiance_factor: float = PATIENCE_FACTOR,\n",
    "                patiance_epochs: int = PATIENCE_EPOCHS,\n",
    "                run_name: Optional[str] = None,\n",
    "                save_path: str = \"\"):\n",
    "    run_name = run_name or model.__class__.__name__\n",
    "    writer = SummaryWriter(log_dir=f\"runs/{run_name}\")  # <- unique folder per model\n",
    "    writer.add_text(\"run/model_name\", model.__class__.__name__, global_step=0)\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patiance_epochs)\n",
    "    best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "    train_losses: list[float] = list()\n",
    "    val_losses: list[float] = list()\n",
    "    best_validation_loss: float = 10000.0\n",
    "    epochs_with_no_improvements: int = 0\n",
    "    vocabulary_size = model.vocab_size\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        current_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device=device, dtype=torch.float32)\n",
    "            targets = targets.to(device=device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            logits = seq_logits(model, inputs)\n",
    "            loss = criterion(\n",
    "                logits.contiguous().view(-1, vocabulary_size),\n",
    "                targets[:, 1:].contiguous().view(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        validation_running_loss = 0.0\n",
    "        test_running_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs  = val_inputs.to(device=device, dtype=torch.float32)\n",
    "                val_targets = val_targets.to(device=device, dtype=torch.long)\n",
    "                val_logits = seq_logits(model, val_inputs)         # [B,T-1,V]\n",
    "                val_loss = criterion(\n",
    "                    val_logits.contiguous().view(-1, vocabulary_size),\n",
    "                    val_targets[:, 1:].contiguous().view(-1)\n",
    "                )\n",
    "                validation_running_loss += val_loss.item() * val_inputs.size(0)\n",
    "            val_epoch_loss = validation_running_loss / len(val_loader.dataset)\n",
    "            val_losses.append(val_epoch_loss)\n",
    "            writer.add_scalar(\"Loss/val\", val_epoch_loss, epoch)   # <-- add here\n",
    "        scheduler.step(val_epoch_loss)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)  # TensorBoard\n",
    "        test_epoch_loss = test_running_loss / len(test_loader.dataset)\n",
    "        writer.add_scalar(\"Loss/test\", test_epoch_loss, epoch)\n",
    "        if  best_validation_loss - val_epoch_loss >= patiance_factor:\n",
    "            epochs_with_no_improvements = 0\n",
    "            best_validation_loss = val_epoch_loss\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_with_no_improvements += 1\n",
    "            print(f'No improvement in epoch. Patiance: {epochs_with_no_improvements}\\\\{patiance_epochs}')\n",
    "        finish_time = time.time() - current_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, time: {finish_time}')\n",
    "        if epochs_with_no_improvements >= patiance_epochs:\n",
    "            print(f'Training ended prematurely due to lack of improvement.')\n",
    "            break\n",
    "    writer.close()  # Close TensorBoard writer\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, f\"{run_name}_{num_epochs}.pth\"))\n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78fa66",
   "metadata": {},
   "source": [
    "<font size=4>Running model 1: Concatenation with midi features: Harmony</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b319f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:27:30.364036Z",
     "start_time": "2025-10-14T14:27:30.324355Z"
    }
   },
   "outputs": [],
   "source": [
    "model_concatenation_harmony = LyricsGenerator_Concatenation(\n",
    "    vocab_size=len(songdata_train_dataset_harmony_midi_features.word_embeddings),\n",
    "    input_size=pretrained_word2vec.vector_size + NUMBER_OF_EXTRACT_MIDI_FEATURES + 1, # word embedding + melody features + artist index\n",
    "    hidden_layer_dim=HIDDEN_LAYER_DIM,\n",
    "    dropout_rate=DROPOUT,\n",
    "    num_layers=LSTM_LAYERS,\n",
    ")\n",
    "print(f'LyricsGenerator_Concatenation harmony:')\n",
    "summary(model_concatenation_harmony, input_size=(BATCH_SIZE, SEQUENCE_LENGTH, WORD_EMBEDDING_SIZE + NUMBER_OF_EXTRACT_MIDI_FEATURES + SIZE_OF_ARTIST_INDEX), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a6a65",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-14T14:27:30.400792Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model_harmony, harmony_concatenation_training_loss, harmony_concatenation_validation_loss = train_model(model_concatenation_harmony,\n",
    "            training_data_loader_harmony, \n",
    "            validation_data_loader_harmony, \n",
    "            test_data_loader_harmony,\n",
    "            run_name=f\"LyricsGenerator_Concatenation_Harmony_{PATIENCE_FACTOR}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda55925",
   "metadata": {},
   "source": [
    "<font size=4>Running model 1: Concatenation with midi features: Rhythm</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba083cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:24:57.589088Z",
     "start_time": "2025-10-14T14:24:57.556953Z"
    }
   },
   "outputs": [],
   "source": [
    "model_concatenation_rhythm = LyricsGenerator_Concatenation(\n",
    "    vocab_size=len(songdata_train_dataset_rhythm_midi_features.word_embeddings),\n",
    "    input_size=pretrained_word2vec.vector_size + NUMBER_OF_EXTRACT_MIDI_FEATURES + 1, # word embedding + melody features + artist index\n",
    "    hidden_layer_dim=HIDDEN_LAYER_DIM,\n",
    "    num_layers=LSTM_LAYERS,\n",
    "    dropout_rate=DROPOUT,\n",
    ")\n",
    "print(f'LyricsGenerator_Concatenation rhythm:')\n",
    "summary(model_concatenation_rhythm, input_size=(BATCH_SIZE, SEQUENCE_LENGTH, WORD_EMBEDDING_SIZE + NUMBER_OF_EXTRACT_MIDI_FEATURES + SIZE_OF_ARTIST_INDEX), dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076e422",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.587632Z",
     "start_time": "2025-10-14T14:24:57.614984Z"
    }
   },
   "outputs": [],
   "source": [
    "model_concatenation_rhythm, rhythm_concatenation_training_loss, rhythm_concatenation_validation_loss = train_model(model_concatenation_rhythm,\n",
    "            training_data_loader_rhythm, \n",
    "            validation_data_loader_rhythm, \n",
    "            test_data_loader_rhythm,\n",
    "            run_name=f\"LyricsGenerator_Concatenation_Rhythm_{PATIENCE_FACTOR}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324df1f",
   "metadata": {},
   "source": [
    "<font size=4>Running model 2: Attention with midi features: Harmony</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421fed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.589627500Z",
     "start_time": "2025-10-14T13:17:22.268709Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_model_harmony = LyricsGenerator_Attention(\n",
    "    vocab_size=len(songdata_train_dataset_harmony_midi_features.word_embeddings),\n",
    "    size_of_word_embeddings=WORD_EMBEDDING_SIZE,\n",
    "    size_of_midi_features=NUMBER_OF_EXTRACT_MIDI_FEATURES,\n",
    "    size_of_artist_index_dim=SIZE_OF_ARTIST_INDEX,\n",
    "    hidden_layer_dim=HIDDEN_LAYER_DIM_ATTENTION,\n",
    ")\n",
    "print(f'LyricsGenerator_Attention Harmony:')\n",
    "summary(attention_model_harmony, \n",
    "        input_size=(BATCH_SIZE, SEQUENCE_LENGTH, WORD_EMBEDDING_SIZE + NUMBER_OF_EXTRACT_MIDI_FEATURES + SIZE_OF_ARTIST_INDEX), \n",
    "        dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f5f7f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.592699400Z",
     "start_time": "2025-10-14T13:17:22.337665Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_model_harmony, attention_harmony_training_loss, attention_harmony_validation_loss = train_model(attention_model_harmony,\n",
    "            training_data_loader_harmony, \n",
    "            validation_data_loader_harmony, \n",
    "            test_data_loader_harmony,\n",
    "            run_name=f\"LyricsGenerator_Attention_Harmony_{PATIENCE_FACTOR}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7702c",
   "metadata": {},
   "source": [
    "<font size=4>Running model 2: Attention with midi features: Rhythm</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833e306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.595691700Z",
     "start_time": "2025-10-14T13:19:33.708088Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_model_rhythm = LyricsGenerator_Attention(\n",
    "    vocab_size=len(songdata_test_dataset_rhythm_midi_features.word_embeddings),\n",
    "    size_of_word_embeddings=WORD_EMBEDDING_SIZE,\n",
    "    size_of_midi_features=NUMBER_OF_EXTRACT_MIDI_FEATURES,\n",
    "    size_of_artist_index_dim=SIZE_OF_ARTIST_INDEX,\n",
    "    hidden_layer_dim=HIDDEN_LAYER_DIM_ATTENTION,\n",
    ")\n",
    "print(f'LyricsGenerator_Attention Rhythm:')\n",
    "summary(attention_model_rhythm, \n",
    "        input_size=(BATCH_SIZE, SEQUENCE_LENGTH, WORD_EMBEDDING_SIZE + NUMBER_OF_EXTRACT_MIDI_FEATURES + SIZE_OF_ARTIST_INDEX), \n",
    "        dtypes=[torch.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6116d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.596690800Z",
     "start_time": "2025-10-14T13:19:33.741738Z"
    }
   },
   "outputs": [],
   "source": [
    "attention_model_rhythm, attention_rhythm_training_loss, attention_rhythm_validation_loss = train_model(attention_model_rhythm,\n",
    "            training_data_loader_rhythm, \n",
    "            validation_data_loader_rhythm, \n",
    "            test_data_loader_rhythm,\n",
    "            run_name=f\"LyricsGenerator_Attention_Rhythm_{PATIENCE_FACTOR}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a70baff07c2c1",
   "metadata": {},
   "source": [
    "<font size=6> Displaying graph comparing for each model and combining results </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa9441615775d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.609654Z",
     "start_time": "2025-10-14T14:11:42.585405Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_graphs_for_loss(training_loss_per_epoch: list[float], validation_loss_per_epoch: list[float], subtitle: str =\"\"):\n",
    "    plt.title(\"Training and validation loss over epochs\")\n",
    "    plt.suptitle(subtitle)\n",
    "    plt.plot(training_loss_per_epoch[:], label=\"Training loss\")\n",
    "    plt.plot(validation_loss_per_epoch[:], label=\"Validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09ad4fac8ac8a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.613643100Z",
     "start_time": "2025-10-14T14:11:42.614982Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_graph_comparing_each_model(title: str, data_points: list[tuple[str, list[float]]]): # title, list(data_point_name, data_points)\n",
    "    figure, axes = plt.subplots()\n",
    "    for name, data_points_per_model in data_points:\n",
    "        data_point_list = range(1, len(data_points_per_model) + 1)\n",
    "        axes.plot(data_point_list, data_points_per_model, label=name)\n",
    "\n",
    "    axes.set_xlabel('Epochs')\n",
    "    axes.set_ylabel('Loss')\n",
    "    axes.set_title(title)\n",
    "    axes.legend(title=\"Models\", loc=\"upper right\")\n",
    "    axes.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55195d8b9bcaeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.613643100Z",
     "start_time": "2025-10-14T14:11:42.660190Z"
    }
   },
   "outputs": [],
   "source": [
    "display_graphs_for_loss(harmony_concatenation_training_loss, harmony_concatenation_validation_loss, subtitle=\"Concatenation + Harmony\")\n",
    "display_graphs_for_loss(rhythm_concatenation_training_loss, rhythm_concatenation_validation_loss, subtitle=\"Concatenation + Rhythm\")\n",
    "display_graphs_for_loss(attention_harmony_training_loss, attention_harmony_validation_loss, subtitle=\"Attention + Harmony\")\n",
    "display_graphs_for_loss(attention_rhythm_training_loss, attention_rhythm_validation_loss, subtitle=\"Attention + Rhythm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb944ecbc7725ca9",
   "metadata": {},
   "source": [
    "Comparing models against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc5cbc826542e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.613643100Z",
     "start_time": "2025-10-14T14:11:43.245273Z"
    }
   },
   "outputs": [],
   "source": [
    "general_training_loss = [(\"Concatenation + Harmony\", harmony_concatenation_training_loss),\n",
    " (\"Concatenation + Rhythm\", rhythm_concatenation_training_loss),\n",
    " (\"Attention + Harmony\", attention_harmony_training_loss),\n",
    " (\"Attention + Rhythm\", attention_rhythm_training_loss)]\n",
    "\n",
    "general_validation_loss = [(\"Concatenation + Harmony\", harmony_concatenation_validation_loss),\n",
    " (\"Concatenation + Rhythm\", rhythm_concatenation_validation_loss),\n",
    " (\"Attention + Harmony\", attention_harmony_validation_loss),\n",
    " (\"Attention + Rhythm\", attention_rhythm_validation_loss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8167bbad41b17ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.614640300Z",
     "start_time": "2025-10-14T14:11:43.280190Z"
    }
   },
   "outputs": [],
   "source": [
    "display_graph_comparing_each_model(\"Training_loss_for_each_model\", general_training_loss)\n",
    "display_graph_comparing_each_model(\"Validation_loss_for_each_model\", general_training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd43df",
   "metadata": {},
   "source": [
    "<font size=6>Generating Lyrics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6c6c5",
   "metadata": {},
   "source": [
    "A function that returns the k most likely words given the input, used for coherent lyric generatin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf15b89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.614640300Z",
     "start_time": "2025-10-14T14:11:43.576758Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_next_word(\n",
    "    model: nn.Module,\n",
    "    word_sequence: list[str],\n",
    "    artist_index: int,\n",
    "    melody_vec,\n",
    "    word_to_id: dict[str, int],\n",
    "    id_to_word: dict[int, str],\n",
    "    embedding_weight: torch.Tensor,\n",
    "    device: str = \"cpu\",\n",
    "    forbidden_words: list[str] | None = None,\n",
    "    strengthened_words: list[tuple[str, float]] | None = None,\n",
    "):\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # embedding matrix on device\n",
    "    if embedding_weight.device.type != device:\n",
    "        embedding_weight = embedding_weight.to(device)\n",
    "\n",
    "    # melody -> [M]\n",
    "    melody_vec = torch.as_tensor(melody_vec, dtype=torch.float32, device=device)\n",
    "\n",
    "    # seq -> ids -> embeddings\n",
    "    unk_id = word_to_id.get(UNK_STRING, UNK_ID)\n",
    "    seq_ids = torch.tensor([word_to_id.get(w, unk_id) for w in word_sequence],\n",
    "                           device=device, dtype=torch.long)              # [T]\n",
    "    seq_embs = embedding_weight[seq_ids]                                  # [T, E]\n",
    "\n",
    "    T = seq_embs.size(0)\n",
    "    # broadcast melody + artist\n",
    "    melody_broadcast = melody_vec.expand(T, -1)                           # [T, M]\n",
    "    # NOTE: your modelâ€™s artist dim must equal size_of_artist_index_dim; here we use 1\n",
    "    artist_broadcast = torch.full((T, 1), float(artist_index),\n",
    "                                  dtype=torch.float32, device=device)     # [T, 1]\n",
    "\n",
    "    # concat â†’ [1, T, E+M+artist_dim]\n",
    "    x = torch.cat([seq_embs, melody_broadcast, artist_broadcast], dim=1).unsqueeze(0)  # [1,T,D]\n",
    "\n",
    "    # forward\n",
    "    out = model(x)\n",
    "    if isinstance(model, LyricsGenerator_Attention):\n",
    "        logits = out[0]         # attention model: [1, V]\n",
    "    elif isinstance(model, LyricsGenerator_Concatenation):\n",
    "        logits = out[:, -1, :]  # [1, V]\n",
    "    else:\n",
    "        raise ValueError(f\"Model: {model} is not recognized.\")\n",
    "    \n",
    "    logits = logits.squeeze(0)  # [V]\n",
    "    # forbid\n",
    "    if forbidden_words:\n",
    "        forb_idx = [word_to_id[w] for w in forbidden_words if w in word_to_id]\n",
    "        if forb_idx:\n",
    "            logits[torch.tensor(forb_idx, device=device, dtype=torch.long)] = float(\"-inf\")\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)  # [V]\n",
    "\n",
    "    # strengthen: set explicit probabilities for selected words\n",
    "    if strengthened_words:\n",
    "        pairs = [(w, float(p)) for (w, p) in strengthened_words if (w in word_to_id) and (p >= 0.0)]\n",
    "        if pairs:\n",
    "            idxs = torch.tensor([word_to_id[w] for (w, _) in pairs], device=device, dtype=torch.long)\n",
    "            p_desired = torch.tensor([min(p, 1.0) for (_, p) in pairs], device=device, dtype=probs.dtype)\n",
    "            # aggregate dupes\n",
    "            uniq, inv = torch.unique(idxs, return_inverse=True)\n",
    "            p_agg = torch.zeros_like(uniq, dtype=probs.dtype).scatter_add(0, inv, p_desired)\n",
    "\n",
    "            base = probs.clone()\n",
    "            base[uniq] = 0.0\n",
    "            base_sum = base.sum()\n",
    "            sum_p = p_agg.sum()\n",
    "\n",
    "            if float(sum_p) >= 1.0 - 1e-8 or base_sum <= 1e-12:\n",
    "                probs = torch.zeros_like(probs)\n",
    "                probs[uniq] = p_agg / (sum_p + 1e-12)\n",
    "            else:\n",
    "                remain = 1.0 - float(sum_p)\n",
    "                base = base * (remain / (base_sum + 1e-12))\n",
    "                probs = base\n",
    "                probs[uniq] = p_agg\n",
    "\n",
    "    idx = torch.multinomial(probs, num_samples=1, replacement=True)\n",
    "    return id_to_word[int(idx.item())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fcbb1",
   "metadata": {},
   "source": [
    "Printing the generated text and handling tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e527c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.614640300Z",
     "start_time": "2025-10-14T14:11:43.622434Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_generated_lyrics(generated_lyrics: list[str]):\n",
    "    capitalize = True\n",
    "    for word in generated_lyrics:\n",
    "        if word == EOL_STRING:\n",
    "            capitalize = True\n",
    "            print()\n",
    "        if word == EOS_STRING:\n",
    "            break\n",
    "        if word != EOL_STRING:\n",
    "            if capitalize:\n",
    "                capitalize = False\n",
    "                print(word.title(), end=' ')\n",
    "            else:\n",
    "                print(word, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239b529",
   "metadata": {},
   "source": [
    "Generating the lyrics and maintaining the lyrics generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3392cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.614640300Z",
     "start_time": "2025-10-14T14:16:56.803066Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_lyrics(\n",
    "        model_to_use: nn.Module,\n",
    "        model_title: str,\n",
    "        initial_word: str,\n",
    "        melody_features: np.ndarray,\n",
    "        melody_title: str,\n",
    "        artist_to_use: str,\n",
    "        word_to_id: dict[str, int],\n",
    "        id_to_word: dict[int, str],\n",
    "        artist_to_index: dict[str, int],\n",
    "        word_embeddings: dict[str, np.ndarray],\n",
    "        max_song_length: int = MAX_SONG_LENGTH_WORDS,\n",
    "        sequence_length: int = SEQUENCE_LENGTH,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates lyrics word by word using the model, melody, and artist.\n",
    "    Picks next word randomly from top_k candidates according to their normalized probabilities.\n",
    "    Artificially increases probability of EOS_STRING after half of max_song_length.\n",
    "    Prints the generated lyrics with line breaks at <eol>.\n",
    "    Enforces some more grammatical rules.\n",
    "    Returns: generated_lyrics (list of str), artist, melody_title.\n",
    "    \"\"\"\n",
    "    print(f\"------------------------- Model: {model_title} -------------------------\")\n",
    "    print(f\"Generating song\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(f\"Initial Word: {initial_word}\")\n",
    "    print(f\"Artist: {artist_to_use}\")\n",
    "    print(f\"Melody: {melody_title}\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    initial_word = initial_word.lower()\n",
    "    melody_vec = torch.as_tensor(melody_features[0], dtype=torch.float32, device=device)\n",
    "    artist_idx = artist_to_index.get(artist_to_use.lower(), -1)\n",
    "    if artist_idx == -1:\n",
    "        print(f\"Warning: Artist '{artist_to_use}' not found, using index -1.\")\n",
    "\n",
    "    embedding_weight = torch.from_numpy(\n",
    "        np.stack([word_embeddings[w].astype(np.float32) for w in word_to_id], axis=0)\n",
    "    ).to(device)\n",
    "    context: deque = deque()\n",
    "    context.extendleft([UNK_STRING for _ in range(sequence_length - 1)])\n",
    "    context.appendleft(initial_word)\n",
    "    unk_index: int = 1\n",
    "    generated_lyrics = [initial_word]\n",
    "    words_in_song: int = 0\n",
    "    current_word: str = initial_word\n",
    "    minimum_song_length = int(max_song_length / 2)\n",
    "    current_words_in_line: int = 1\n",
    "    current_word = \"\"\n",
    "    next_word = \"\"\n",
    "    words_not_to_end_lines_on: list[str] = ['the', 'at', 'and']\n",
    "    number_of_lines_in_song: int = 0\n",
    "    while True:\n",
    "        # Tries to enforce certain rules.\n",
    "        # Don't allow end of song before minimum amount of lines.\n",
    "        # Don't repeat the same word twice\n",
    "        # Don't allow lines that are too short.\n",
    "        # Don't end lines on words in a way that would make no sense.\n",
    "        forbidden_words = [current_word]\n",
    "        strengthened_words = list()\n",
    "        if words_in_song < int(minimum_song_length):\n",
    "            forbidden_words.append(EOS_STRING)\n",
    "        if current_word in words_not_to_end_lines_on:\n",
    "            forbidden_words.extend([EOL_STRING, EOS_STRING])\n",
    "        if current_words_in_line < MIN_LINE_LENGTH:\n",
    "            forbidden_words.append(EOL_STRING)\n",
    "        if current_words_in_line > int(MIN_LINE_LENGTH):\n",
    "            probability_of_eol: float = min((current_words_in_line - int(MIN_LINE_LENGTH))/ int(MIN_LINE_LENGTH), 1.0)\n",
    "            strengthened_words.append((EOL_STRING, probability_of_eol))\n",
    "        if words_in_song > int(MAX_SONG_LENGTH_WORDS / 2):\n",
    "            probability_of_eos: float = min((words_in_song - int(MAX_SONG_LENGTH_WORDS / 2))/ int(MAX_SONG_LENGTH_WORDS/2), 1.0)\n",
    "            strengthened_words.append((EOS_STRING, probability_of_eos))\n",
    "        next_word = predict_next_word(\n",
    "            model=model_to_use,\n",
    "            word_sequence=context,\n",
    "            artist_index=artist_idx,\n",
    "            melody_vec=melody_vec,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            embedding_weight=embedding_weight,\n",
    "            forbidden_words=forbidden_words,\n",
    "            strengthened_words=strengthened_words,\n",
    "            device=device,\n",
    "        )\n",
    "        if next_word == EOS_STRING and current_word == EOL_STRING:\n",
    "            generated_lyrics[-1] = next_word # In case an end of song comes after linebreak, just end the song instead.\n",
    "        else:\n",
    "            generated_lyrics.append(next_word)\n",
    "        current_word = next_word\n",
    "        if unk_index < sequence_length:\n",
    "            context[unk_index] = next_word\n",
    "            unk_index += 1\n",
    "        else:\n",
    "            context.popleft()\n",
    "            context.append(next_word)\n",
    "        if next_word == EOS_STRING:\n",
    "            break\n",
    "        if words_in_song >= max_song_length:\n",
    "            generated_lyrics.append(EOS_STRING)\n",
    "            break\n",
    "        if next_word != EOL_STRING:\n",
    "            words_in_song += 1\n",
    "            current_words_in_line += 1\n",
    "        else:\n",
    "            number_of_lines_in_song += 1\n",
    "            current_words_in_line = 0\n",
    "    print_generated_lyrics(generated_lyrics=generated_lyrics)\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(\"\\n--- End of generated lyrics ---\")\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    print(f'Number of words in lyrics: {words_in_song}')\n",
    "    print(f'Number of lines in lyrics: {number_of_lines_in_song}')\n",
    "    print(f\"--------------------------------------------------\")\n",
    "    return generated_lyrics, artist_to_use, melody_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7590a0af713",
   "metadata": {},
   "source": [
    "<font size=6>Testing with the testing set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949fa72",
   "metadata": {},
   "source": [
    "For each melody and each model, the output of the architecture given the melody and the initial word of the real lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca4e7f0af0f837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.614640300Z",
     "start_time": "2025-10-14T14:16:56.826718Z"
    }
   },
   "outputs": [],
   "source": [
    "harmony_model_name_model_list: list[tuple[str, nn.Module]] = [\n",
    "    (\"Concatenation + Harmony\", model_concatenation_harmony),\n",
    "    (\"Attention + Harmony\", attention_model_harmony)\n",
    "]\n",
    "rhythm_model_name_model_list: list[tuple[str, nn.Module]] = [\n",
    "    (\"Concatenation + Rhythm\", model_concatenation_rhythm),\n",
    "    (\"Attention + Rhythm\", attention_model_rhythm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e489d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.614640300Z",
     "start_time": "2025-10-14T14:16:56.895813Z"
    }
   },
   "outputs": [],
   "source": [
    "for model_name, model in harmony_model_name_model_list:\n",
    "    print('======================================')\n",
    "    print('===========Harmoney Models============')\n",
    "    print('======================================')\n",
    "    for test_song in test_midi_data:\n",
    "        test_song: SongData\n",
    "        lyrics, artist, melody = generate_lyrics(\n",
    "            model_to_use=model,\n",
    "            model_title=model_name,\n",
    "            initial_word=test_song.lyrics[0],\n",
    "            melody_features=test_song.midi_harmony_and_pitch_features,\n",
    "            melody_title=test_song.title,\n",
    "            artist_to_use=test_song.artist,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            artist_to_index=artist_to_index,\n",
    "            word_embeddings=unified_embeddings,\n",
    "            max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "        )\n",
    "    print('======================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed720a3174090224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.614640300Z",
     "start_time": "2025-10-14T14:19:34.002712Z"
    }
   },
   "outputs": [],
   "source": [
    "for model_name, model in rhythm_model_name_model_list:\n",
    "    print('==================================================================================')\n",
    "    print('==================================Rhythm Models===================================')\n",
    "    print('==================================================================================')\n",
    "    for test_song in test_midi_data:\n",
    "        test_song: SongData\n",
    "        lyrics, artist, melody = generate_lyrics(\n",
    "            model_to_use=model,\n",
    "            model_title=model_name,\n",
    "            initial_word=test_song.lyrics[0],\n",
    "            melody_features=test_song.midi_rhythm_and_dynamic_features,\n",
    "            melody_title=test_song.title,\n",
    "            artist_to_use=test_song.artist,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            artist_to_index=artist_to_index,\n",
    "            word_embeddings=unified_embeddings,\n",
    "            max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92433b27",
   "metadata": {},
   "source": [
    "For each melody, the output of the architecture given the melody and different starting words. The same word should be used for all melodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0dea71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.615666100Z",
     "start_time": "2025-10-14T14:19:35.616866Z"
    }
   },
   "outputs": [],
   "source": [
    "starting_words: list[str] = ['love', 'baby', 'time']\n",
    "for model_name, model in harmony_model_name_model_list:\n",
    "    print('======================================')\n",
    "    print('===========Harmoney Models============')\n",
    "    print('======================================')\n",
    "    for word in starting_words:\n",
    "        print(f'-------------Initial Word Selected: {word}-------------------')\n",
    "        for test_song in test_midi_data:\n",
    "            print('--------------------------------')\n",
    "            test_song: SongData\n",
    "            lyrics, artist, melody = generate_lyrics(\n",
    "                model_to_use=model,\n",
    "                model_title=model_name,\n",
    "                initial_word=word,\n",
    "                melody_features=test_song.midi_harmony_and_pitch_features,\n",
    "                melody_title=test_song.title,\n",
    "                artist_to_use=test_song.artist,\n",
    "                word_to_id=word_to_id,\n",
    "                id_to_word=id_to_word,\n",
    "                artist_to_index=artist_to_index,\n",
    "                word_embeddings=unified_embeddings,\n",
    "                max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "            )\n",
    "            print('--------------------------------')\n",
    "    print('======================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26a26bb8fe2fd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T14:25:30.615666100Z",
     "start_time": "2025-10-14T14:19:40.603459Z"
    }
   },
   "outputs": [],
   "source": [
    "starting_words: list[str] = ['love', 'baby', 'time']\n",
    "for model_name, model in rhythm_model_name_model_list:\n",
    "    print('======================================')\n",
    "    print('============Rhythm Models=============')\n",
    "    print('======================================')\n",
    "    for word in starting_words:\n",
    "        print(f'-------------Initial Word Selected: {word}-------------------')\n",
    "        for test_song in test_midi_data:\n",
    "            print('--------------------------------')\n",
    "            test_song: SongData\n",
    "            lyrics, artist, melody = generate_lyrics(\n",
    "                model_to_use=model,\n",
    "                model_title=model_name,\n",
    "                initial_word=word,\n",
    "                melody_features=test_song.midi_rhythm_and_dynamic_features,\n",
    "                melody_title=test_song.title,\n",
    "                artist_to_use=test_song.artist,\n",
    "                word_to_id=word_to_id,\n",
    "                id_to_word=id_to_word,\n",
    "                artist_to_index=artist_to_index,\n",
    "                word_embeddings=unified_embeddings,\n",
    "                max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "            )\n",
    "            print('--------------------------------')\n",
    "    print('======================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
