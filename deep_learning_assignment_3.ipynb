{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd41e9a8",
   "metadata": {},
   "source": [
    "<font size=6>Downloading required libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4be7a3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: filelock in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: torch in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchsummary in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: torch in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (2.8.0+cu128)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.75.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from tensorboard) (11.0.0)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.5 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 2.6/5.5 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.9/5.5 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.75.1-cp312-cp312-win_amd64.whl (4.6 MB)\n",
      "   ---------------------------------------- 0.0/4.6 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.8/4.6 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.2/4.6 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.6/4.6 MB 10.0 MB/s eta 0:00:00\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading protobuf-6.32.1-cp310-abi3-win_amd64.whl (435 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, typing-extensions, tensorboard-data-server, protobuf, markdown, absl-py, grpcio, tensorboard\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "Successfully installed absl-py-2.3.1 grpcio-1.75.1 markdown-3.9 protobuf-6.32.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 typing-extensions-4.12.2 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q numpy\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip3 install torch torchsummary\n",
    "!pip3 install torch tensorboard\n",
    "!pip3 install -q pretty_midi\n",
    "!pip3 install -q gensim\n",
    "!pip3 install -q nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4068e6c",
   "metadata": {},
   "source": [
    "<font size=6>Imports</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a414349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.8.0+cu128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aviv\n",
      "[nltk_data]     Metz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import gdown\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "from typing import Optional\n",
    "import csv\n",
    "import string\n",
    "from pretty_midi import PrettyMIDI, Note\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import gensim.downloader\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "print(\"Using torch\", torch.__version__)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9359ed8",
   "metadata": {},
   "source": [
    "<font size=6>Constants</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60457569",
   "metadata": {},
   "outputs": [],
   "source": [
    "LYRIC_TRAIN_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_train_set.csv')\n",
    "LYRIC_TEST_SET_CSV_PATH: str = os.path.join(os.getcwd(), 'data', 'lyrics_test_set.csv')\n",
    "MIDI_FILE_PATH: str = os.path.join(os.getcwd(), 'data', 'midi_files')\n",
    "PICKLING_PATH: str = os.path.join(os.getcwd(), 'loaded_midi_files.pkl') # Path to save/load pickled MIDI files, for faster loading.\n",
    "EPSILON: float = 1e-9\n",
    "SEQUENCE_LENGTH: int = 10  # Number of words in the input sequence\n",
    "BATCH_SIZE: int = 128\n",
    "LSTM_LAYERS: int = 2\n",
    "DROPOUT: float = 0.3\n",
    "RANDOM_LOADER_SEED: int = 42\n",
    "VALIDATION_SPLIT: float = 0.1\n",
    "LEARNING_RATE: float = 0.001\n",
    "MAX_EPOCHS: int = 50\n",
    "NUMBER_OF_EXTRACT_MIDI_FEATURES: int = 33\n",
    "PATIANCE_FACTOR: float = 0.001\n",
    "PATIANCE_EPOCHS: int = 10\n",
    "UNK_ID: int = 0\n",
    "MIN_LINE_LENGTH: int = 5\n",
    "MAX_LINE_LENGTH: int = SEQUENCE_LENGTH\n",
    "SONG_STRUCTURE_PENALTY: float = 10.0\n",
    "EOL_STRING: str = 'eol'\n",
    "UNK_STRING: str = 'unk'\n",
    "EOS_STRING: str = '<eos>'\n",
    "TOP_K_WORDS_TO_PREDICT: int = 10\n",
    "MAX_SONG_LENGTH_WORDS: int = 80\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1947ce",
   "metadata": {},
   "source": [
    "<font size=6>Midi Feature extraction</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083e542",
   "metadata": {},
   "source": [
    "Auxlliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_time_signature(changes: tuple[int, int]) -> tuple[int, int]:\n",
    "    if not changes: return (4, 4)\n",
    "    pairs: list[tuple[int, int]] = [(ts.numerator, ts.denominator) for ts in changes]\n",
    "    return Counter(pairs).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2526597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_duration_weighted_pitch_stats(notes: list[Note]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute pitch statistics weighted by each note's duration.\n",
    "    Returns a dict with:\n",
    "      mean (duration-weighted),\n",
    "      std  (duration-weighted),\n",
    "      p10 / p50 / p90 (duration-weighted percentiles),\n",
    "      ambitus = p90 - p10 (robust range).\n",
    "    If `notes` is empty, returns safe defaults.\n",
    "    \"\"\"\n",
    "    # Empty guard: nothing to measure → return neutral stats.\n",
    "    if not notes:\n",
    "        return dict(mean=0.0, std=0.0, p10=-1, p50=-1, p90=-1, ambitus=0.0)\n",
    "\n",
    "    # Vectorize pitches as float for math (MIDI 0..127, but floats simplify ops).\n",
    "    pitches: np.ndarray[np.float32] = np.fromiter((n.pitch for n in notes), dtype=np.float32)\n",
    "\n",
    "    # Each note's weight = its duration in seconds; clamp tiny/negative to epsilon.\n",
    "    weights: np.ndarray[np.float32] = np.fromiter((max(EPSILON, n.end - n.start) for n in notes), dtype=np.float32)\n",
    "    total_weights: float = weights.sum()\n",
    "    duration_weight_mean: float = float((weights * pitches).sum() / total_weights)\n",
    "    duration_weighted_variance: float = float((weights * (pitches - duration_weight_mean) ** 2).sum() / total_weights)\n",
    "    weighted_std: float = duration_weighted_variance ** 0.5\n",
    "\n",
    "    # ---------- Duration-weighted percentiles ----------\n",
    "    order: np.ndarray[np.int32] = np.argsort(pitches)\n",
    "    ordered_pitches, ordered_weights = pitches[order], weights[order]\n",
    "    cumulative_weight_sum: np.ndarray[np.float32] = np.cumsum(ordered_weights)\n",
    "\n",
    "    # Weighted quantile: find the first index where cumulative weight crosses q%.\n",
    "    def weighted_quantile(quantile: float) -> float:\n",
    "        # Target cumulative weight at quantile q (0..100).\n",
    "        target: float = (quantile / 100.0) * cumulative_weight_sum[-1]\n",
    "        # Index where cumulative_weight_sum >= target; take leftmost to be consistent.\n",
    "        idx: int = np.searchsorted(cumulative_weight_sum, target, side=\"left\")\n",
    "        return float(ordered_pitches[min(idx, len(ordered_pitches) - 1)])\n",
    "\n",
    "    # 10th / 50th (median) / 90th percentiles, duration-weighted.\n",
    "    percentile_10, percentile_50, percentile_90 = weighted_quantile(10), weighted_quantile(50), weighted_quantile(90)\n",
    "\n",
    "    # Ambitus = robust spread (p90 - p10), less sensitive than raw max - min.\n",
    "    return dict(mean=duration_weight_mean, \n",
    "                std=weighted_std,\n",
    "                p10=percentile_10, \n",
    "                p50=percentile_50, \n",
    "                p90=percentile_90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66999304",
   "metadata": {},
   "source": [
    "Extracting high level features relating to the entire song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85395762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_midi_features(midi: PrettyMIDI) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return song-level features (vector+names).\n",
    "    \"\"\" \n",
    "    duration_sec: float = midi.get_end_time()                                       # total length\n",
    "    if duration_sec <= 0: raise ValueError(\"Empty/zero-length MIDI.\")        # guard\n",
    "\n",
    "    tempo_times, tempo_bpms = midi.get_tempo_changes()                               # tempo changes\n",
    "    if len(tempo_bpms) == 0:                                                     # no changes\n",
    "        tempo_times = np.array([0.0], dtype=np.float32)                          # start time\n",
    "        tempo_bpms = np.array([midi.estimate_tempo()], dtype=np.float32)         # single bpm\n",
    "    segment_ends: np.ndarray[np.float32] = np.r_[tempo_times[1:], duration_sec]                              # segment ends\n",
    "    segment_durs: np.ndarray[np.float32] = np.maximum(1e-6, segment_ends - tempo_times[:len(segment_ends)])          # segment durations\n",
    "    tempo_mean: float = float(np.dot(tempo_bpms[:len(segment_durs)], segment_durs) / np.sum(segment_durs)) # duration-weighted mean\n",
    "    tempo_std: float = float(np.std(np.repeat(\n",
    "        tempo_bpms[:len(segment_durs)],                                              # repeat bpm by\n",
    "        np.maximum(1, (segment_durs/np.sum(segment_durs)*1000).astype(int))          # rough weights\n",
    "    )))                                                                       # dispersion proxy\n",
    "    tempo_change_count: int = int(len(tempo_bpms))                                     # number of states\n",
    "\n",
    "    time_signature_numerator, time_signature_denominator = most_common_time_signature(midi.time_signature_changes)  # mode time sig\n",
    "    instrument_count: int = sum(1 for inst in midi.instruments                     # non-drum count\n",
    "                          if not inst.is_drum and inst.notes)\n",
    "\n",
    "    instruments: list = [inst for inst in midi.instruments if not inst.is_drum and inst.notes]\n",
    "    instruments_velocities: list[float] = []\n",
    "    instrument_notes: list[Note] = []\n",
    "    for instrument in instruments:                                 # melody track\n",
    "        mel_velocity = [note.velocity for note in instrument.notes]     # melody velocities\n",
    "        instruments_velocities.extend(mel_velocity)\n",
    "        instrument_notes.extend(instrument.notes)\n",
    "\n",
    "    instrument_velocities_min: float = min(instruments_velocities)      # min pitch\n",
    "    instrument_velocities_max: float = max(instruments_velocities)      # max pitch\n",
    "    instrument_velocities_mean: float = np.mean(instruments_velocities)    # mean pitch\n",
    "    instrument_velocities_std: float = np.std(instruments_velocities)     # std pitch\n",
    "\n",
    "    duration_weight_pitch_stats_dict: dict = get_duration_weighted_pitch_stats(instrument_notes)\n",
    "    instrument_pitch_10_percentile: float = duration_weight_pitch_stats_dict['p10']\n",
    "    instrument_pitch_50_percentile: float = duration_weight_pitch_stats_dict['p50']\n",
    "    instrument_pitch_90_percentile: float = duration_weight_pitch_stats_dict['p90']\n",
    "    instrument_pitch_mean: float = duration_weight_pitch_stats_dict['mean']\n",
    "    instrument_pitch_std: float = duration_weight_pitch_stats_dict['std']\n",
    "    instrument_pitch_range_by_percentiles: float = instrument_pitch_90_percentile - instrument_pitch_10_percentile\n",
    "\n",
    "    note_durations: list[float] = [note.end - note.start for note in instrument_notes]\n",
    "    note_durations_mean: float = np.mean(note_durations) if note_durations else 0.0\n",
    "    note_durations_std: float = np.std(note_durations) if note_durations else 0.0\n",
    "    note_durations_range: float = max(note_durations) - min(note_durations) if note_durations else 0.0\n",
    "\n",
    "    note_density: float = float(len(instrument.notes) / max(EPSILON, duration_sec))           # notes/sec\n",
    "\n",
    "    chroma_global = midi.get_pitch_class_histogram(use_duration=True)        # 12-bin chroma\n",
    "    chroma_global = chroma_global / (np.sum(chroma_global) + EPSILON)           # normalize\n",
    "    names = [                                                                # feature names\n",
    "        \"duration_sec\",\n",
    "        \"tempo_mean_bpm\", \n",
    "        \"tempo_std_bpm\", \n",
    "        \"tempo_change_count\",\n",
    "        \"time_sig_num\", \n",
    "        \"time_sig_den\",\n",
    "        \"instrument_count\",\n",
    "        \"instrument_velocities_min\", \n",
    "        \"instrument_velocities_max\", \n",
    "        \"instrument_velocities_mean\", \n",
    "        \"instrument_velocities_std\", \n",
    "        \"instrument_pitch_10_percentile\",\n",
    "        \"instrument_pitch_50_percentile\",\n",
    "        \"instrument_pitch_90_percentile\",\n",
    "        \"instrument_pitch_mean\",\n",
    "        \"instrument_pitch_std\",\n",
    "        \"instrument_pitch_range_by_percentiles\",\n",
    "        \"note_durations_mean\",\n",
    "        \"note_durations_std\",\n",
    "        \"note_durations_range\",\n",
    "        \"melody_note_density_per_sec\",\n",
    "    ] + [f\"chroma_{i}\" for i in range(12)]                                   # chroma names\n",
    "    vec = np.array([                                                         # feature vector\n",
    "        duration_sec, \n",
    "        tempo_mean, \n",
    "        tempo_std, \n",
    "        tempo_change_count,\n",
    "        time_signature_numerator, \n",
    "        time_signature_denominator,   \n",
    "        instrument_count,\n",
    "        instrument_velocities_min, \n",
    "        instrument_velocities_max, \n",
    "        instrument_velocities_mean, \n",
    "        instrument_velocities_std, \n",
    "        instrument_pitch_10_percentile,\n",
    "        instrument_pitch_50_percentile,\n",
    "        instrument_pitch_90_percentile,\n",
    "        instrument_pitch_mean,\n",
    "        instrument_pitch_std,\n",
    "        instrument_pitch_range_by_percentiles,\n",
    "        note_durations_mean,\n",
    "        note_durations_std,\n",
    "        note_durations_range,   \n",
    "        note_density,\n",
    "        *chroma_global.tolist()\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return {\"vector\": vec, \"names\": names} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c68e2f",
   "metadata": {},
   "source": [
    "<font size=6>Auxlilliary Data Structures</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f7e0b",
   "metadata": {},
   "source": [
    "Auxilliary functions for creation of word sequences and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ba93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_sequences_with_targets(tokenized_lyrics: list[str], sequence_length: int = SEQUENCE_LENGTH) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given tokenized lyrics as a list of strings, create sequences of word indices and their corresponding target word indices.\n",
    "    Each sequence is of length `sequence_length`, and the target is the next word following the sequence.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Create sequences and targets\n",
    "    for i in range(len(tokenized_lyrics) - sequence_length):\n",
    "        seq = tokenized_lyrics[i:i + sequence_length]\n",
    "        target = tokenized_lyrics[i + sequence_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return sequences,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongData:\n",
    "    def __init__(self, song_data_cell: list[str] = None, midi_file: PrettyMIDI = None):\n",
    "        if len(song_data_cell) != 3:\n",
    "            raise ValueError(\"song_data_cell must have exactly three elements: [artist, title, lyrics]\")\n",
    "        self.artist = song_data_cell[0]\n",
    "        self.title = song_data_cell[1]\n",
    "        self.lyrics = song_data_cell[2]\n",
    "        self.midi_data = midi_file\n",
    "        self._midi_features: Optional[dict[str, np.ndarray]] = None\n",
    "\n",
    "    @property\n",
    "    def midi_features(self):\n",
    "        if self._midi_features is None:\n",
    "            self._midi_features = extract_midi_features(self.midi_data)\n",
    "        return self._midi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "7c8efdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataset(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 songs_data: list[SongData],\n",
    "                 word_embeddings: dict[str, np.ndarray],\n",
    "                 artist_to_index: dict[str, int]):\n",
    "        self.midi_features: list[np.ndarray] = list()\n",
    "        self.artists: list[str] = list()\n",
    "        self.sequence_artists: list[str] = list()\n",
    "        self.word_sequences: list[str] = list()\n",
    "        self.sequences_targets: list[str] = list()\n",
    "        self.sequence_to_midi: list[int] = list() # Maps each sequence to its corresponding MIDI feature index \n",
    "        self.sequence_to_artist: list[int] = list() # Maps each sequence to its corresponding artist embedding index\n",
    "        self.word_embeddings: dict[str, np.ndarray] = word_embeddings\n",
    "        self.artist_to_index: dict[str, np.ndarray] = artist_to_index\n",
    "        # Instead of saving each sequence's MIDI features, we save the index of the MIDI features in the midi_features list to save space.\n",
    "        for idx, song in enumerate(songs_data):\n",
    "            sequences, targets = create_word_sequences_with_targets(song.lyrics)\n",
    "            self.word_sequences.extend(sequences)\n",
    "            self.sequences_targets.extend(targets)\n",
    "            self.midi_features.append(song.midi_features['vector']) # Creates a mapping of the features to the sequences.\n",
    "            self.sequence_artists.append(song.artist)\n",
    "            self.sequence_to_midi.extend([idx] * len(sequences))\n",
    "            self.sequence_to_artist.extend([idx] * len(sequences))\n",
    "        print(f'Dataset has: {len(self.word_sequences)} sequences and {len(self.sequences_targets)} targets')\n",
    "\n",
    "    \n",
    "    def word_vec(self, tok: str) -> np.ndarray:\n",
    "        # helper to get word vector, or zeros if OOV\n",
    "        v = self.word_embeddings.get(tok)\n",
    "        if v is None:\n",
    "            # OOV → zeros with same dim as any known word vector\n",
    "            sample = next(iter(self.word_embeddings.values()))\n",
    "            v = np.zeros_like(sample, dtype=np.float32)\n",
    "        return v.astype(np.float32, copy=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_sequences)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        word_sequence = self.word_sequences[idx]          \n",
    "        target_word = self.sequences_targets[idx]\n",
    "        midi_feature: np.ndarray = self.midi_features[self.sequence_to_midi[idx]]\n",
    "        artist_name: str = self.sequence_artists[self.sequence_to_artist[idx]]\n",
    "        artist_index: int = self.artist_to_index[artist_name]\n",
    "        stacked_word_sequence = np.stack([self.word_vec(tok) for tok in word_sequence], axis=0)\n",
    "        stacked_word_sequence_with_midi_features = np.broadcast_to(midi_feature, (stacked_word_sequence.shape[0], midi_feature.shape[0]))\n",
    "        stacked_word_sequence_with_artist = np.broadcast_to(artist_index, (stacked_word_sequence.shape[0], 1)) # simple index.\n",
    "        concatenated_features = np.concatenate((stacked_word_sequence, stacked_word_sequence_with_midi_features, stacked_word_sequence_with_artist), axis=1)\n",
    "        line_length = 0\n",
    "        for w in word_sequence:\n",
    "            if w == f\"{EOL_STRING}\":\n",
    "                line_length = 0\n",
    "            else:\n",
    "                line_length += 1\n",
    "        # If no <eol> yet, line_length is length of sequence\n",
    "        return concatenated_features, target_word, line_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec874e9",
   "metadata": {},
   "source": [
    "<font size=6>Reading CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1959fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LYRIC_TRAIN_SET_CSV_PATH, mode='r', encoding='utf-8') as train_file:\n",
    "    reader = csv.reader(train_file)\n",
    "    lyric_train_data = list(reader)\n",
    "\n",
    "with open(LYRIC_TEST_SET_CSV_PATH, mode='r', encoding='utf-8') as test_file:\n",
    "    reader = csv.reader(test_file)\n",
    "    lyric_test_data = list(reader)\n",
    "\n",
    "if len(lyric_train_data) < 1 or len(lyric_test_data) < 1:\n",
    "    raise Exception(\"CSV files are empty or not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996afc4a",
   "metadata": {},
   "source": [
    "<font size=6>Parsing CSV files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f5d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_csv_data(raw_csv_data: list[list[str]]) -> list[tuple[str, str, list[str]]]:\n",
    "    returned_cleaned_csv_data: list[tuple[str, str, list[str]]] = []\n",
    "    for row in raw_csv_data:\n",
    "        artist = row[0].strip()\n",
    "        title_index = 1\n",
    "        lyrics_index = 2\n",
    "        while lyrics_index < len(row):\n",
    "            title = row[title_index].strip()\n",
    "            title = title.removesuffix('-2') # Remove '-2' suffix if present, relevant in 1 case.\n",
    "            title = row[title_index].strip()\n",
    "            lyrics = row[lyrics_index].strip()\n",
    "            lyrics = lyrics.lower()\n",
    "            lyrics = re.sub(f\"[{re.escape('&')}]\", f\" {EOL_STRING} \", lyrics) # Changing ampersands to eol to indicate end of line.\n",
    "            lyrics = re.sub(f\"[{re.escape('\\'')}]\", \"\", lyrics) # Removing apostrophes.\n",
    "            lyrics = re.sub(f\"[{re.escape('-')}]\", \" \", lyrics) # Removing hyphens.\n",
    "            lyrics = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", lyrics) # Removing punctuation.\n",
    "            lyrics = lyrics.split(' ') # Tokenzing each word by space.\n",
    "            lyrics = [word.strip() for word in lyrics if word] # Removing empty strings.\n",
    "            lyrics.append(EOS_STRING) # Adding end of song token.\n",
    "            if len(title) > 0 and len(lyrics) > 0:\n",
    "                returned_cleaned_csv_data.append((artist, title, lyrics))\n",
    "            title_index += 2\n",
    "            lyrics_index += 2\n",
    "    return returned_cleaned_csv_data\n",
    "\n",
    "cleaned_lyric_train_data = clean_csv_data(lyric_train_data)\n",
    "cleaned_lyric_test_data = clean_csv_data(lyric_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca9578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in training set: 7436\n",
      "Number of unique words in test set: 291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('eol', 24435),\n",
       " ('you', 6136),\n",
       " ('the', 5970),\n",
       " ('i', 5436),\n",
       " ('to', 3892),\n",
       " ('and', 3754),\n",
       " ('a', 3153),\n",
       " ('me', 2898),\n",
       " ('my', 2215),\n",
       " ('in', 2086),\n",
       " ('it', 2085),\n",
       " ('of', 1587),\n",
       " ('your', 1533),\n",
       " ('that', 1497),\n",
       " ('be', 1410),\n",
       " ('on', 1352),\n",
       " ('is', 1234),\n",
       " ('love', 1219),\n",
       " ('all', 1128),\n",
       " ('oh', 1119),\n",
       " ('im', 1095),\n",
       " ('for', 1075),\n",
       " ('so', 983),\n",
       " ('dont', 980),\n",
       " ('its', 978),\n",
       " ('we', 919),\n",
       " ('but', 918),\n",
       " ('know', 904),\n",
       " ('do', 904),\n",
       " ('just', 900),\n",
       " ('can', 876),\n",
       " ('with', 860),\n",
       " ('when', 840),\n",
       " ('no', 818),\n",
       " ('like', 804),\n",
       " ('what', 759),\n",
       " ('if', 727),\n",
       " ('up', 705),\n",
       " ('baby', 704),\n",
       " ('now', 679),\n",
       " ('want', 676),\n",
       " ('youre', 658),\n",
       " ('this', 642),\n",
       " ('<eos>', 615),\n",
       " ('out', 585),\n",
       " ('got', 583),\n",
       " ('time', 575),\n",
       " ('never', 564),\n",
       " ('they', 560),\n",
       " ('say', 553),\n",
       " ('down', 543),\n",
       " ('have', 541),\n",
       " ('come', 538),\n",
       " ('see', 537),\n",
       " ('yeah', 517),\n",
       " ('was', 515),\n",
       " ('are', 513),\n",
       " ('get', 508),\n",
       " ('will', 497),\n",
       " ('one', 492),\n",
       " ('go', 484),\n",
       " ('let', 460),\n",
       " ('way', 445),\n",
       " ('cant', 440),\n",
       " ('cause', 435),\n",
       " ('gonna', 434),\n",
       " ('ill', 434),\n",
       " ('make', 425),\n",
       " ('at', 410),\n",
       " ('there', 409),\n",
       " ('right', 406),\n",
       " ('not', 388),\n",
       " ('feel', 378),\n",
       " ('he', 376),\n",
       " ('she', 374),\n",
       " ('take', 371),\n",
       " ('here', 369),\n",
       " ('were', 364),\n",
       " ('how', 361),\n",
       " ('heart', 360),\n",
       " ('from', 359),\n",
       " ('tell', 354),\n",
       " ('need', 346),\n",
       " ('could', 346),\n",
       " ('as', 345),\n",
       " ('away', 345),\n",
       " ('life', 341),\n",
       " ('her', 333),\n",
       " ('night', 324),\n",
       " ('more', 323),\n",
       " ('think', 312),\n",
       " ('ive', 310),\n",
       " ('again', 303),\n",
       " ('day', 302),\n",
       " ('well', 293),\n",
       " ('by', 293),\n",
       " ('good', 290),\n",
       " ('girl', 287),\n",
       " ('only', 286),\n",
       " ('thats', 286)]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of unique words in the lyrics\n",
    "def get_word_frequencies(lyrics_data: list[tuple[str, str, list[str]]]) -> dict[str, int]:\n",
    "    words_frequency = defaultdict(int)\n",
    "    for _, _, lyrics in lyrics_data:\n",
    "        for word in lyrics:\n",
    "            words_frequency[word] += 1\n",
    "    return words_frequency    \n",
    "word_frequencies_training: dict[str, int] = get_word_frequencies(cleaned_lyric_train_data)\n",
    "word_frequencies_test: dict[str, int] = get_word_frequencies(cleaned_lyric_test_data)\n",
    "print(f\"Number of unique words in training set: {len(word_frequencies_training)}\")\n",
    "print(f\"Number of unique words in test set: {len(word_frequencies_test)}\")\n",
    "\n",
    "d_sorted_by_val = sorted(word_frequencies_training.items(), key=lambda kv: kv[1], reverse=True)\n",
    "d_sorted_by_val[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141709c5",
   "metadata": {},
   "source": [
    "<font size=6>Reading MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80433ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_files(midi_files_location: str, pickling_path: Optional[str] = None, failed_loads_path: Optional[str] = None) -> \\\n",
    "                    tuple[dict[str, dict[str, PrettyMIDI]], dict[str, set[str]]]: # artist -> title -> PrettyMIDI, failed loads[artist, song_set]\n",
    "    failed_loads = dict()\n",
    "    if failed_loads_path is not None and os.path.isfile(failed_loads_path):\n",
    "        with open(failed_loads_path, \"rb\") as f:\n",
    "            failed_loads = pickle.load(f)\n",
    "        print(f\"Loaded failed MIDI loads from pickled file {failed_loads_path}.\")\n",
    "    if pickling_path is not None and os.path.isfile(pickling_path):\n",
    "        with open(pickling_path, \"rb\") as f:\n",
    "            loaded_midi_files = pickle.load(f)\n",
    "        print(f\"Loaded MIDI files from pickled file {pickling_path}.\")\n",
    "        print(f'Loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.')\n",
    "        return loaded_midi_files, failed_loads\n",
    "    if not os.path.isdir(midi_files_location):\n",
    "        raise ValueError(f\"MIDI file path {midi_files_location} is not a valid directory.\")\n",
    "\n",
    "    # Traversing over all files and attempt to load them with pretty_midi:\n",
    "    loaded_midi_files: dict[str, dict[str, PrettyMIDI]] = defaultdict(dict) # artist -> title -> PrettyMIDI\n",
    "    failed_loads: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "    for file in os.listdir(midi_files_location):\n",
    "        if file.endswith('.mid') or file.endswith('.midi'):\n",
    "            file_path = os.path.join(midi_files_location, file)\n",
    "            file = file.removesuffix('.mid')\n",
    "            splitted_artist_and_title = file.split('_-_')\n",
    "            artist = splitted_artist_and_title[0]\n",
    "            title = splitted_artist_and_title[1]\n",
    "            if len(splitted_artist_and_title) > 2:\n",
    "                print(f\"Warning: file {file} has more than one '_-_' separator, ignoring the rest after second \\\"_-_\\\".\")\n",
    "            artist = artist.replace('_', ' ').strip().lower()\n",
    "            title = title.replace('_', ' ').strip().lower()\n",
    "            try:\n",
    "                midi_data = PrettyMIDI(file_path)\n",
    "                loaded_midi_files[artist][title] = midi_data\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file}: {e}\")\n",
    "                failed_loads[artist].add(title)\n",
    "\n",
    "\n",
    "\n",
    "    if failed_loads:\n",
    "        print(\"Failed to load the following artist and lyric midi files:\")\n",
    "        for artist, lyrics in failed_loads.items():\n",
    "            print(f\"{artist} - [{', '.join(lyrics)}]\")\n",
    "\n",
    "    if pickling_path is not None:\n",
    "        with open(pickling_path, \"wb\") as f:\n",
    "            pickle.dump(loaded_midi_files, f)\n",
    "            print(f\"Pickled loaded MIDI files to {pickling_path}.\")\n",
    "    if failed_loads_path is not None:\n",
    "        with open(failed_loads_path, \"wb\") as f:\n",
    "            pickle.dump(failed_loads, f)\n",
    "            print(f\"Pickled failed MIDI loads to {failed_loads_path}.\")\n",
    "\n",
    "    print(f\"Successfully loaded {sum([len(songs) for songs in loaded_midi_files.values()])} MIDI files.\")\n",
    "    return loaded_midi_files, failed_loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a752d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_midi_files, failed_midi_loads = load_midi_files(MIDI_FILE_PATH, PICKLING_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ffe14",
   "metadata": {},
   "source": [
    "<font size=6>Mapping CSV data to MIDI files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e5010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_data_to_songdata_list(csv_data: list[list[str]], \n",
    "                              failed_midi_load: dict[str, set[str]], \n",
    "                              midi_files_dict: dict[str, dict[str, PrettyMIDI]]) -> list[SongData]:\n",
    "    song_data_list: list[SongData] = list()\n",
    "    missing_midi_count = 0\n",
    "    for row in csv_data:\n",
    "        artist = row[0]\n",
    "        title = row[1]\n",
    "        if artist in failed_midi_load and title in failed_midi_load[artist]:\n",
    "            print(f\"Skipping {artist} - {title} due to previous MIDI load failure.\")\n",
    "            continue\n",
    "        if artist in midi_files_dict and title in midi_files_dict[artist]:\n",
    "            midi_file = midi_files_dict[artist][title]\n",
    "            song_data = SongData(row, midi_file)\n",
    "            song_data_list.append(song_data)\n",
    "        else:\n",
    "            missing_midi_count += 1\n",
    "            print(f\"Missing MIDI file for artist '{artist}' and title '{title}'\")\n",
    "    print(f\"Total songs with missing MIDI files: {missing_midi_count}\")\n",
    "    return song_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f061cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_train_data, failed_midi_loads, loaded_midi_files)\n",
    "test_midi_data: list[SongData] = csv_data_to_songdata_list(cleaned_lyric_test_data, failed_midi_loads, loaded_midi_files)\n",
    "print(f\"Total training songs with MIDI data: {len(train_midi_data)}\")\n",
    "print(f\"Total test songs with MIDI data: {len(test_midi_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e0f1d",
   "metadata": {},
   "source": [
    "<font size=6>Handling word embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95f984",
   "metadata": {},
   "source": [
    "Downloading pretrained word2vec, containing 300 dims, trained on news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0482d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0bebb8",
   "metadata": {},
   "source": [
    "Extracting the vocabulary from the lyrics.\n",
    "Getting the data from the test set aswell since the vocbulary needs to be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_vocabulary: set[str] = set()\n",
    "# Getting the data from the test set aswell since the vocbulary needs to be known\n",
    "for song in train_midi_data + test_midi_data:\n",
    "    for word in song.lyrics:\n",
    "        lyrics_vocabulary.add(word)\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305abc9",
   "metadata": {},
   "source": [
    "Creating unified embedding.\n",
    "Extracting embeddings from word2vec and using random embeddings for words not found in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_embeddings: dict[str, np.ndarray] = dict()\n",
    "existing_words_in_pretrained = 0\n",
    "not_existing_in_pretrained = 0\n",
    "added_stopwords = 0\n",
    "for word in list(lyrics_vocabulary):\n",
    "    if word in pretrained_word2vec:\n",
    "        unified_embeddings[word] = pretrained_word2vec[word]\n",
    "        existing_words_in_pretrained += 1\n",
    "    else:\n",
    "        unified_embeddings[word] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,)) # Random init for unknown words.  \n",
    "        not_existing_in_pretrained += 1\n",
    "    # Adding stopwords as well, since they are common and should be in the vocabulary.\n",
    "for stopword in stopwords.words('english'):\n",
    "    cleaned_stopword = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", stopword.strip().lower()) # Cleaning the stopword, since it contains punctuation.\n",
    "    if cleaned_stopword not in unified_embeddings:\n",
    "        unified_embeddings[cleaned_stopword] = np.random.uniform(low=-1.0, high=1.0, size=(pretrained_word2vec.vector_size,))\n",
    "        added_stopwords += 1\n",
    "\n",
    "print(f\"Total unique words in lyrics vocabulary: {len(lyrics_vocabulary)}\")\n",
    "print(f\"Existing words in pretrained embeddings: {existing_words_in_pretrained}\")\n",
    "print(f\"Not existing in pretrained embeddings (randomly initialized): {not_existing_in_pretrained}\")\n",
    "print(f\"Added stopwords (randomly initialized): {added_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b23e9d",
   "metadata": {},
   "source": [
    "<font size=6>Handling artist embeddings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f40eba",
   "metadata": {},
   "source": [
    "Using simple indexing for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_artists = [song.artist for song in train_midi_data]\n",
    "test_artists = [song.artist for song in test_midi_data]\n",
    "artist_set: set = (set(train_artists).union(set(test_artists)))\n",
    "artist_to_index: dict[str, int] = dict()\n",
    "index_to_artist: dict[int, str] = dict()\n",
    "for index, artist in enumerate(artist_set):\n",
    "    artist_to_index[artist] = index\n",
    "    index_to_artist[index] = artist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df1ccb",
   "metadata": {},
   "source": [
    "<font size=6>Load dataset and dataloader</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "163de739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jean', 'eol', 'though', 'i', 'never', 'knew', 'you', 'at', 'all', 'eol']\n",
      "['way', 'down', 'on', 'baker', 'street', 'eol', 'lite', 'in', 'your', 'head']\n",
      "['i', 'need', 'your', 'love', 'eol', 'youve', 'got', 'that', 'hold', 'over']\n",
      "['check', 'it', 'out', 'yall', 'eol', 'come', 'on', 'come', 'on', 'eol']\n",
      "['beat', 'control', 'your', 'body', 'eol', 'let', 'the', 'beat', 'control', 'your']\n",
      "['for', 'this', 'eol', 'get', 'down', 'with', 'the', 'style', 'eol', 'house']\n",
      "['the', 'bass', 'into', 'the', 'jam', 'eol', 'then', 'let', 'the', 'music']\n",
      "['ive', 'lost', 'everything', 'to', 'you', 'eol', 'you', 'say', 'you', 'want']\n",
      "['oh', 'im', 'bein', 'followed', 'by', 'a', 'moonshadow', 'moon', 'shadow', 'moonshadow']\n",
      "['broken', 'like', 'the', 'first', 'morning', 'eol', 'blackbird', 'has', 'spoken', 'like']\n",
      "['another', 'saturday', 'night', 'and', 'i', 'aint', 'got', 'nobody', 'eol', 'ive']\n",
      "['been', 'happy', 'lately', 'eol', 'thinking', 'about', 'the', 'good', 'things', 'to']\n",
      "['time', 'to', 'make', 'a', 'change', 'eol', 'just', 'relax', 'take', 'it']\n",
      "['young', 'what', 'will', 'you', 'leave', 'us', 'this', 'time', 'eol', 'youre']\n",
      "['me', 'eol', 'i', 'was', 'wondering', 'if', 'after', 'all', 'these', 'years']\n",
      "['walking', 'in', 'the', 'same', 'way', 'as', 'i', 'did', 'eol', 'missing']\n",
      "['that', 'youre', 'settled', 'down', 'eol', 'that', 'you', 'found', 'a', 'girl']\n",
      "['yourself', 'in', 'stupid', 'places', 'eol', 'yes', 'i', 'think', 'you', 'know']\n",
      "['the', 'money', 'that', 'i', 'owe', 'you', 'eol', 'yes', 'you', 'can']\n",
      "['still', 'living', 'with', 'your', 'ghost', 'eol', 'lonely', 'and', 'dreaming', 'of']\n",
      "['mine', 'eol', 'tell', 'me', 'where', 'have', 'you', 'been', 'eol', 'you']\n",
      "['him', 'on', 'a', 'monday', 'and', 'my', 'heart', 'stood', 'still', 'eol']\n",
      "['successful', 'fella', 'thought', 'to', 'himself', 'eol', 'oops', 'ive', 'got', 'a']\n",
      "['the', 'night', 'eol', 'lying', 'by', 'your', 'side', 'eol', 'tender', 'is']\n",
      "['eol', 'woo', 'hoo', 'eol', 'woo', 'hoo', 'eol', 'woo', 'hoo', 'eol']\n",
      "['him', 'in', 'a', 'crowded', 'room', 'eol', 'where', 'people', 'go', 'to']\n",
      "['the', 'neon', 'lights', 'are', 'bright', 'on', 'broadway', 'eol', 'they', 'say']\n",
      "['love', 'around', 'eol', 'george', 'benson', 'eol', 'youve', 'got', 'thelove', 'eol']\n",
      "['really', 'happy', 'here', 'eol', 'with', 'this', 'lonely', 'game', 'we', 'play']\n",
      "['has', 'fallen', 'eol', 'you', 'know', 'the', 'spirit', 'of', 'the', 'party']\n",
      "['gonna', 'be', 'around', 'eol', 'hey', 'were', 'gonna', 'work', 'it', 'out']\n",
      "['were', 'to', 'say', 'to', 'you', 'eol', 'can', 'you', 'keep', 'a']\n",
      "['world', 'is', 'full', 'of', 'strange', 'arrangements', 'eol', 'and', 'gravity', 'wont']\n",
      "['down', 'i', 'call', 'on', 'you', 'my', 'friend', 'eol', 'a', 'helping']\n",
      "['like', 'to', 'get', 'to', 'know', 'if', 'i', 'could', 'be', 'eol']\n",
      "['is', 'mine', 'the', 'boy', 'is', 'mine', 'eol', 'the', 'boy', 'is']\n",
      "['cake', 'eol', 'give', 'me', 'a', 'little', 'piece', 'let', 'me', 'lick']\n",
      "['the', 'pieces', 'uh', 'huh', 'eol', 'pick', 'up', 'the', 'pieces', 'alright']\n",
      "['with', 'tact', 'just', 'what', 'are', 'you', 'trying', 'to', 'say', 'eol']\n",
      "['pulled', 'out', 'of', 'pittsburgh', 'headig', 'down', 'that', 'eastern', 'seaboard', 'eol']\n",
      "['gettin', 'down', 'baby', 'eol', 'i', 'want', 'it', 'now', 'baby', 'eol']\n",
      "['boys', 'with', 'the', 'power', 'to', 'rock', 'you', 'eol', 'blowing', 'your']\n",
      "['time', 'is', 'through', 'eol', 'now', 'and', 'forever', 'eol', 'until', 'the']\n",
      "['get', 'up', 'in', 'the', 'mornin', 'eol', 'i', 'believe', 'ill', 'dust']\n",
      "['old', 'paintings', 'on', 'the', 'tombs', 'eol', 'they', 'do', 'the', 'sand']\n",
      "['time', 'eol', 'see', 'whats', 'become', 'of', 'me', 'eol', 'time', 'time']\n",
      "['already', 'eol', 'i', 'was', 'just', 'in', 'the', 'middle', 'of', 'a']\n",
      "['walk', 'on', 'the', 'water', 'with', 'i', 'you', 'and', 'i', 'eol']\n",
      "['the', 'witch', 'doctor', 'eol', 'i', 'was', 'in', 'love', 'with', 'you']\n",
      "['a', 'luck', 'out', 'a', 'love', 'eol', 'gotta', 'photograph', 'picture', 'of']\n",
      "['tell', 'the', 'world', 'you', 'never', 'was', 'my', 'girl', 'eol', 'you']\n",
      "['know', 'where', 'youre', 'going', 'to', 'eol', 'do', 'you', 'like', 'the']\n",
      "['upside', 'down', 'eol', 'youre', 'turning', 'me', 'eol', 'youre', 'giving', 'love']\n",
      "['in', 'the', 'morning', 'eol', 'then', 'just', 'walk', 'away', 'eol', 'we']\n",
      "['me', 'eol', 'you', 'may', 'think', 'you', 'see', 'eol', 'who', 'i']\n",
      "['like', 'ive', 'been', 'locked', 'up', 'tight', 'eol', 'for', 'a', 'century']\n",
      "['friends', 'are', 'gonna', 'party', 'all', 'night', 'long', 'eol', 'cmon', 'over']\n",
      "['lost', 'in', 'the', 'rain', 'in', 'your', 'eyes', 'eol', 'i', 'know']\n",
      "['bad', 'tasting', 'eol', 'full', 'bodied', 'butt', 'wasting', 'eol', 'loose', 'living']\n",
      "['el', 'hombre', 'eol', 'con', 'fuego', 'en', 'la', 'sangre', 'eol', 'ive']\n",
      "['child', 'and', 'dont', 'you', 'cry', 'eol', 'your', 'folks', 'might', 'understand']\n",
      "['line', 'marking', 'time', 'eol', 'waiting', 'for', 'the', 'welfare', 'dime', 'eol']\n",
      "['time', 'to', 'realize', 'my', 'crime', 'eol', 'let', 'me', 'love', 'and']\n",
      "['in', 'your', 'eyes', 'all', 'the', 'way', 'eol', 'if', 'i', 'listen']\n",
      "['in', 'your', 'eyes', 'eol', 'you', 'used', 'and', 'made', 'my', 'life']\n",
      "['talking', 'eol', 'a', 'lot', 'of', 'a', 'sharks', 'out', 'there', 'tryna']\n",
      "['can', 'show', 'you', 'the', 'world', 'eol', 'shining', 'shimmering', 'splendid', 'eol']\n",
      "['time', 'i', 'get', 'to', 'phoenix', 'shell', 'be', 'rising', 'eol', 'shell']\n",
      "['a', 'lineman', 'for', 'the', 'county', 'eol', 'and', 'i', 'drive', 'the']\n",
      "['eol', 'my', 'tears', 'have', 'stopped', 'falling', 'eol', 'the', 'long', 'lonely']\n",
      "['a', 'long', 'time', 'long', 'time', 'we', 'shouldnt', 'of', 'left', 'you']\n",
      "['eol', 'can', 'yall', 'really', 'feel', 'me', 'feel', 'this', 'eol', 'east']\n",
      "['n', 'harmony', 'eol', 'bone', 'xalot', 'eol', 'wasteland', 'soldier', 'these', 'are']\n",
      "['ruggish', 'niggas', 'always', 'always', 'eol', 'and', 'ready', 'to', 'bring', 'the']\n",
      "['gotta', 'let', 'me', 'know', 'eol', 'should', 'i', 'stay', 'or', 'should']\n",
      "['king', 'told', 'the', 'boogie', 'men', 'eol', 'you', 'have', 'to', 'let']\n",
      "['seen', 'places', 'and', 'faces', 'eol', 'and', 'things', 'you', 'aint', 'never']\n",
      "['up', 'cause', 'im', 'about', 'to', 'get', 'my', 'speak', 'on', 'eol']\n",
      "['yall', 'lets', 'take', 'a', 'ride', 'eol', 'dont', 'you', 'say', 'shit']\n",
      "['those', 'times', 'you', 'stood', 'by', 'me', 'eol', 'for', 'all', 'the']\n",
      "['is', 'who', 'i', 'am', 'eol', 'and', 'this', 'is', 'all', 'i']\n",
      "['back', 'in', 'the', 'arms', 'i', 'love', 'eol', 'need', 'me', 'like']\n",
      "['my', 'mood', 'in', 'shades', 'of', 'blue', 'eol', 'paint', 'my', 'soul']\n",
      "['so', 'afraid', 'to', 'show', 'i', 'care', 'eol', 'will', 'he', 'think']\n",
      "['your', 'eyes', 'i', 'see', 'ribbons', 'of', 'color', 'eol', 'i', 'see']\n",
      "['was', 'young', 'eol', 'i', 'never', 'needed', 'anyone', 'eol', 'and', 'makin']\n",
      "['gettin', 'ready', 'to', 'put', 'yall', 'up', 'on', 'somethin', 'man', 'whats']\n",
      "['lick', 'you', 'up', 'and', 'down', 'till', 'you', 'say', 'stop', 'eol']\n",
      "['i', 'want', 'you', 'for', 'myself', 'eol', 'i', 'dont', 'want', 'nobody']\n",
      "['eol', 'put', 'on', 'your', 'red', 'shoes', 'and', 'dance', 'the', 'blues']\n",
      "['up', 'their', 'minds', 'and', 'they', 'started', 'packing', 'eol', 'they', 'left']\n",
      "['feel', 'like', 'im', 'drunk', 'behind', 'the', 'wheel', 'eol', 'the', 'wheel']\n",
      "['streets', 'eol', 'and', 'the', 'pavements', 'are', 'burning', 'eol', 'i', 'sit']\n",
      "['the', 'mountain', 'top', 'eol', 'burning', 'like', 'a', 'silver', 'flame', 'eol']\n",
      "['day', 'after', 'christmas', 'eol', 'i', 'throw', 'some', 'clothes', 'on', 'in']\n",
      "['eol', 'talking', 'about', 'the', 'sad', 'girls', 'eol', 'sad', 'girls', 'eol']\n",
      "['eol', 'last', 'chance', 'for', 'love', 'eol', 'yes', 'its', 'my', 'last']\n",
      "['eatin', 'my', 'heart', 'out', 'waitin', 'eol', 'waitin', 'for', 'some', 'lover']\n",
      "['so', 'good', 'its', 'so', 'good', 'eol', 'its', 'so', 'good', 'its']\n",
      "['a', 'letter', 'you', 'wrote', 'me', 'on', 'the', 'radio', 'eol', 'and']\n",
      "['me', 'if', 'i', 'love', 'you', 'eol', 'and', 'i', 'choke', 'on']\n",
      "['hours', 'for', 'this', 'eol', 'ive', 'made', 'myself', 'so', 'sick', 'eol']\n",
      "['eol', 'woman', 'take', 'me', 'in', 'your', 'arms', 'eol', 'rock', 'your']\n",
      "['la', 'la', 'la', 'la', 'la', 'eol', 'mmm', 'eol', 'uh', 'huh']\n",
      "['the', 'front', 'door', 'like', 'a', 'ghost', 'eol', 'into', 'the', 'fog']\n",
      "['to', 'escape', 'eol', 'the', 'city', 'was', 'sticky', 'and', 'cruel', 'eol']\n",
      "['home', 'in', 'the', 'morning', 'light', 'eol', 'my', 'mother', 'says', 'when']\n",
      "['my', 'bed', 'i', 'hear', 'the', 'clock', 'tick', 'eol', 'and', 'think']\n",
      "['see', 'them', 'every', 'night', 'in', 'tight', 'blue', 'jeans', 'eol', 'in']\n",
      "['the', 'night', 'eol', 'ill', 'be', 'awake', 'and', 'ill', 'be', 'with']\n",
      "['the', 'sad', 'eyes', 'eol', 'dont', 'be', 'discouraged', 'eol', 'oh', 'i']\n",
      "['are', 'eol', 'hanging', 'onto', 'strains', 'of', 'greed', 'and', 'blues', 'eol']\n",
      "['turn', 'turn', 'turn', 'eol', 'there', 'is', 'a', 'season', 'turn', 'turn']\n",
      "['stand', 'it', 'i', 'know', 'you', 'planned', 'it', 'eol', 'but', 'im']\n",
      "['eol', 'you', 'wake', 'up', 'late', 'for', 'school', 'man', 'you', 'dont']\n",
      "['planetary', 'intergalactic', 'eol', 'intergalactic', 'planetary', 'planetary', 'intergalactic', 'eol', 'intergalactic', 'planetary']\n",
      "['blues', 'you', 'made', 'me', 'cry', 'eol', 'i', 'dont', 'want', 'to']\n",
      "['in', 'the', 'evening', 'shes', 'wondering', 'what', 'clothes', 'to', 'wear', 'eol']\n",
      "['youve', 'gone', 'eol', 'all', 'thats', 'left', 'is', 'a', 'band', 'of']\n",
      "['do', 'you', 'know', 'what', 'thats', 'worth', 'eol', 'ooh', 'heaven', 'is']\n",
      "['feel', 'it', 'see', 'it', 'hear', 'it', 'today', 'eol', 'if', 'you']\n",
      "['a', 'tear', 'you', 'wiped', 'it', 'dry', 'eol', 'i', 'was', 'confused']\n",
      "['snowy', 'mantle', 'cold', 'and', 'clean', 'eol', 'the', 'unborn', 'grass', 'lies']\n",
      "['each', 'other', 'eol', 'trying', 'so', 'hard', 'to', 'stay', 'warm', 'eol']\n",
      "['oh', 'eol', 'for', 'the', 'longest', 'time', 'eol', 'oh', 'oh', 'oh']\n",
      "['oclock', 'on', 'a', 'saturday', 'eol', 'the', 'regular', 'crowd', 'shuffles', 'in']\n",
      "['virginia', 'dont', 'let', 'em', 'wait', 'eol', 'you', 'catholic', 'girls', 'start']\n",
      "['eol', 'i', 'dont', 'want', 'to', 'see', 'you', 'let', 'a', 'good']\n",
      "['like', 'to', 'get', 'away', 'eol', 'take', 'a', 'holiday', 'from', 'the']\n",
      "['eol', 'shes', 'been', 'living', 'in', 'her', 'uptown', 'world', 'eol', 'i']\n",
      "['all', 'have', 'a', 'face', 'eol', 'that', 'we', 'hide', 'away', 'forever']\n",
      "['of', 'white', 'a', 'bottle', 'of', 'red', 'eol', 'perhaps', 'a', 'bottle']\n",
      "['i', 'crashed', 'your', 'party', 'eol', 'saturday', 'i', 'said', 'im', 'sorry']\n",
      "['matter', 'with', 'the', 'clothes', 'im', 'wearing', 'eol', 'cant', 'you', 'tell']\n",
      "['went', 'uptown', 'riding', 'in', 'your', 'limousine', 'eol', 'with', 'your', 'fine']\n",
      "['living', 'here', 'in', 'allentown', 'eol', 'and', 'theyre', 'closing', 'all', 'the']\n",
      "['middle', 'of', 'the', 'night', 'eol', 'i', 'go', 'walking', 'in', 'my']\n",
      "['as', 'soulmates', 'eol', 'on', 'parris', 'inland', 'eol', 'we', 'left', 'as']\n",
      "['changing', 'to', 'try', 'and', 'please', 'me', 'eol', 'you', 'never', 'let']\n",
      "['a', 'way', 'about', 'her', 'eol', 'i', 'dont', 'know', 'what', 'it']\n",
      "['call', 'from', 'an', 'old', 'friend', 'wed', 'used', 'to', 'be', 'real']\n",
      "['onto', 'the', 'milkmans', 'hand', 'eol', 'and', 'then', 'she', 'finally', 'gave']\n",
      "['but', 'the', 'world', 'keeps', 'spinning', 'eol', 'take', 'a', 'spin', 'through']\n",
      "['of', 'an', 'ancient', 'radiation', 'eol', 'that', 'haunts', 'dismembered', 'constellations', 'eol']\n",
      "['at', 'the', 'starting', 'line', 'eol', 'engines', 'pumping', 'and', 'thumping', 'in']\n",
      "['your', 'arms', 'around', 'me', 'eol', 'i', 'need', 'to', 'feel', 'your']\n",
      "['born', 'on', 'a', 'summer', 'day', '1951', 'eol', 'and', 'with', 'a']\n",
      "['all', 'on', 'my', 'roots', 'eol', 'i', 'showed', 'up', 'in', 'boots']\n",
      "['feelin', 'the', 'blues', 'eol', 'i', 'was', 'watching', 'the', 'news', 'eol']\n",
      "['i', 'say', 'eol', 'what', 'can', 'i', 'do', 'eol', 'three', 'am']\n",
      "['the', 'boat', 'that', 'day', 'eol', 'he', 'left', 'the', 'shack', 'eol']\n",
      "['life', 'youve', 'waited', 'for', 'love', 'to', 'eol', 'come', 'and', 'stay']\n",
      "['eol', 'its', 'love', 'eol', 'love', 'get', 'busy', 'eol', 'everybodys', 'talkin']\n",
      "['havent', 'seen', 'you', 'in', 'a', 'while', 'eol', 'howve', 'you', 'been']\n",
      "['stop', 'the', 'way', 'i', 'feel', 'eol', 'things', 'you', 'do', 'dont']\n",
      "['my', 'love', 'has', 'come', 'along', 'eol', 'my', 'lonely', 'days', 'are']\n",
      "['not', 'far', 'down', 'to', 'paradise', 'at', 'least', 'its', 'not', 'for']\n",
      "['the', 'night', 'eol', 'my', 'bodys', 'weak', 'eol', 'im', 'on', 'the']\n",
      "['in', 'the', 'saddle', 'again', 'eol', 'out', 'where', 'a', 'friend', 'is']\n",
      "['chaka', 'chaka', 'khan', 'eol', 'chaka', 'khan', 'chaka', 'khan', 'chaka', 'khan']\n",
      "['eol', 'thats', 'the', 'way', 'it', 'was', 'eol', 'happened', 'so', 'naturally']\n",
      "['got', 'no', 'kind', 'of', 'feeling', 'inside', 'eol', 'i', 'got', 'something']\n",
      "['baby', 'lets', 'do', 'the', 'twist', 'eol', 'come', 'on', 'baby', 'lets']\n",
      "['the', 'old', 'apartment', 'eol', 'this', 'is', 'where', 'we', 'used', 'to']\n",
      "['in', 'the', 'door', 'a', 'step', 'on', 'the', 'floor', 'eol', 'a']\n",
      "['one', 'week', 'since', 'you', 'looked', 'at', 'me', 'eol', 'cocked', 'your']\n",
      "['be', 'loved', 'and', 'be', 'loved', 'eol', 'could', 'you', 'be', 'loved']\n",
      "['a', 'song', 'eol', 'sing', 'out', 'loud', 'eol', 'sing', 'out', 'strong']\n",
      "['at', 'the', 'two', 'of', 'us', 'eol', 'strangers', 'in', 'many', 'ways']\n",
      "['in', 'the', 'world', 'eol', 'ever', 'had', 'a', 'love', 'as', 'sweet']\n",
      "['in', 'my', 'heart', 'eol', 'from', 'early', 'in', 'the', 'mornin', 'til']\n",
      "['feelins', 'comin', 'over', 'me', 'eol', 'there', 'is', 'wonder', 'in', 'most']\n",
      "['enough', 'of', 'being', 'alone', 'eol', 'everyone', 'must', 'face', 'their', 'share']\n",
      "['so', 'many', 'places', 'in', 'my', 'life', 'and', 'time', 'eol', 'ive']\n",
      "['was', 'young', 'id', 'listen', 'to', 'the', 'radio', 'eol', 'waitin', 'for']\n",
      "['have', 'all', 'been', 'sent', 'eol', 'the', 'christmas', 'rush', 'is', 'through']\n",
      "['and', 'oh', 'so', 'far', 'away', 'eol', 'i', 'fell', 'in', 'love']\n",
      "['goodbye', 'to', 'love', 'eol', 'no', 'one', 'ever', 'cared', 'if', 'i']\n",
      "['myself', 'and', 'feelin', 'old', 'eol', 'sometimes', 'id', 'like', 'to', 'quit']\n",
      "['birds', 'suddenly', 'appear', 'eol', 'every', 'time', 'you', 'are', 'near', 'eol']\n",
      "['by', 'the', 'moon', 'and', 'the', 'stars', 'in', 'the', 'skies', 'eol']\n",
      "['by', 'the', 'moon', 'and', 'the', 'stars', 'in', 'the', 'skies', 'eol']\n",
      "['to', 'play', 'a', 'game', 'eol', 'that', 'is', 'so', 'much', 'fun']\n",
      "['old', 'lover', 'in', 'the', 'grocery', 'store', 'eol', 'the', 'snow', 'was']\n",
      "['place', 'for', 'me', 'to', 'hide', 'eol', 'the', 'thoughts', 'of', 'all']\n",
      "['bop', 'a', 'lula', 'shes', 'my', 'baby', 'eol', 'be', 'bop', 'a']\n",
      "['to', 'say', 'it', 'but', 'i', 'told', 'you', 'so', 'dont', 'mind']\n",
      "['watchin', 'you', 'for', 'days', 'now', 'baby', 'eol', 'i', 'just', 'love']\n",
      "['look', 'at', 'the', 'letters', 'that', 'you', 'wrote', 'to', 'me', 'eol']\n",
      "['my', 'heart', 'lies', 'a', 'melody', 'eol', 'a', 'song', 'of', 'old']\n",
      "['four', 'eol', 'eins', 'zwei', 'drei', 'eol', 'na', 'es', 'is', 'nix']\n",
      "['all', 'the', 'time', 'to', 'the', 'top', 'eol', 'er', 'war', 'ein']\n",
      "['happening', 'now', 'eol', 'hey', 'hey', 'whoa', 'oh', 'eol', 'hey', 'hey']\n",
      "['up', 'heres', 'a', 'story', 'eol', 'about', 'a', 'little', 'guy', 'that']\n",
      "['wanna', 'be', 'your', 'clown', 'again', 'eol', 'and', 'i', 'dont', 'wanna']\n",
      "['too', 'much', 'of', 'heaven', 'can', 'bring', 'you', 'underground', 'eol', 'heaven']\n",
      "['its', 'another', 'race', 'from', 'outer', 'space', 'eol', 'its', 'another', 'race']\n",
      "['is', 'looking', 'for', 'the', 'dub', 'in', 'this', 'life', 'eol', 'everyone']\n",
      "['i', 'was', 'afraid', 'i', 'was', 'petrified', 'eol', 'kept', 'thinking', 'i']\n",
      "['can', 'say', 'goodbye', 'no', 'no', 'no', 'no', 'now', 'eol', 'never']\n",
      "['was', 'a', 'preachers', 'son', 'eol', 'and', 'when', 'his', 'daddy', 'would']\n",
      "['know', 'what', 'it', 'is', 'that', 'makes', 'me', 'love', 'you', 'so']\n",
      "['questions', 'that', 'i', 'need', 'to', 'know', 'eol', 'how', 'you', 'could']\n",
      "['on', 'bring', 'it', 'bring', 'it', 'on', 'now', 'eol', 'bring', 'it']\n",
      "['go', 'sister', 'soul', 'sister', 'go', 'sister', 'eol', 'hey', 'sister', 'go']\n",
      "['fi', 'fi', 'fo', 'fo', 'fum', 'eol', 'i', 'smell', 'smoke', 'in']\n",
      "['the', 'papers', 'and', 'the', 'trash', 'eol', 'or', 'you', 'dont', 'get']\n",
      "['my', 'sunglasses', 'at', 'night', 'eol', 'so', 'i', 'can', 'so', 'i']\n",
      "['of', 'my', 'mind', 'eol', 'going', 'out', 'of', 'my', 'mind', 'eol']\n",
      "['right', 'now', 'eol', 'right', 'here', 'right', 'now', 'eol', 'right', 'here']\n",
      "['now', 'the', 'funk', 'soul', 'brother', 'eol', 'check', 'it', 'out', 'now']\n",
      "['a', 'long', 'long', 'way', 'together', 'eol', 'through', 'the', 'hard', 'times']\n",
      "['eol', 'shamari', 'eol', 'there', 'you', 'are', 'eol', 'looking', 'as', 'fine']\n",
      "['when', 'theres', 'someone', 'else', 'who', 'cares', 'eol', 'when', 'theres', 'someone']\n",
      "['was', 'lola', 'she', 'was', 'a', 'showgirl', 'eol', 'with', 'yellow', 'feathers']\n",
      "['all', 'my', 'life', 'eol', 'raining', 'down', 'as', 'cold', 'as', 'ice']\n",
      "['night', 'has', 'come', 'eol', 'and', 'the', 'land', 'is', 'dark', 'eol']\n",
      "['a', 'song', 'that', 'aint', 'no', 'melody', 'im', 'gonna', 'sing', 'it']\n",
      "['there', 'i', 'want', 'to', 'go', 'there', 'eol', 'take', 'me', 'there']\n",
      "['you', 'know', 'what', 'eol', 'i', 'like', 'the', 'playettes', 'eol', 'no']\n",
      "['motion', 'eol', 'in', 'my', 'foolish', 'lovers', 'game', 'eol', 'on', 'this']\n",
      "['eol', 'berlin', 'eol', 'im', 'alone', 'eol', 'sitting', 'with', 'my', 'empty']\n",
      "['was', 'young', 'eol', 'i', 'never', 'needed', 'anyone', 'eol', 'and', 'makin']\n",
      "['meaning', 'to', 'tell', 'you', 'eol', 'ive', 'got', 'this', 'feelin', 'that']\n",
      "['bailamos', 'eol', 'te', 'doy', 'toda', 'mi', 'vida', 'eol', 'quã©date', 'conmigo']\n",
      "['de', 'ti', 'para', 'sobrevivir', 'eol', 'esta', 'noche', 'que', 'viene', 'fria']\n",
      "['i', 'feel', 'so', 'low', 'eol', 'i', 'count', 'the', 'hours', 'they']\n",
      "['called', 'it', 'puppy', 'love', 'eol', 'oh', 'i', 'guess', 'theyll', 'never']\n",
      "['the', 'middle', 'of', 'a', 'dry', 'spell', 'eol', 'jimmy', 'rodgers', 'on']\n",
      "['a', 'moment', 'in', 'space', 'eol', 'when', 'the', 'dream', 'is', 'gone']\n",
      "['the', 'corners', 'of', 'my', 'mind', 'eol', 'misty', 'water', 'colored', 'memries']\n",
      "['whisperin', 'in', 'the', 'trees', 'eol', 'its', 'two', 'sailors', 'and', 'theyre']\n",
      "['my', 'church', 'eol', 'this', 'is', 'where', 'i', 'heal', 'my', 'hurts']\n",
      "['submitted', 'those', 'lyrics', 'must', 'have', 'been', 'deaf', 'lolhere', 'are', 'the']\n",
      "['my', 'heart', 'i', 'love', 'you', 'baby', 'eol', 'stay', 'with', 'me']\n",
      "['standin', 'at', 'the', 'station', 'eol', 'ten', 'to', 'midnight', 'in', 'the']\n",
      "['lost', 'in', 'your', 'eyes', 'eol', 'and', 'i', 'feel', 'my', 'spirits']\n",
      "['there', 'was', 'a', 'time', 'when', 'eol', 'broken', 'hearts', 'and', 'broken']\n",
      "['up', 'the', 'country', 'baby', 'dont', 'you', 'want', 'to', 'go', 'eol']\n",
      "['me', 'youre', 'in', 'love', 'with', 'me', 'eol', 'like', 'you', 'cant']\n",
      "['baby', 'eol', 'oh', 'baby', 'baby', 'eol', 'oh', 'baby', 'baby', 'how']\n",
      "['a', 'story', 'about', 'a', 'girl', 'named', 'lucky', 'eol', 'early', 'morning']\n",
      "['yeah', 'eol', 'hush', 'just', 'stop', 'eol', 'theres', 'nothing', 'you', 'can']\n",
      "['eol', 'ah', 'here', 'we', 'go', 'now', 'oh', 'eol', 'like', 'a']\n",
      "['life', 'oh', 'yeah', 'yeah', 'eol', 'oh', 'yeah', 'eol', 'im', 'sitting']\n",
      "['back', 'we', 'said', 'eol', 'how', 'was', 'i', 'to', 'know', 'id']\n",
      "['say', 'youre', 'so', 'into', 'me', 'eol', 'and', 'that', 'you', 'need']\n",
      "['so', 'into', 'you', 'eol', 'youve', 'got', 'that', 'something', 'what', 'can']\n",
      "['good', 'eol', 'aint', 'it', 'right', 'eol', 'that', 'you', 'are', 'with']\n",
      "['ever', 'told', 'you', 'eol', 'how', 'good', 'it', 'feels', 'to', 'hold']\n",
      "['i', 'wanted', 'eol', 'something', 'special', 'something', 'sacred', 'eol', 'in', 'your']\n",
      "['hear', 'a', 'different', 'story', 'eol', 'people', 'saying', 'that', 'youre', 'no']\n",
      "['enough', 'of', 'danger', 'eol', 'and', 'people', 'on', 'the', 'streets', 'eol']\n",
      "['guess', 'it', 'would', 'be', 'nice', 'eol', 'if', 'i', 'could', 'touch']\n",
      "['your', 'eyes', 'on', 'the', 'road', 'your', 'hands', 'upon', 'the', 'wheel']\n",
      "['a', 'bad', 'bad', 'girl', 'eol', 'ive', 'been', 'careless', 'with', 'a']\n",
      "['eol', 'we', 'have', 'no', 'secrets', 'eol', 'we', 'tell', 'each', 'other']\n",
      "['new', 'kind', 'of', 'dancing', 'eol', 'thats', 'going', 'to', 'be', 'the']\n",
      "['wed', 'light', 'the', 'firmament', 'eol', 'you', 'said', 'our', 'love', 'was']\n",
      "['it', 'better', 'makes', 'me', 'feel', 'sad', 'for', 'the', 'rest', 'eol']\n",
      "['you', 'heard', 'eol', 'hes', 'gonna', 'buy', 'me', 'a', 'mockingbird', 'eol']\n",
      "['oh', 'venus', 'eol', 'venus', 'if', 'you', 'will', 'eol', 'please', 'send']\n",
      "['bought', 'his', 'babe', 'a', 'diamond', 'ring', 'eol', 'if', 'that', 'diamond']\n",
      "['want', 'it', 'here', 'it', 'is', 'come', 'and', 'get', 'it', 'eol']\n",
      "['what', 'you', 'are', 'eol', 'i', 'will', 'always', 'be', 'with', 'you']\n",
      "['finding', 'out', 'about', 'you', 'eol', 'every', 'day', 'my', 'mind', 'is']\n",
      "['got', 'what', 'i', 'deserve', 'eol', 'kept', 'you', 'waiting', 'there', 'too']\n",
      "['feel', 'the', 'magic', 'floating', 'in', 'the', 'air', 'eol', 'being', 'with']\n",
      "['eol', 'never', 'turn', 'you', 'down', 'eol', 'when', 'all', 'the', 'others']\n",
      "['to', 'think', 'that', 'love', 'was', 'just', 'a', 'fairy', 'tale', 'eol']\n",
      "['know', 'what', 'youve', 'got', 'but', 'it', 'plays', 'with', 'my', 'emotions']\n",
      "['people', 'so', 'why', 'should', 'it', 'be', 'eol', 'you', 'and', 'i']\n",
      "['eol', 'seals', 'the', 'contract', 'eol', 'from', 'the', 'contracts', 'eol', 'theres']\n",
      "['new', 'game', 'eol', 'we', 'like', 'to', 'play', 'you', 'see', 'eol']\n",
      "['learnin', 'eol', 'that', 'so', 'many', 'yearnings', 'eol', 'are', 'never', 'to']\n",
      "['knew', 'thered', 'come', 'a', 'day', 'eol', 'when', 'id', 'be', 'sayin']\n",
      "['love', 'me', 'i', 'want', 'to', 'know', 'eol', 'how', 'can', 'i']\n",
      "['how', 'hard', 'i', 'try', 'eol', 'you', 'keep', 'pushing', 'me', 'aside']\n",
      "['wonder', 'eol', 'how', 'id', 'ever', 'make', 'it', 'through', 'eol', 'through']\n",
      "['on', 'the', 'morning', 'rain', 'eol', 'i', 'used', 'to', 'feel', 'so']\n",
      "['want', 'eol', 'baby', 'i', 'got', 'it', 'eol', 'what', 'you', 'need']\n",
      "['think', 'think', 'eol', 'think', 'about', 'what', 'youre', 'trying', 'to', 'do']\n",
      "['carefree', 'laughter', 'eol', 'silence', 'ever', 'after', 'eol', 'walking', 'through', 'an']\n",
      "['hear', 'the', 'drums', 'fernando', 'eol', 'i', 'remember', 'long', 'ago', 'another']\n",
      "['you', 'can', 'dance', 'eol', 'you', 'can', 'jive', 'eol', 'having', 'the']\n",
      "['eol', 'at', 'waterloo', 'napoleon', 'did', 'surrender', 'eol', 'oh', 'yeah', 'eol']\n",
      "['beams', 'are', 'gonna', 'blind', 'me', 'eol', 'but', 'i', 'wont', 'feel']\n",
      "['change', 'your', 'mind', 'im', 'the', 'first', 'in', 'line', 'eol', 'honey']\n",
      "['all', 'night', 'i', 'work', 'all', 'day', 'to', 'pay', 'the', 'bills']\n",
      "['i', 'see', 'the', 'sign', 'that', 'points', 'one', 'way', 'eol', 'the']\n",
      "['twelve', 'eol', 'watchin', 'the', 'late', 'show', 'eol', 'in', 'my', 'flat']\n",
      "['special', 'in', 'fact', 'im', 'a', 'bit', 'of', 'a', 'bore', 'eol']\n",
      "['want', 'to', 'talk', 'eol', 'about', 'the', 'things', 'weve', 'gone', 'through']\n",
      "['these', 'are', 'the', 'good', 'times', 'eol', 'leave', 'your', 'cares', 'behind']\n",
      "['out', 'le', 'freak', 'cest', 'chic', 'eol', 'have', 'you', 'heard', 'about']\n",
      "['my', 'thrill', 'eol', 'on', 'blueberry', 'hill', 'eol', 'on', 'blueberry', 'hill']\n",
      "['first', 'time', 'wont', 'be', 'the', 'last', 'time', 'eol', 'dont', 'you']\n",
      "['the', 'park', 'eol', 'i', 'think', 'it', 'was', 'the', 'fourth', 'of']\n",
      "['the', 'break', 'of', 'day', 'eol', 'searching', 'for', 'something', 'to', 'say']\n",
      "['and', 'dreamless', 'nights', 'and', 'far', 'aways', 'eol', 'ooo', 'ooo', 'ooo']\n",
      "['goes', 'on', 'eol', 'i', 'realize', 'eol', 'just', 'what', 'you', 'mean']\n",
      "['with', 'you', 'eol', 'it', 'doesnt', 'matter', 'where', 'we', 'are', 'eol']\n",
      "['boys', 'are', 'eol', 'someone', 'waits', 'for', 'me', 'eol', 'a', 'smiling']\n",
      "['youre', 'a', 'real', 'mean', 'guy', 'eol', 'id', 'like', 'to', 'clip']\n",
      "['gotta', 'say', 'goodbye', 'for', 'the', 'summer', 'eol', 'baby', 'i', 'promise']\n",
      "['blue', 'velvet', 'eol', 'bluer', 'than', 'velvet', 'was', 'the', 'night', 'eol']\n",
      "['like', 'to', 'ride', 'in', 'my', 'beautiful', 'balloon', 'eol', 'would', 'you']\n",
      "['du', 'du', 'dudududududuud', 'eol', 'dudududuudu', 'eol', 'dudududududududududuududududududududuud', 'eol', 'dudududuu', 'eol']\n",
      "['to', 'say', 'it', 'and', 'its', 'hard', 'for', 'me', 'eol', 'you']\n",
      "['i', 'wake', 'up', 'eol', 'before', 'i', 'put', 'on', 'my', 'makeup']\n",
      "['know', 'the', 'way', 'to', 'san', 'jose', 'eol', 'ive', 'been', 'away']\n",
      "['see', 'me', 'walking', 'down', 'the', 'street', 'eol', 'and', 'i', 'start']\n",
      "['was', 'on', 'fire', 'and', 'no', 'one', 'could', 'save', 'me', 'but']\n",
      "['boats', 'go', 'out', 'across', 'the', 'evening', 'water', 'eol', 'smuggling', 'guns']\n",
      "['late', 'in', 'december', 'the', 'sky', 'turned', 'to', 'snow', 'eol', 'all']\n",
      "['morning', 'from', 'a', 'bogart', 'movie', 'eol', 'in', 'a', 'country', 'where']\n",
      "['must', 'be', 'a', 'kind', 'of', 'blind', 'love', 'eol', 'i', 'cant']\n",
      "['you', 'walking', 'everyday', 'eol', 'with', 'a', 'smile', 'beneath', 'frown', 'eol']\n",
      "['the', 'right', 'time', 'once', 'in', 'a', 'lifetime', 'eol', 'so', 'i']\n",
      "['you', 'in', 'my', 'memory', 'eol', 'as', 'vivid', 'as', 'today', 'eol']\n",
      "['yeah', 'yeah', 'yeah', 'eol', 'last', 'night', 'before', 'you', 'fell', 'asleep']\n",
      "['yeah', 'yeah', 'yeah', 'eol', 'yeah', 'yeah', 'yeah', 'yeah', 'yeah', 'eol']\n",
      "['you', 'go', 'again', 'eol', 'you', 'say', 'you', 'want', 'your', 'freedom']\n",
      "['my', 'love', 'by', 'his', 'way', 'of', 'walking', 'eol', 'and', 'i']\n",
      "['me', 'with', 'your', 'stories', 'eol', 'i', 'cant', 'believe', 'that', 'i']\n",
      "['at', 'night', 'and', 'im', 'feeling', 'down', 'eol', 'there', 'are', 'couples']\n",
      "['love', 'to', 'love', 'you', 'like', 'you', 'do', 'me', 'eol', 'id']\n",
      "['staring', 'on', 'eol', 'watching', 'her', 'life', 'go', 'by', 'eol', 'when']\n",
      "['oh', 'oh', 'yeah', 'yeah', 'eol', 'she', 'drove', 'a', 'long', 'way']\n",
      "['our', 'love', 'thats', 'all', 'we', 'have', 'eol', 'what', 'we', 'had']\n",
      "['yeah', 'eol', 'just', 'when', 'i', 'thought', 'i', 'was', 'safe', 'eol']\n",
      "['eol', 'hiding', 'in', 'the', 'dark', 'eol', 'im', 'looking', 'for', 'a']\n",
      "['just', 'a', 'dream', 'boat', 'eol', 'sailing', 'in', 'my', 'head', 'eol']\n",
      "['true', 'theres', 'nothing', 'like', 'me', 'and', 'you', 'eol', 'not', 'alone']\n",
      "['is', 'a', 'river', 'and', 'your', 'heart', 'is', 'a', 'boat', 'eol']\n",
      "['slept', 'at', 'all', 'in', 'days', 'eol', 'its', 'been', 'so', 'long']\n",
      "['giant', 'to', 'supernova', 'eol', 'back', 'to', 'you', 'and', 'me', 'eol']\n",
      "['drops', 'and', 'no', 'one', 'stirs', 'eol', 'on', 'a', 'lazy', 'summers']\n",
      "['in', 'our', 'hearts', 'eol', 'strangers', 'apart', 'eol', 'oh', 'please', 'come']\n",
      "['a', 'home', 'in', 'a', 'quiet', 'place', 'eol', 'i', 'see', 'myself']\n",
      "['daylights', 'gone', 'and', 'youre', 'on', 'your', 'own', 'eol', 'and', 'you']\n",
      "['the', 'sunshine', 'baby', 'whenever', 'you', 'smiled', 'eol', 'but', 'i', 'call']\n",
      "['here', 'and', 'warm', 'eol', 'but', 'i', 'could', 'look', 'away', 'and']\n",
      "['those', 'big', 'dark', 'eyes', 'that', 'flash', 'at', 'me', 'baby', 'eol']\n",
      "['the', 'christmas', 'tree', 'eol', 'at', 'the', 'christmas', 'party', 'hop', 'eol']\n",
      "['is', 'this', 'who', 'laid', 'to', 'rest', 'eol', 'on', 'marys', 'lap']\n",
      "['watch', 'out', 'eol', 'you', 'better', 'not', 'cry', 'eol', 'better', 'not']\n",
      "['those', 'sleigh', 'bells', 'jingling', 'eol', 'ring', 'ting', 'tingling', 'too', 'eol']\n",
      "['of', 'a', 'white', 'christmas', 'eol', 'just', 'like', 'the', 'ones', 'i']\n",
      "['mommy', 'kissing', 'santa', 'claus', 'eol', 'underneath', 'the', 'mistletoe', 'last', 'night']\n",
      "['ring', 'eol', 'are', 'you', 'listening', 'eol', 'in', 'the', 'lane', 'eol']\n",
      "['night', 'wind', 'to', 'the', 'little', 'lamb', 'eol', 'do', 'you', 'see']\n",
      "['are', 'brightly', 'shining', 'eol', 'it', 'is', 'the', 'night', 'of', 'our']\n",
      "['the', 'world', 'the', 'lord', 'is', 'come', 'eol', 'let', 'earth', 'receive']\n",
      "['the', 'snow', 'eol', 'in', 'a', 'one', 'horse', 'open', 'sleigh', 'eol']\n",
      "['mmm', 'eol', 'have', 'yourself', 'a', 'merry', 'little', 'christmas', 'eol', 'let']\n",
      "['noel', 'the', 'angels', 'did', 'say', 'eol', 'was', 'to', 'certain', 'poor']\n",
      "['all', 'ye', 'faithful', 'joyful', 'and', 'triumphant', 'eol', 'oh', 'come', 'ye']\n",
      "['snowman', 'was', 'a', 'jolly', 'happy', 'soul', 'eol', 'with', 'a', 'corncob']\n",
      "['have', 'heard', 'on', 'high', 'eol', 'angels', 'we', 'have', 'heard', 'on']\n",
      "['weather', 'outside', 'is', 'frightful', 'eol', 'but', 'the', 'fire', 'is', 'so']\n",
      "['holly', 'jolly', 'christmas', 'eol', 'its', 'the', 'best', 'time', 'of', 'the']\n",
      "['dasher', 'and', 'dancer', 'and', 'prancer', 'and', 'vixen', 'eol', 'you', 'know']\n",
      "['told', 'me', 'eol', 'pa', 'rum', 'pum', 'pum', 'pum', 'eol', 'a']\n",
      "['kings', 'eol', 'we', 'three', 'kings', 'of', 'orient', 'are', 'eol', 'bearing']\n",
      "['busy', 'sidewalks', 'eol', 'dressed', 'in', 'holiday', 'style', 'eol', 'in', 'the']\n",
      "['halls', 'with', 'boughs', 'of', 'holly', 'eol', 'fa', 'la', 'la', 'la']\n",
      "['my', 'problems', 'and', 'i', 'see', 'the', 'light', 'eol', 'we', 'got']\n",
      "['seen', 'a', 'man', 'at', 'the', 'liquor', 'store', 'beggin', 'for', 'your']\n",
      "['i', 'do', 'now', 'eol', 'i', 'do', 'i', 'do', 'eol', 'all']\n",
      "['know', 'that', 'i', 'shouldnt', 'be', 'here', 'eol', 'this', 'is', 'wrong']\n",
      "['flipmode', 'eol', 'here', 'we', 'come', 'bout', 'to', 'bust', 'and', 'explode']\n",
      "['than', 'you', 'want', 'me', 'to', 'eol', 'cant', 'help', 'myself', 'when']\n",
      "['the', 'city', 'of', 'new', 'orleans', 'eol', 'illinois', 'central', 'monday', 'morning']\n",
      "['dee', 'do', 'day', 'eol', 'dee', 'do', 'dee', 'do', 'dee', 'do']\n",
      "['nothing', 'but', 'bad', 'luck', 'eol', 'since', 'the', 'day', 'i', 'saw']\n",
      "['the', 'woman', 'that', 'ive', 'always', 'dreamed', 'of', 'eol', 'i', 'knew']\n",
      "['all', 'goes', 'crazy', 'and', 'the', 'thrill', 'is', 'gone', 'eol', 'when']\n",
      "['now', 'eol', 'its', 'in', 'your', 'eyes', 'eol', 'theres', 'no', 'disguising']\n",
      "['ride', 'the', 'hounds', 'of', 'hell', 'eol', 'twist', 'my', 'foot', 'i']\n",
      "['distance', 'eol', 'the', 'world', 'looks', 'blue', 'and', 'green', 'eol', 'and']\n",
      "['love', 'it', 'is', 'a', 'river', 'eol', 'that', 'drowns', 'the', 'tender']\n",
      "['knew', 'there', 'was', 'a', 'eol', 'love', 'like', 'this', 'before', 'eol']\n",
      "['my', 'world', 'eol', 'wont', 'you', 'come', 'on', 'in', 'eol', 'miracles']\n",
      "['make', 'me', 'lose', 'my', 'mind', 'eol', 'up', 'in', 'here', 'up']\n",
      "['back', 'to', 'memphis', 'eol', 'gotta', 'find', 'my', 'daisy', 'jane', 'eol']\n",
      "['eol', 'when', 'things', 'are', 'real', 'eol', 'and', 'the', 'people', 'share']\n",
      "['foggy', 'outside', 'eol', 'all', 'the', 'planes', 'have', 'been', 'grounded', 'eol']\n",
      "['tried', 'to', 'make', 'it', 'sunday', 'but', 'i', 'got', 'so', 'damn']\n",
      "['for', 'all', 'the', 'lonely', 'people', 'eol', 'thinking', 'that', 'life', 'has']\n",
      "['a', 'piece', 'of', 'grass', 'eol', 'walking', 'down', 'the', 'road', 'eol']\n",
      "['to', 'laugh', 'we', 'used', 'to', 'cry', 'eol', 'we', 'used', 'to']\n",
      "['if', 'you', 'could', 'return', 'eol', 'dont', 'let', 'it', 'burn', 'eol']\n",
      "['hangs', 'lowly', 'eol', 'child', 'is', 'slowly', 'taken', 'eol', 'and', 'if']\n",
      "['you', 'ruled', 'my', 'mind', 'eol', 'i', 'thought', 'youd', 'always', 'be']\n",
      "['swallow', 'my', 'pride', 'eol', 'i', 'would', 'choke', 'on', 'the', 'rinds']\n",
      "['nights', 'id', 'sit', 'by', 'my', 'window', 'eol', 'waiting', 'for', 'someone']\n",
      "['think', 'youre', 'better', 'off', 'alone', 'eol', 'do', 'you', 'think', 'youre']\n",
      "['sail', 'let', 'me', 'sail', 'eol', 'let', 'the', 'orinoco', 'flow', 'eol']\n",
      "['girlfriend', 'takes', 'me', 'home', 'when', 'im', 'too', 'drunk', 'to', 'drive']\n",
      "['to', 'tell', 'me', 'what', 'you', 'think', 'about', 'me', 'eol', 'i']\n",
      "['i', 'said', 'it', 'many', 'ways', 'eol', 'too', 'scared', 'to', 'run']\n",
      "['the', 'crowd', 'and', 'plays', 'so', 'loud', 'eol', 'baby', 'its', 'the']\n",
      "['through', 'runnin', 'its', 'true', 'eol', 'id', 'be', 'a', 'fool', 'to']\n",
      "['her', 'diary', 'underneath', 'a', 'tree', 'eol', 'and', 'started', 'reading', 'about']\n",
      "['me', 'from', 'harm', 'eol', 'kept', 'me', 'warm', 'kept', 'me', 'warm']\n",
      "['you', 'ever', 'tried', 'eol', 'really', 'reaching', 'out', 'for', 'the', 'other']\n",
      "['picture', 'paints', 'a', 'thousand', 'words', 'eol', 'then', 'why', 'cant', 'i']\n",
      "['life', 'youve', 'waited', 'for', 'love', 'to', 'eol', 'come', 'and', 'stay']\n",
      "['was', 'her', 'name', 'eol', 'a', 'not', 'so', 'very', 'ordinary', 'girl']\n",
      "['the', 'neon', 'lights', 'are', 'bright', 'eol', 'on', 'broadway', 'on', 'broadway']\n",
      "['dance', 'eol', 'every', 'dance', 'with', 'the', 'guy', 'who', 'gives', 'you']\n",
      "['old', 'world', 'starts', 'getting', 'me', 'down', 'eol', 'and', 'people', 'are']\n",
      "['moment', 'so', 'different', 'and', 'so', 'new', 'eol', 'was', 'like', 'any']\n",
      "['the', 'sun', 'beats', 'down', 'and', 'burns', 'the', 'tar', 'up', 'on']\n",
      "['mad', 'about', 'saffron', 'eol', 'saffrons', 'mad', 'about', 'me', 'eol', 'im']\n",
      "['softly', 'through', 'my', 'a', 'window', 'today', 'eol', 'couldve', 'tripped', 'out']\n",
      "['down', 'eol', 'im', 'going', 'down', 'down', 'down', 'eol', 'down', 'down']\n",
      "['she', 'may', 'not', 'a', 'look', 'eol', 'like', 'one', 'of', 'those']\n",
      "['a', 'drag', 'eol', 'when', 'your', 'baby', 'dont', 'love', 'you', 'eol']\n",
      "['could', 'reach', 'the', 'stars', 'id', 'pull', 'one', 'down', 'for', 'you']\n",
      "['my', 'heart', 'beat', 'again', 'eol', 'when', 'does', 'the', 'pain', 'ever']\n",
      "['tight', 'clothing', 'and', 'high', 'heel', 'shoes', 'eol', 'it', 'doesnt', 'mean']\n",
      "['never', 'gonna', 'get', 'it', 'eol', 'never', 'ever', 'gonna', 'get', 'it']\n",
      "['legs', 'dont', 'work', 'like', 'they', 'used', 'to', 'before', 'eol', 'and']\n",
      "['i', 'hope', 'and', 'pray', 'eol', 'a', 'dream', 'lover', 'will', 'come']\n",
      "['shark', 'babe', 'has', 'such', 'teeth', 'dear', 'eol', 'and', 'it', 'shows']\n",
      "['walk', 'walk', 'walk', 'eol', 'oh', 'walk', 'walk', 'walk', 'man', 'eol']\n",
      "['on', 'living', 'and', 'keep', 'on', 'forgiving', 'because', 'eol', 'you', 'were']\n",
      "['baby', 'eol', 'sherry', 'sherry', 'baby', 'eol', 'sherry', 'baby', 'sherry', 'baby']\n",
      "['oh', 'rag', 'doll', 'ooh', 'eol', 'hand', 'me', 'down', 'eol', 'when']\n",
      "['a', 'midsummers', 'morn', 'eol', 'they', 'call', 'her', 'dawn', 'eol', 'dawn']\n",
      "['love', 'eol', 'when', 'i', 'think', 'about', 'those', 'nights', 'in', 'montreal']\n",
      "['is', 'on', 'on', 'the', 'street', 'eol', 'inside', 'your', 'head', 'on']\n",
      "['goes', 'down', 'eol', 'the', 'night', 'rolls', 'in', 'eol', 'you', 'can']\n",
      "['you', 'need', 'a', 'friend', 'eol', 'someone', 'you', 'can', 'talk', 'to']\n",
      "['down', 'baby', 'ooo', 'uh', 'boogie', 'baby', 'lets', 'boogie', 'down', 'im']\n",
      "['the', 'longest', 'night', 'wont', 'last', 'forever', 'eol', 'but', 'too', 'many']\n",
      "['like', 'a', 'river', 'eol', 'time', 'beckoning', 'me', 'eol', 'who', 'knows']\n",
      "['no', 'sign', 'of', 'light', 'as', 'we', 'stand', 'in', 'the', 'darkness']\n",
      "['we', 'go', 'from', 'here', 'now', 'that', 'all', 'other', 'children', 'are']\n",
      "['got', 'a', 'heart', 'of', 'stone', 'eol', 'im', 'hurtin', 'more', 'now']\n",
      "['sorrys', 'easily', 'said', 'eol', 'dont', 'try', 'turning', 'tables', 'instead', 'eol']\n",
      "['closed', 'my', 'eyes', 'again', 'eol', 'climbed', 'aboard', 'the', 'dream', 'weaver']\n",
      "['think', 'its', 'time', 'to', 'get', 'ready', 'eol', 'to', 'realize', 'just']\n",
      "['a', 'patient', 'boy', 'eol', 'i', 'wait', 'i', 'wait', 'i', 'wait']\n",
      "['want', 'to', 'be', 'the', 'kind', 'to', 'hesitate', 'eol', 'be', 'too']\n",
      "['is', 'unconditional', 'eol', 'we', 'knew', 'it', 'from', 'the', 'start', 'eol']\n",
      "['all', 'my', 'exs', 'live', 'in', 'texas', 'eol', 'and', 'texas', 'is']\n",
      "['mornin', 'eol', 'up', 'from', 'san', 'antone', 'eol', 'everything', 'that', 'i']\n",
      "['of', 'god', 'eol', 'youve', 'got', 'to', 'go', 'faster', 'than', 'that']\n",
      "['of', 'bed', 'wasnt', 'feeling', 'too', 'good', 'eol', 'with', 'my', 'wallet']\n",
      "['need', 'is', 'a', 'tv', 'show', 'that', 'and', 'the', 'radio', 'eol']\n",
      "['key', 'to', 'my', 'survival', 'eol', 'was', 'never', 'in', 'much', 'doubt']\n",
      "['asleep', 'they', 'may', 'show', 'you', 'eol', 'aerial', 'views', 'of', 'the']\n",
      "['been', 'waiting', 'waiting', 'here', 'so', 'long', 'eol', 'but', 'thinking', 'nothing']\n",
      "['dreamed', 'a', 'thousand', 'dreams', 'eol', 'been', 'haunted', 'by', 'a', 'million']\n",
      "['dust', 'that', 'settles', 'all', 'around', 'me', 'eol', 'i', 'must', 'find']\n",
      "['the', 'life', 'on', 'the', 'city', 'of', 'gold', 'eol', 'hed', 'left']\n",
      "['be', 'some', 'misunderstanding', 'eol', 'there', 'must', 'be', 'some', 'kind', 'of']\n",
      "['time', 'i', 'was', 'searching', 'nowhere', 'to', 'run', 'to', 'it', 'started']\n",
      "['see', 'the', 'face', 'on', 'the', 'tv', 'screen', 'eol', 'coming', 'at']\n",
      "['is', 'clear', 'eol', 'though', 'no', 'eyes', 'can', 'see', 'eol', 'the']\n",
      "['on', 'the', 'wall', 'there', 'on', 'the', 'floor', 'eol', 'under', 'the']\n",
      "['the', 'blind', 'side', 'shinning', 'up', 'the', 'wall', 'eol', 'stealing', 'through']\n",
      "['eol', 'paperlate', 'paperlate', 'eol', 'paperlate', 'oh', 'im', 'sorry', 'but', 'there']\n",
      "['stations', 'eol', 'can', 'anybody', 'tell', 'me', 'tell', 'me', 'exactly', 'where']\n",
      "['down', 'eol', 'coming', 'down', 'like', 'a', 'monkey', 'eol', 'but', 'its']\n",
      "['my', 'do', 'you', 'want', 'to', 'say', 'goodbye', 'eol', 'to', 'have']\n",
      "['are', 'to', 'me', 'all', 'that', 'a', 'woman', 'should', 'be', 'eol']\n",
      "['too', 'much', 'for', 'the', 'man', 'eol', 'too', 'much', 'for', 'the']\n",
      "['youre', 'home', 'eol', 'now', 'did', 'you', 'really', 'miss', 'me', 'eol']\n",
      "['day', 'im', 'more', 'confused', 'eol', 'so', 'i', 'look', 'for', 'the']\n",
      "['love', 'you', 'eol', 'is', 'not', 'the', 'words', 'i', 'want', 'to']\n",
      "['we', 'need', 'is', 'candlelight', 'eol', 'you', 'and', 'me', 'and', 'a']\n",
      "['the', 'earth', 'move', 'under', 'my', 'feet', 'eol', 'i', 'feel', 'the']\n",
      "['away', 'eol', 'doesnt', 'anybody', 'stay', 'in', 'one', 'place', 'any', 'more']\n",
      "['mine', 'completely', 'eol', 'you', 'give', 'your', 'love', 'so', 'sweety', 'eol']\n",
      "['every', 'now', 'and', 'then', 'i', 'get', 'a', 'little', 'bit', 'lonely']\n",
      "['all', 'the', 'good', 'men', 'gone', 'eol', 'and', 'where', 'are', 'all']\n",
      "['duke', 'duke', 'of', 'earl', 'eol', 'duke', 'duke', 'duke', 'of', 'earl']\n",
      "['doin', 'out', 'there', 'yever', 'seem', 'to', 'have', 'one', 'of', 'those']\n",
      "['a', 'lonely', 'life', 'eol', 'she', 'leads', 'a', 'lonely', 'life', 'eol']\n",
      "['always', 'will', 'eol', 'i', 'was', 'mesmerized', 'when', 'i', 'first', 'met']\n",
      "['do', 'what', 'you', 'want', 'just', 'seize', 'the', 'day', 'eol', 'what']\n",
      "['got', 'a', 'new', 'life', 'you', 'would', 'hardly', 'recognize', 'me', 'im']\n",
      "['want', 'to', 'leave', 'eol', 'i', 'wont', 'beg', 'you', 'to', 'stay']\n",
      "['could', 'right', 'the', 'wrongs', 'that', 'made', 'you', 'cry', 'eol', 'i']\n",
      "['rock', 'your', 'body', 'eol', 'everybody', 'eol', 'rock', 'your', 'body', 'right']\n",
      "['that', 'i', 'cant', 'live', 'without', 'you', 'eol', 'its', 'just', 'that']\n",
      "['know', 'youre', 'hurting', 'eol', 'right', 'now', 'you', 'feel', 'like', 'you']\n",
      "['run', 'and', 'hide', 'eol', 'when', 'youre', 'screamin', 'my', 'name', 'alright']\n",
      "['turn', 'out', 'the', 'lights', 'eol', 'the', 'two', 'of', 'us', 'alone']\n",
      "['heah', 'eol', 'you', 'are', 'my', 'fire', 'eol', 'the', 'one', 'desire']\n",
      "['as', 'you', 'love', 'eol', 'although', 'loneliness', 'has', 'always', 'been', 'a']\n",
      "['the', 'one', 'eol', 'i', 'guess', 'you', 'were', 'lost', 'when', 'i']\n",
      "['try', 'to', 'forgive', 'me', 'eol', 'stay', 'here', 'dont', 'put', 'out']\n",
      "['your', 'heart', 'to', 'me', 'eol', 'and', 'say', 'whats', 'on', 'your']\n",
      "['the', 'meaning', 'of', 'being', 'lonely', 'eol', 'so', 'many', 'words', 'for']\n",
      "['sometimes', 'being', 'eol', 'on', 'the', 'road', 'is', 'rough', 'especially', 'the']\n",
      "['eol', 'even', 'in', 'my', 'heart', 'i', 'see', 'eol', 'youre', 'not']\n",
      "['someone', 'writes', 'a', 'song', 'with', 'a', 'eol', 'simple', 'rhyme', 'touch']\n",
      "['that', 'we', 'should', 'be', 'together', 'eol', 'its', 'unbelievable', 'how', 'i']\n",
      "['remember', 'why', 'we', 'fell', 'apart', 'eol', 'from', 'something', 'that', 'was']\n",
      "['to', 'hurry', 'a', 'lot', 'i', 'used', 'to', 'worry', 'a', 'lot']\n",
      "['a', 'runnin', 'down', 'the', 'road', 'tryn', 'to', 'loosen', 'my', 'load']\n",
      "['on', 'the', 'street', 'it', 'sounds', 'so', 'familiar', 'eol', 'great', 'expectations']\n",
      "['the', 'shiny', 'night', 'eol', 'the', 'rain', 'was', 'softly', 'falling', 'eol']\n",
      "['heard', 'some', 'people', 'talking', 'just', 'the', 'other', 'day', 'eol', 'and']\n",
      "['these', 'nights', 'one', 'of', 'these', 'crazy', 'old', 'nights', 'eol', 'were']\n",
      "['tequila', 'sunrise', 'eol', 'starin', 'slowly', 'cross', 'the', 'sky', 'said', 'goodbye']\n",
      "['of', 'love', 'have', 'you', 'got', 'eol', 'you', 'should', 'be', 'home']\n",
      "['at', 'the', 'end', 'of', 'the', 'evening', 'eol', 'and', 'the', 'bright']\n",
      "['out', 'there', 'on', 'your', 'own', 'eol', 'where', 'your', 'memories', 'can']\n",
      "['you', 'come', 'to', 'your', 'senses', 'eol', 'you', 'been', 'out', 'ridin']\n",
      "['stars', 'eol', 'in', 'the', 'southern', 'sky', 'eol', 'southward', 'as', 'you']\n",
      "['eol', 'im', 'lying', 'in', 'bed', 'eol', 'holdin', 'you', 'close', 'in']\n",
      "['just', 'seem', 'to', 'find', 'out', 'early', 'eol', 'how', 'to', 'open']\n",
      "['dark', 'desert', 'highway', 'cool', 'wind', 'in', 'my', 'hair', 'eol', 'warm']\n",
      "['standing', 'eol', 'all', 'alone', 'against', 'the', 'world', 'outside', 'eol', 'you']\n",
      "['a', 'hard', 'headed', 'man', 'he', 'was', 'brutally', 'handsome', 'eol', 'and']\n",
      "['from', 'providence', 'the', 'one', 'in', 'rhode', 'island', 'eol', 'where', 'the']\n",
      "['be', 'ringing', 'this', 'sad', 'sad', 'new', 'years', 'eol', 'oh', 'what']\n",
      "['the', 'way', 'sparkling', 'earrings', 'lay', 'eol', 'against', 'your', 'skin', 'so']\n",
      "['there', 'you', 'stand', 'eol', 'with', 'your', 'little', 'head', 'down', 'in']\n",
      "['how', 'are', 'ya', 'eol', 'its', 'been', 'a', 'long', 'time', 'eol']\n",
      "['hurt', 'someone', 'before', 'the', 'night', 'is', 'through', 'eol', 'somebodys', 'gonna']\n",
      "['and', 'ruby', 'lips', 'eol', 'sparks', 'fly', 'from', 'her', 'finger', 'tips']\n",
      "['long', 'time', 'ago', 'eol', 'i', 'can', 'still', 'remember', 'how', 'eol']\n",
      "['starry', 'night', 'eol', 'paint', 'your', 'palette', 'blue', 'and', 'grey', 'eol']\n",
      "['floors', 'of', 'tokyo', 'eol', 'a', 'down', 'in', 'london', 'towns', 'a']\n",
      "['sister', 'what', 'have', 'you', 'done', 'eol', 'hey', 'little', 'sister', 'whos']\n",
      "['comes', 'now', 'sayin', 'mony', 'mony', 'eol', 'shoot', 'em', 'down', 'turn']\n",
      "['the', 'time', 'so', 'i', 'will', 'sing', 'yeah', 'eol', 'im', 'just']\n",
      "['a', 'little', 'dancer', 'came', 'dancin', 'to', 'my', 'door', 'eol', 'last']\n",
      "['change', 'in', 'pace', 'eol', 'of', 'fantasy', 'and', 'taste', 'eol', 'do']\n",
      "['out', 'of', 'hope', 'eol', 'one', 'more', 'bad', 'dream', 'could', 'bring']\n",
      "['stranger', 'stranger', 'eol', 'its', 'hot', 'here', 'at', 'night', 'lonely', 'black']\n",
      "['a', 'clown', 'eol', 'gary', 'lewis', 'and', 'the', 'playboys', 'eol', 'written']\n",
      "['well', 'now', 'eol', 'we', 'call', 'this', 'the', 'act', 'of', 'mating']\n",
      "['the', 'roof', 'the', 'roof', 'is', 'on', 'fire', 'eol', 'we', 'dont']\n",
      "['you', 'wrapped', 'up', 'in', 'her', 'satin', 'and', 'lace', 'eol', 'tied']\n",
      "['and', 'wanda', 'were', 'the', 'best', 'of', 'friends', 'eol', 'all', 'through']\n",
      "['know', 'what', 'youre', 'looking', 'for', 'eol', 'you', 'havent', 'found', 'it']\n",
      "['did', 'you', 'hear', 'me', 'say', 'eol', 'you', 'know', 'the', 'difference']\n",
      "['kung', 'fu', 'fighting', 'eol', 'those', 'kicks', 'were', 'fast', 'as', 'lightning']\n",
      "['you', 'hold', 'me', 'how', 'you', 'control', 'me', 'eol', 'you', 'bend']\n",
      "['what', 'they', 'tell', 'us', 'eol', 'no', 'matter', 'what', 'they', 'do']\n",
      "['is', 'all', 'that', 'you', 'cant', 'say', 'eol', 'years', 'gone', 'by']\n",
      "['a', 'tear', 'eol', 'you', 'wiped', 'it', 'dry', 'eol', 'i', 'was']\n",
      "['pinch', 'of', 'one', 'man', 'eol', 'wrap', 'him', 'up', 'in', 'black']\n",
      "['lost', 'and', 'alone', 'eol', 'trying', 'to', 'grow', 'making', 'my', 'way']\n",
      "['the', 'feel', 'of', 'your', 'name', 'on', 'my', 'lips', 'eol', 'and']\n",
      "['is', 'all', 'that', 'you', 'cant', 'say', 'eol', 'years', 'gone', 'by']\n",
      "['home', 'eol', 'the', 'thoughts', 'racing', 'through', 'my', 'crazy', 'mind', 'eol']\n",
      "['our', 'lives', 'we', 'all', 'have', 'pain', 'eol', 'we', 'all', 'have']\n",
      "['the', 'crystal', 'raindrops', 'fall', 'eol', 'and', 'the', 'beauty', 'of', 'it']\n",
      "['got', 'a', 'problem', 'i', 'dont', 'care', 'what', 'it', 'is', 'eol']\n",
      "['theres', 'got', 'to', 'be', 'a', 'better', 'way', 'eol', 'say', 'it']\n",
      "['when', 'we', 'used', 'to', 'sit', 'in', 'the', 'government', 'yard', 'in']\n",
      "['not', 'here', 'i', 'come', 'you', 'cant', 'hide', 'eol', 'gonna', 'find']\n",
      "['he', 'sang', 'a', 'good', 'song', 'i', 'heard', 'he', 'had', 'a']\n",
      "['on', 'a', 'sunday', 'morning', 'eol', 'outside', 'i', 'see', 'the', 'rain']\n",
      "['you', 'do', 'what', 'you', 'do', 'to', 'me', 'i', 'wish', 'i']\n",
      "['need', 'to', 'be', 'a', 'global', 'citizen', 'eol', 'because', 'im', 'blessed']\n",
      "['for', 'a', 'walk', 'eol', 'not', 'the', 'after', 'dinner', 'kind', 'eol']\n",
      "['sign', 'said', 'long', 'haired', 'freaky', 'people', 'need', 'not', 'apply', 'eol']\n",
      "['the', 'eyes', 'that', 'never', 'knew', 'how', 'to', 'smile', 'eol', 'till']\n",
      "['the', 'ground', 'eol', 'there', 'is', 'movement', 'all', 'around', 'eol', 'there']\n",
      "['goin', 'back', 'to', 'massachusetts', 'eol', 'somethings', 'telling', 'me', 'i', 'must']\n",
      "['light', 'eol', 'a', 'certain', 'kind', 'of', 'light', 'eol', 'that', 'never']\n",
      "['your', 'jive', 'talkin', 'eol', 'youre', 'telling', 'me', 'lies', 'yeah', 'eol']\n",
      "['long', 'eol', 'you', 'and', 'me', 'been', 'finding', 'each', 'other', 'for']\n",
      "['known', 'you', 'very', 'well', 'eol', 'ive', 'seen', 'you', 'growing', 'every']\n",
      "['lie', 'eol', 'in', 'a', 'lost', 'and', 'lonely', 'part', 'of', 'town']\n",
      "['can', 'tell', 'by', 'the', 'way', 'i', 'use', 'my', 'walk', 'eol']\n",
      "['think', 'of', 'younger', 'days', 'eol', 'when', 'living', 'for', 'my', 'life']\n",
      "['everlasting', 'smile', 'eol', 'a', 'smile', 'can', 'bring', 'you', 'near', 'to']\n",
      "['too', 'much', 'heaven', 'no', 'more', 'eol', 'its', 'much', 'harder', 'to']\n",
      "['your', 'eyes', 'in', 'the', 'morning', 'sun', 'eol', 'i', 'feel', 'you']\n",
      "['a', 'joke', 'which', 'started', 'the', 'whole', 'world', 'crying', 'eol', 'but']\n",
      "['are', 'eol', 'in', 'a', 'room', 'full', 'of', 'strangers', 'eol', 'standing']\n",
      "['cant', 'figure', 'it', 'out', 'eol', 'your', 'kisses', 'taste', 'like', 'honey']\n",
      "['at', 'daddys', 'baby', 'girl', 'eol', 'thats', 'daddy', 'baby', 'eol', 'little']\n",
      "['name', 'is', 'what', 'eol', 'my', 'name', 'is', 'who', 'eol', 'my']\n",
      "['dre', 'just', 'let', 'it', 'run', 'eol', 'ey', 'yo', 'turn', 'the']\n",
      "['you', 'aint', 'familiar', 'with', 'these', 'here', 'parts', 'eol', 'you', 'know']\n",
      "['have', 'your', 'attention', 'please', 'eol', 'may', 'i', 'have', 'your', 'attention']\n",
      "['i', 'am', 'going', 'to', 'attempt', 'to', 'drown', 'myself', 'eol', 'you']\n",
      "['gone', 'cold', 'im', 'wondering', 'why', 'i', 'eol', 'got', 'out', 'of']\n",
      "['can', 'suck', 'my', 'dick', 'if', 'you', 'dont', 'like', 'my', 'shit']\n",
      "['twenty', 'three', 'years', 'old', 'eol', 'fed', 'up', 'with', 'life', 'and']\n",
      "['you', 'high', 'baby', 'eol', 'yeah', 'eol', 'yeah', 'eol', 'ha', 'ha']\n",
      "['get', 'home', 'babe', 'gonna', 'light', 'your', 'fire', 'eol', 'all', 'day']\n",
      "['stop', 'this', 'feelin', 'eol', 'deep', 'inside', 'of', 'me', 'eol', 'girl']\n",
      "['rocks', 'in', 'the', 'hot', 'sun', 'eol', 'i', 'fought', 'the', 'law']\n",
      "['an', 'obsession', 'eol', 'i', 'cannot', 'sleep', 'eol', 'i', 'am', 'your']\n",
      "['the', 'days', 'were', 'long', 'eol', 'and', 'rolled', 'beneath', 'a', 'deep']\n",
      "['my', 'living', 'off', 'the', 'evening', 'news', 'eol', 'just', 'give', 'me']\n",
      "['up', 'eol', 'dressed', 'all', 'in', 'black', 'eol', 'went', 'down', 'to']\n",
      "['the', 'basement', 'eol', 'mixing', 'up', 'the', 'medicine', 'eol', 'im', 'on']\n",
      "['her', 'sunday', 'that', 'was', 'yesterday', 'eol', 'the', 'girl', 'i', 'knew']\n",
      "['me', 'still', 'the', 'same', 'og', 'but', 'i', 'been', 'low', 'key']\n",
      "Dataset has: 176046 sequences and 176046 targets\n",
      "['jean', 'eol', 'though', 'i', 'never', 'knew', 'you', 'at', 'all', 'eol']\n",
      "['way', 'down', 'on', 'baker', 'street', 'eol', 'lite', 'in', 'your', 'head']\n",
      "['i', 'need', 'your', 'love', 'eol', 'youve', 'got', 'that', 'hold', 'over']\n",
      "['check', 'it', 'out', 'yall', 'eol', 'come', 'on', 'come', 'on', 'eol']\n",
      "['beat', 'control', 'your', 'body', 'eol', 'let', 'the', 'beat', 'control', 'your']\n",
      "['for', 'this', 'eol', 'get', 'down', 'with', 'the', 'style', 'eol', 'house']\n",
      "['the', 'bass', 'into', 'the', 'jam', 'eol', 'then', 'let', 'the', 'music']\n",
      "['ive', 'lost', 'everything', 'to', 'you', 'eol', 'you', 'say', 'you', 'want']\n",
      "['oh', 'im', 'bein', 'followed', 'by', 'a', 'moonshadow', 'moon', 'shadow', 'moonshadow']\n",
      "['broken', 'like', 'the', 'first', 'morning', 'eol', 'blackbird', 'has', 'spoken', 'like']\n",
      "['another', 'saturday', 'night', 'and', 'i', 'aint', 'got', 'nobody', 'eol', 'ive']\n",
      "['been', 'happy', 'lately', 'eol', 'thinking', 'about', 'the', 'good', 'things', 'to']\n",
      "['time', 'to', 'make', 'a', 'change', 'eol', 'just', 'relax', 'take', 'it']\n",
      "['young', 'what', 'will', 'you', 'leave', 'us', 'this', 'time', 'eol', 'youre']\n",
      "['me', 'eol', 'i', 'was', 'wondering', 'if', 'after', 'all', 'these', 'years']\n",
      "['walking', 'in', 'the', 'same', 'way', 'as', 'i', 'did', 'eol', 'missing']\n",
      "['that', 'youre', 'settled', 'down', 'eol', 'that', 'you', 'found', 'a', 'girl']\n",
      "['yourself', 'in', 'stupid', 'places', 'eol', 'yes', 'i', 'think', 'you', 'know']\n",
      "['the', 'money', 'that', 'i', 'owe', 'you', 'eol', 'yes', 'you', 'can']\n",
      "['still', 'living', 'with', 'your', 'ghost', 'eol', 'lonely', 'and', 'dreaming', 'of']\n",
      "['mine', 'eol', 'tell', 'me', 'where', 'have', 'you', 'been', 'eol', 'you']\n",
      "['him', 'on', 'a', 'monday', 'and', 'my', 'heart', 'stood', 'still', 'eol']\n",
      "['successful', 'fella', 'thought', 'to', 'himself', 'eol', 'oops', 'ive', 'got', 'a']\n",
      "['the', 'night', 'eol', 'lying', 'by', 'your', 'side', 'eol', 'tender', 'is']\n",
      "['eol', 'woo', 'hoo', 'eol', 'woo', 'hoo', 'eol', 'woo', 'hoo', 'eol']\n",
      "['him', 'in', 'a', 'crowded', 'room', 'eol', 'where', 'people', 'go', 'to']\n",
      "['the', 'neon', 'lights', 'are', 'bright', 'on', 'broadway', 'eol', 'they', 'say']\n",
      "['love', 'around', 'eol', 'george', 'benson', 'eol', 'youve', 'got', 'thelove', 'eol']\n",
      "['really', 'happy', 'here', 'eol', 'with', 'this', 'lonely', 'game', 'we', 'play']\n",
      "['has', 'fallen', 'eol', 'you', 'know', 'the', 'spirit', 'of', 'the', 'party']\n",
      "['gonna', 'be', 'around', 'eol', 'hey', 'were', 'gonna', 'work', 'it', 'out']\n",
      "['were', 'to', 'say', 'to', 'you', 'eol', 'can', 'you', 'keep', 'a']\n",
      "['world', 'is', 'full', 'of', 'strange', 'arrangements', 'eol', 'and', 'gravity', 'wont']\n",
      "['down', 'i', 'call', 'on', 'you', 'my', 'friend', 'eol', 'a', 'helping']\n",
      "['like', 'to', 'get', 'to', 'know', 'if', 'i', 'could', 'be', 'eol']\n",
      "['is', 'mine', 'the', 'boy', 'is', 'mine', 'eol', 'the', 'boy', 'is']\n",
      "['cake', 'eol', 'give', 'me', 'a', 'little', 'piece', 'let', 'me', 'lick']\n",
      "['the', 'pieces', 'uh', 'huh', 'eol', 'pick', 'up', 'the', 'pieces', 'alright']\n",
      "['with', 'tact', 'just', 'what', 'are', 'you', 'trying', 'to', 'say', 'eol']\n",
      "['pulled', 'out', 'of', 'pittsburgh', 'headig', 'down', 'that', 'eastern', 'seaboard', 'eol']\n",
      "['gettin', 'down', 'baby', 'eol', 'i', 'want', 'it', 'now', 'baby', 'eol']\n",
      "['boys', 'with', 'the', 'power', 'to', 'rock', 'you', 'eol', 'blowing', 'your']\n",
      "['time', 'is', 'through', 'eol', 'now', 'and', 'forever', 'eol', 'until', 'the']\n",
      "['get', 'up', 'in', 'the', 'mornin', 'eol', 'i', 'believe', 'ill', 'dust']\n",
      "['old', 'paintings', 'on', 'the', 'tombs', 'eol', 'they', 'do', 'the', 'sand']\n",
      "['time', 'eol', 'see', 'whats', 'become', 'of', 'me', 'eol', 'time', 'time']\n",
      "['already', 'eol', 'i', 'was', 'just', 'in', 'the', 'middle', 'of', 'a']\n",
      "['walk', 'on', 'the', 'water', 'with', 'i', 'you', 'and', 'i', 'eol']\n",
      "['the', 'witch', 'doctor', 'eol', 'i', 'was', 'in', 'love', 'with', 'you']\n",
      "['a', 'luck', 'out', 'a', 'love', 'eol', 'gotta', 'photograph', 'picture', 'of']\n",
      "['tell', 'the', 'world', 'you', 'never', 'was', 'my', 'girl', 'eol', 'you']\n",
      "['know', 'where', 'youre', 'going', 'to', 'eol', 'do', 'you', 'like', 'the']\n",
      "['upside', 'down', 'eol', 'youre', 'turning', 'me', 'eol', 'youre', 'giving', 'love']\n",
      "['in', 'the', 'morning', 'eol', 'then', 'just', 'walk', 'away', 'eol', 'we']\n",
      "['me', 'eol', 'you', 'may', 'think', 'you', 'see', 'eol', 'who', 'i']\n",
      "['like', 'ive', 'been', 'locked', 'up', 'tight', 'eol', 'for', 'a', 'century']\n",
      "['friends', 'are', 'gonna', 'party', 'all', 'night', 'long', 'eol', 'cmon', 'over']\n",
      "['lost', 'in', 'the', 'rain', 'in', 'your', 'eyes', 'eol', 'i', 'know']\n",
      "['bad', 'tasting', 'eol', 'full', 'bodied', 'butt', 'wasting', 'eol', 'loose', 'living']\n",
      "['el', 'hombre', 'eol', 'con', 'fuego', 'en', 'la', 'sangre', 'eol', 'ive']\n",
      "['child', 'and', 'dont', 'you', 'cry', 'eol', 'your', 'folks', 'might', 'understand']\n",
      "['line', 'marking', 'time', 'eol', 'waiting', 'for', 'the', 'welfare', 'dime', 'eol']\n",
      "['time', 'to', 'realize', 'my', 'crime', 'eol', 'let', 'me', 'love', 'and']\n",
      "['in', 'your', 'eyes', 'all', 'the', 'way', 'eol', 'if', 'i', 'listen']\n",
      "['in', 'your', 'eyes', 'eol', 'you', 'used', 'and', 'made', 'my', 'life']\n",
      "['talking', 'eol', 'a', 'lot', 'of', 'a', 'sharks', 'out', 'there', 'tryna']\n",
      "['can', 'show', 'you', 'the', 'world', 'eol', 'shining', 'shimmering', 'splendid', 'eol']\n",
      "['time', 'i', 'get', 'to', 'phoenix', 'shell', 'be', 'rising', 'eol', 'shell']\n",
      "['a', 'lineman', 'for', 'the', 'county', 'eol', 'and', 'i', 'drive', 'the']\n",
      "['eol', 'my', 'tears', 'have', 'stopped', 'falling', 'eol', 'the', 'long', 'lonely']\n",
      "['a', 'long', 'time', 'long', 'time', 'we', 'shouldnt', 'of', 'left', 'you']\n",
      "['eol', 'can', 'yall', 'really', 'feel', 'me', 'feel', 'this', 'eol', 'east']\n",
      "['n', 'harmony', 'eol', 'bone', 'xalot', 'eol', 'wasteland', 'soldier', 'these', 'are']\n",
      "['ruggish', 'niggas', 'always', 'always', 'eol', 'and', 'ready', 'to', 'bring', 'the']\n",
      "['gotta', 'let', 'me', 'know', 'eol', 'should', 'i', 'stay', 'or', 'should']\n",
      "['king', 'told', 'the', 'boogie', 'men', 'eol', 'you', 'have', 'to', 'let']\n",
      "['seen', 'places', 'and', 'faces', 'eol', 'and', 'things', 'you', 'aint', 'never']\n",
      "['up', 'cause', 'im', 'about', 'to', 'get', 'my', 'speak', 'on', 'eol']\n",
      "['yall', 'lets', 'take', 'a', 'ride', 'eol', 'dont', 'you', 'say', 'shit']\n",
      "['those', 'times', 'you', 'stood', 'by', 'me', 'eol', 'for', 'all', 'the']\n",
      "['is', 'who', 'i', 'am', 'eol', 'and', 'this', 'is', 'all', 'i']\n",
      "['back', 'in', 'the', 'arms', 'i', 'love', 'eol', 'need', 'me', 'like']\n",
      "['my', 'mood', 'in', 'shades', 'of', 'blue', 'eol', 'paint', 'my', 'soul']\n",
      "['so', 'afraid', 'to', 'show', 'i', 'care', 'eol', 'will', 'he', 'think']\n",
      "['your', 'eyes', 'i', 'see', 'ribbons', 'of', 'color', 'eol', 'i', 'see']\n",
      "['was', 'young', 'eol', 'i', 'never', 'needed', 'anyone', 'eol', 'and', 'makin']\n",
      "['gettin', 'ready', 'to', 'put', 'yall', 'up', 'on', 'somethin', 'man', 'whats']\n",
      "['lick', 'you', 'up', 'and', 'down', 'till', 'you', 'say', 'stop', 'eol']\n",
      "['i', 'want', 'you', 'for', 'myself', 'eol', 'i', 'dont', 'want', 'nobody']\n",
      "['eol', 'put', 'on', 'your', 'red', 'shoes', 'and', 'dance', 'the', 'blues']\n",
      "['up', 'their', 'minds', 'and', 'they', 'started', 'packing', 'eol', 'they', 'left']\n",
      "['feel', 'like', 'im', 'drunk', 'behind', 'the', 'wheel', 'eol', 'the', 'wheel']\n",
      "['streets', 'eol', 'and', 'the', 'pavements', 'are', 'burning', 'eol', 'i', 'sit']\n",
      "['the', 'mountain', 'top', 'eol', 'burning', 'like', 'a', 'silver', 'flame', 'eol']\n",
      "['day', 'after', 'christmas', 'eol', 'i', 'throw', 'some', 'clothes', 'on', 'in']\n",
      "['eol', 'talking', 'about', 'the', 'sad', 'girls', 'eol', 'sad', 'girls', 'eol']\n",
      "['eol', 'last', 'chance', 'for', 'love', 'eol', 'yes', 'its', 'my', 'last']\n",
      "['eatin', 'my', 'heart', 'out', 'waitin', 'eol', 'waitin', 'for', 'some', 'lover']\n",
      "['so', 'good', 'its', 'so', 'good', 'eol', 'its', 'so', 'good', 'its']\n",
      "['a', 'letter', 'you', 'wrote', 'me', 'on', 'the', 'radio', 'eol', 'and']\n",
      "['me', 'if', 'i', 'love', 'you', 'eol', 'and', 'i', 'choke', 'on']\n",
      "['hours', 'for', 'this', 'eol', 'ive', 'made', 'myself', 'so', 'sick', 'eol']\n",
      "['eol', 'woman', 'take', 'me', 'in', 'your', 'arms', 'eol', 'rock', 'your']\n",
      "['la', 'la', 'la', 'la', 'la', 'eol', 'mmm', 'eol', 'uh', 'huh']\n",
      "['the', 'front', 'door', 'like', 'a', 'ghost', 'eol', 'into', 'the', 'fog']\n",
      "['to', 'escape', 'eol', 'the', 'city', 'was', 'sticky', 'and', 'cruel', 'eol']\n",
      "['home', 'in', 'the', 'morning', 'light', 'eol', 'my', 'mother', 'says', 'when']\n",
      "['my', 'bed', 'i', 'hear', 'the', 'clock', 'tick', 'eol', 'and', 'think']\n",
      "['see', 'them', 'every', 'night', 'in', 'tight', 'blue', 'jeans', 'eol', 'in']\n",
      "['the', 'night', 'eol', 'ill', 'be', 'awake', 'and', 'ill', 'be', 'with']\n",
      "['the', 'sad', 'eyes', 'eol', 'dont', 'be', 'discouraged', 'eol', 'oh', 'i']\n",
      "['are', 'eol', 'hanging', 'onto', 'strains', 'of', 'greed', 'and', 'blues', 'eol']\n",
      "['turn', 'turn', 'turn', 'eol', 'there', 'is', 'a', 'season', 'turn', 'turn']\n",
      "['stand', 'it', 'i', 'know', 'you', 'planned', 'it', 'eol', 'but', 'im']\n",
      "['eol', 'you', 'wake', 'up', 'late', 'for', 'school', 'man', 'you', 'dont']\n",
      "['planetary', 'intergalactic', 'eol', 'intergalactic', 'planetary', 'planetary', 'intergalactic', 'eol', 'intergalactic', 'planetary']\n",
      "['blues', 'you', 'made', 'me', 'cry', 'eol', 'i', 'dont', 'want', 'to']\n",
      "['in', 'the', 'evening', 'shes', 'wondering', 'what', 'clothes', 'to', 'wear', 'eol']\n",
      "['youve', 'gone', 'eol', 'all', 'thats', 'left', 'is', 'a', 'band', 'of']\n",
      "['do', 'you', 'know', 'what', 'thats', 'worth', 'eol', 'ooh', 'heaven', 'is']\n",
      "['feel', 'it', 'see', 'it', 'hear', 'it', 'today', 'eol', 'if', 'you']\n",
      "['a', 'tear', 'you', 'wiped', 'it', 'dry', 'eol', 'i', 'was', 'confused']\n",
      "['snowy', 'mantle', 'cold', 'and', 'clean', 'eol', 'the', 'unborn', 'grass', 'lies']\n",
      "['each', 'other', 'eol', 'trying', 'so', 'hard', 'to', 'stay', 'warm', 'eol']\n",
      "['oh', 'eol', 'for', 'the', 'longest', 'time', 'eol', 'oh', 'oh', 'oh']\n",
      "['oclock', 'on', 'a', 'saturday', 'eol', 'the', 'regular', 'crowd', 'shuffles', 'in']\n",
      "['virginia', 'dont', 'let', 'em', 'wait', 'eol', 'you', 'catholic', 'girls', 'start']\n",
      "['eol', 'i', 'dont', 'want', 'to', 'see', 'you', 'let', 'a', 'good']\n",
      "['like', 'to', 'get', 'away', 'eol', 'take', 'a', 'holiday', 'from', 'the']\n",
      "['eol', 'shes', 'been', 'living', 'in', 'her', 'uptown', 'world', 'eol', 'i']\n",
      "['all', 'have', 'a', 'face', 'eol', 'that', 'we', 'hide', 'away', 'forever']\n",
      "['of', 'white', 'a', 'bottle', 'of', 'red', 'eol', 'perhaps', 'a', 'bottle']\n",
      "['i', 'crashed', 'your', 'party', 'eol', 'saturday', 'i', 'said', 'im', 'sorry']\n",
      "['matter', 'with', 'the', 'clothes', 'im', 'wearing', 'eol', 'cant', 'you', 'tell']\n",
      "['went', 'uptown', 'riding', 'in', 'your', 'limousine', 'eol', 'with', 'your', 'fine']\n",
      "['living', 'here', 'in', 'allentown', 'eol', 'and', 'theyre', 'closing', 'all', 'the']\n",
      "['middle', 'of', 'the', 'night', 'eol', 'i', 'go', 'walking', 'in', 'my']\n",
      "['as', 'soulmates', 'eol', 'on', 'parris', 'inland', 'eol', 'we', 'left', 'as']\n",
      "['changing', 'to', 'try', 'and', 'please', 'me', 'eol', 'you', 'never', 'let']\n",
      "['a', 'way', 'about', 'her', 'eol', 'i', 'dont', 'know', 'what', 'it']\n",
      "['call', 'from', 'an', 'old', 'friend', 'wed', 'used', 'to', 'be', 'real']\n",
      "['onto', 'the', 'milkmans', 'hand', 'eol', 'and', 'then', 'she', 'finally', 'gave']\n",
      "['but', 'the', 'world', 'keeps', 'spinning', 'eol', 'take', 'a', 'spin', 'through']\n",
      "['of', 'an', 'ancient', 'radiation', 'eol', 'that', 'haunts', 'dismembered', 'constellations', 'eol']\n",
      "['at', 'the', 'starting', 'line', 'eol', 'engines', 'pumping', 'and', 'thumping', 'in']\n",
      "['your', 'arms', 'around', 'me', 'eol', 'i', 'need', 'to', 'feel', 'your']\n",
      "['born', 'on', 'a', 'summer', 'day', '1951', 'eol', 'and', 'with', 'a']\n",
      "['all', 'on', 'my', 'roots', 'eol', 'i', 'showed', 'up', 'in', 'boots']\n",
      "['feelin', 'the', 'blues', 'eol', 'i', 'was', 'watching', 'the', 'news', 'eol']\n",
      "['i', 'say', 'eol', 'what', 'can', 'i', 'do', 'eol', 'three', 'am']\n",
      "['the', 'boat', 'that', 'day', 'eol', 'he', 'left', 'the', 'shack', 'eol']\n",
      "['life', 'youve', 'waited', 'for', 'love', 'to', 'eol', 'come', 'and', 'stay']\n",
      "['eol', 'its', 'love', 'eol', 'love', 'get', 'busy', 'eol', 'everybodys', 'talkin']\n",
      "['havent', 'seen', 'you', 'in', 'a', 'while', 'eol', 'howve', 'you', 'been']\n",
      "['stop', 'the', 'way', 'i', 'feel', 'eol', 'things', 'you', 'do', 'dont']\n",
      "['my', 'love', 'has', 'come', 'along', 'eol', 'my', 'lonely', 'days', 'are']\n",
      "['not', 'far', 'down', 'to', 'paradise', 'at', 'least', 'its', 'not', 'for']\n",
      "['the', 'night', 'eol', 'my', 'bodys', 'weak', 'eol', 'im', 'on', 'the']\n",
      "['in', 'the', 'saddle', 'again', 'eol', 'out', 'where', 'a', 'friend', 'is']\n",
      "['chaka', 'chaka', 'khan', 'eol', 'chaka', 'khan', 'chaka', 'khan', 'chaka', 'khan']\n",
      "['eol', 'thats', 'the', 'way', 'it', 'was', 'eol', 'happened', 'so', 'naturally']\n",
      "['got', 'no', 'kind', 'of', 'feeling', 'inside', 'eol', 'i', 'got', 'something']\n",
      "['baby', 'lets', 'do', 'the', 'twist', 'eol', 'come', 'on', 'baby', 'lets']\n",
      "['the', 'old', 'apartment', 'eol', 'this', 'is', 'where', 'we', 'used', 'to']\n",
      "['in', 'the', 'door', 'a', 'step', 'on', 'the', 'floor', 'eol', 'a']\n",
      "['one', 'week', 'since', 'you', 'looked', 'at', 'me', 'eol', 'cocked', 'your']\n",
      "['be', 'loved', 'and', 'be', 'loved', 'eol', 'could', 'you', 'be', 'loved']\n",
      "['a', 'song', 'eol', 'sing', 'out', 'loud', 'eol', 'sing', 'out', 'strong']\n",
      "['at', 'the', 'two', 'of', 'us', 'eol', 'strangers', 'in', 'many', 'ways']\n",
      "['in', 'the', 'world', 'eol', 'ever', 'had', 'a', 'love', 'as', 'sweet']\n",
      "['in', 'my', 'heart', 'eol', 'from', 'early', 'in', 'the', 'mornin', 'til']\n",
      "['feelins', 'comin', 'over', 'me', 'eol', 'there', 'is', 'wonder', 'in', 'most']\n",
      "['enough', 'of', 'being', 'alone', 'eol', 'everyone', 'must', 'face', 'their', 'share']\n",
      "['so', 'many', 'places', 'in', 'my', 'life', 'and', 'time', 'eol', 'ive']\n",
      "['was', 'young', 'id', 'listen', 'to', 'the', 'radio', 'eol', 'waitin', 'for']\n",
      "['have', 'all', 'been', 'sent', 'eol', 'the', 'christmas', 'rush', 'is', 'through']\n",
      "['and', 'oh', 'so', 'far', 'away', 'eol', 'i', 'fell', 'in', 'love']\n",
      "['goodbye', 'to', 'love', 'eol', 'no', 'one', 'ever', 'cared', 'if', 'i']\n",
      "['myself', 'and', 'feelin', 'old', 'eol', 'sometimes', 'id', 'like', 'to', 'quit']\n",
      "['birds', 'suddenly', 'appear', 'eol', 'every', 'time', 'you', 'are', 'near', 'eol']\n",
      "['by', 'the', 'moon', 'and', 'the', 'stars', 'in', 'the', 'skies', 'eol']\n",
      "['by', 'the', 'moon', 'and', 'the', 'stars', 'in', 'the', 'skies', 'eol']\n",
      "['to', 'play', 'a', 'game', 'eol', 'that', 'is', 'so', 'much', 'fun']\n",
      "['old', 'lover', 'in', 'the', 'grocery', 'store', 'eol', 'the', 'snow', 'was']\n",
      "['place', 'for', 'me', 'to', 'hide', 'eol', 'the', 'thoughts', 'of', 'all']\n",
      "['bop', 'a', 'lula', 'shes', 'my', 'baby', 'eol', 'be', 'bop', 'a']\n",
      "['to', 'say', 'it', 'but', 'i', 'told', 'you', 'so', 'dont', 'mind']\n",
      "['watchin', 'you', 'for', 'days', 'now', 'baby', 'eol', 'i', 'just', 'love']\n",
      "['look', 'at', 'the', 'letters', 'that', 'you', 'wrote', 'to', 'me', 'eol']\n",
      "['my', 'heart', 'lies', 'a', 'melody', 'eol', 'a', 'song', 'of', 'old']\n",
      "['four', 'eol', 'eins', 'zwei', 'drei', 'eol', 'na', 'es', 'is', 'nix']\n",
      "['all', 'the', 'time', 'to', 'the', 'top', 'eol', 'er', 'war', 'ein']\n",
      "['happening', 'now', 'eol', 'hey', 'hey', 'whoa', 'oh', 'eol', 'hey', 'hey']\n",
      "['up', 'heres', 'a', 'story', 'eol', 'about', 'a', 'little', 'guy', 'that']\n",
      "['wanna', 'be', 'your', 'clown', 'again', 'eol', 'and', 'i', 'dont', 'wanna']\n",
      "['too', 'much', 'of', 'heaven', 'can', 'bring', 'you', 'underground', 'eol', 'heaven']\n",
      "['its', 'another', 'race', 'from', 'outer', 'space', 'eol', 'its', 'another', 'race']\n",
      "['is', 'looking', 'for', 'the', 'dub', 'in', 'this', 'life', 'eol', 'everyone']\n",
      "['i', 'was', 'afraid', 'i', 'was', 'petrified', 'eol', 'kept', 'thinking', 'i']\n",
      "['can', 'say', 'goodbye', 'no', 'no', 'no', 'no', 'now', 'eol', 'never']\n",
      "['was', 'a', 'preachers', 'son', 'eol', 'and', 'when', 'his', 'daddy', 'would']\n",
      "['know', 'what', 'it', 'is', 'that', 'makes', 'me', 'love', 'you', 'so']\n",
      "['questions', 'that', 'i', 'need', 'to', 'know', 'eol', 'how', 'you', 'could']\n",
      "['on', 'bring', 'it', 'bring', 'it', 'on', 'now', 'eol', 'bring', 'it']\n",
      "['go', 'sister', 'soul', 'sister', 'go', 'sister', 'eol', 'hey', 'sister', 'go']\n",
      "['fi', 'fi', 'fo', 'fo', 'fum', 'eol', 'i', 'smell', 'smoke', 'in']\n",
      "['the', 'papers', 'and', 'the', 'trash', 'eol', 'or', 'you', 'dont', 'get']\n",
      "['my', 'sunglasses', 'at', 'night', 'eol', 'so', 'i', 'can', 'so', 'i']\n",
      "['of', 'my', 'mind', 'eol', 'going', 'out', 'of', 'my', 'mind', 'eol']\n",
      "['right', 'now', 'eol', 'right', 'here', 'right', 'now', 'eol', 'right', 'here']\n",
      "['now', 'the', 'funk', 'soul', 'brother', 'eol', 'check', 'it', 'out', 'now']\n",
      "['a', 'long', 'long', 'way', 'together', 'eol', 'through', 'the', 'hard', 'times']\n",
      "['eol', 'shamari', 'eol', 'there', 'you', 'are', 'eol', 'looking', 'as', 'fine']\n",
      "['when', 'theres', 'someone', 'else', 'who', 'cares', 'eol', 'when', 'theres', 'someone']\n",
      "['was', 'lola', 'she', 'was', 'a', 'showgirl', 'eol', 'with', 'yellow', 'feathers']\n",
      "['all', 'my', 'life', 'eol', 'raining', 'down', 'as', 'cold', 'as', 'ice']\n",
      "['night', 'has', 'come', 'eol', 'and', 'the', 'land', 'is', 'dark', 'eol']\n",
      "['a', 'song', 'that', 'aint', 'no', 'melody', 'im', 'gonna', 'sing', 'it']\n",
      "['there', 'i', 'want', 'to', 'go', 'there', 'eol', 'take', 'me', 'there']\n",
      "['you', 'know', 'what', 'eol', 'i', 'like', 'the', 'playettes', 'eol', 'no']\n",
      "['motion', 'eol', 'in', 'my', 'foolish', 'lovers', 'game', 'eol', 'on', 'this']\n",
      "['eol', 'berlin', 'eol', 'im', 'alone', 'eol', 'sitting', 'with', 'my', 'empty']\n",
      "['was', 'young', 'eol', 'i', 'never', 'needed', 'anyone', 'eol', 'and', 'makin']\n",
      "['meaning', 'to', 'tell', 'you', 'eol', 'ive', 'got', 'this', 'feelin', 'that']\n",
      "['bailamos', 'eol', 'te', 'doy', 'toda', 'mi', 'vida', 'eol', 'quã©date', 'conmigo']\n",
      "['de', 'ti', 'para', 'sobrevivir', 'eol', 'esta', 'noche', 'que', 'viene', 'fria']\n",
      "['i', 'feel', 'so', 'low', 'eol', 'i', 'count', 'the', 'hours', 'they']\n",
      "['called', 'it', 'puppy', 'love', 'eol', 'oh', 'i', 'guess', 'theyll', 'never']\n",
      "['the', 'middle', 'of', 'a', 'dry', 'spell', 'eol', 'jimmy', 'rodgers', 'on']\n",
      "['a', 'moment', 'in', 'space', 'eol', 'when', 'the', 'dream', 'is', 'gone']\n",
      "['the', 'corners', 'of', 'my', 'mind', 'eol', 'misty', 'water', 'colored', 'memries']\n",
      "['whisperin', 'in', 'the', 'trees', 'eol', 'its', 'two', 'sailors', 'and', 'theyre']\n",
      "['my', 'church', 'eol', 'this', 'is', 'where', 'i', 'heal', 'my', 'hurts']\n",
      "['submitted', 'those', 'lyrics', 'must', 'have', 'been', 'deaf', 'lolhere', 'are', 'the']\n",
      "['my', 'heart', 'i', 'love', 'you', 'baby', 'eol', 'stay', 'with', 'me']\n",
      "['standin', 'at', 'the', 'station', 'eol', 'ten', 'to', 'midnight', 'in', 'the']\n",
      "['lost', 'in', 'your', 'eyes', 'eol', 'and', 'i', 'feel', 'my', 'spirits']\n",
      "['there', 'was', 'a', 'time', 'when', 'eol', 'broken', 'hearts', 'and', 'broken']\n",
      "['up', 'the', 'country', 'baby', 'dont', 'you', 'want', 'to', 'go', 'eol']\n",
      "['me', 'youre', 'in', 'love', 'with', 'me', 'eol', 'like', 'you', 'cant']\n",
      "['baby', 'eol', 'oh', 'baby', 'baby', 'eol', 'oh', 'baby', 'baby', 'how']\n",
      "['a', 'story', 'about', 'a', 'girl', 'named', 'lucky', 'eol', 'early', 'morning']\n",
      "['yeah', 'eol', 'hush', 'just', 'stop', 'eol', 'theres', 'nothing', 'you', 'can']\n",
      "['eol', 'ah', 'here', 'we', 'go', 'now', 'oh', 'eol', 'like', 'a']\n",
      "['life', 'oh', 'yeah', 'yeah', 'eol', 'oh', 'yeah', 'eol', 'im', 'sitting']\n",
      "['back', 'we', 'said', 'eol', 'how', 'was', 'i', 'to', 'know', 'id']\n",
      "['say', 'youre', 'so', 'into', 'me', 'eol', 'and', 'that', 'you', 'need']\n",
      "['so', 'into', 'you', 'eol', 'youve', 'got', 'that', 'something', 'what', 'can']\n",
      "['good', 'eol', 'aint', 'it', 'right', 'eol', 'that', 'you', 'are', 'with']\n",
      "['ever', 'told', 'you', 'eol', 'how', 'good', 'it', 'feels', 'to', 'hold']\n",
      "['i', 'wanted', 'eol', 'something', 'special', 'something', 'sacred', 'eol', 'in', 'your']\n",
      "['hear', 'a', 'different', 'story', 'eol', 'people', 'saying', 'that', 'youre', 'no']\n",
      "['enough', 'of', 'danger', 'eol', 'and', 'people', 'on', 'the', 'streets', 'eol']\n",
      "['guess', 'it', 'would', 'be', 'nice', 'eol', 'if', 'i', 'could', 'touch']\n",
      "['your', 'eyes', 'on', 'the', 'road', 'your', 'hands', 'upon', 'the', 'wheel']\n",
      "['a', 'bad', 'bad', 'girl', 'eol', 'ive', 'been', 'careless', 'with', 'a']\n",
      "['eol', 'we', 'have', 'no', 'secrets', 'eol', 'we', 'tell', 'each', 'other']\n",
      "['new', 'kind', 'of', 'dancing', 'eol', 'thats', 'going', 'to', 'be', 'the']\n",
      "['wed', 'light', 'the', 'firmament', 'eol', 'you', 'said', 'our', 'love', 'was']\n",
      "['it', 'better', 'makes', 'me', 'feel', 'sad', 'for', 'the', 'rest', 'eol']\n",
      "['you', 'heard', 'eol', 'hes', 'gonna', 'buy', 'me', 'a', 'mockingbird', 'eol']\n",
      "['oh', 'venus', 'eol', 'venus', 'if', 'you', 'will', 'eol', 'please', 'send']\n",
      "['bought', 'his', 'babe', 'a', 'diamond', 'ring', 'eol', 'if', 'that', 'diamond']\n",
      "['want', 'it', 'here', 'it', 'is', 'come', 'and', 'get', 'it', 'eol']\n",
      "['what', 'you', 'are', 'eol', 'i', 'will', 'always', 'be', 'with', 'you']\n",
      "['finding', 'out', 'about', 'you', 'eol', 'every', 'day', 'my', 'mind', 'is']\n",
      "['got', 'what', 'i', 'deserve', 'eol', 'kept', 'you', 'waiting', 'there', 'too']\n",
      "['feel', 'the', 'magic', 'floating', 'in', 'the', 'air', 'eol', 'being', 'with']\n",
      "['eol', 'never', 'turn', 'you', 'down', 'eol', 'when', 'all', 'the', 'others']\n",
      "['to', 'think', 'that', 'love', 'was', 'just', 'a', 'fairy', 'tale', 'eol']\n",
      "['know', 'what', 'youve', 'got', 'but', 'it', 'plays', 'with', 'my', 'emotions']\n",
      "['people', 'so', 'why', 'should', 'it', 'be', 'eol', 'you', 'and', 'i']\n",
      "['eol', 'seals', 'the', 'contract', 'eol', 'from', 'the', 'contracts', 'eol', 'theres']\n",
      "['new', 'game', 'eol', 'we', 'like', 'to', 'play', 'you', 'see', 'eol']\n",
      "['learnin', 'eol', 'that', 'so', 'many', 'yearnings', 'eol', 'are', 'never', 'to']\n",
      "['knew', 'thered', 'come', 'a', 'day', 'eol', 'when', 'id', 'be', 'sayin']\n",
      "['love', 'me', 'i', 'want', 'to', 'know', 'eol', 'how', 'can', 'i']\n",
      "['how', 'hard', 'i', 'try', 'eol', 'you', 'keep', 'pushing', 'me', 'aside']\n",
      "['wonder', 'eol', 'how', 'id', 'ever', 'make', 'it', 'through', 'eol', 'through']\n",
      "['on', 'the', 'morning', 'rain', 'eol', 'i', 'used', 'to', 'feel', 'so']\n",
      "['want', 'eol', 'baby', 'i', 'got', 'it', 'eol', 'what', 'you', 'need']\n",
      "['think', 'think', 'eol', 'think', 'about', 'what', 'youre', 'trying', 'to', 'do']\n",
      "['carefree', 'laughter', 'eol', 'silence', 'ever', 'after', 'eol', 'walking', 'through', 'an']\n",
      "['hear', 'the', 'drums', 'fernando', 'eol', 'i', 'remember', 'long', 'ago', 'another']\n",
      "['you', 'can', 'dance', 'eol', 'you', 'can', 'jive', 'eol', 'having', 'the']\n",
      "['eol', 'at', 'waterloo', 'napoleon', 'did', 'surrender', 'eol', 'oh', 'yeah', 'eol']\n",
      "['beams', 'are', 'gonna', 'blind', 'me', 'eol', 'but', 'i', 'wont', 'feel']\n",
      "['change', 'your', 'mind', 'im', 'the', 'first', 'in', 'line', 'eol', 'honey']\n",
      "['all', 'night', 'i', 'work', 'all', 'day', 'to', 'pay', 'the', 'bills']\n",
      "['i', 'see', 'the', 'sign', 'that', 'points', 'one', 'way', 'eol', 'the']\n",
      "['twelve', 'eol', 'watchin', 'the', 'late', 'show', 'eol', 'in', 'my', 'flat']\n",
      "['special', 'in', 'fact', 'im', 'a', 'bit', 'of', 'a', 'bore', 'eol']\n",
      "['want', 'to', 'talk', 'eol', 'about', 'the', 'things', 'weve', 'gone', 'through']\n",
      "['these', 'are', 'the', 'good', 'times', 'eol', 'leave', 'your', 'cares', 'behind']\n",
      "['out', 'le', 'freak', 'cest', 'chic', 'eol', 'have', 'you', 'heard', 'about']\n",
      "['my', 'thrill', 'eol', 'on', 'blueberry', 'hill', 'eol', 'on', 'blueberry', 'hill']\n",
      "['first', 'time', 'wont', 'be', 'the', 'last', 'time', 'eol', 'dont', 'you']\n",
      "['the', 'park', 'eol', 'i', 'think', 'it', 'was', 'the', 'fourth', 'of']\n",
      "['the', 'break', 'of', 'day', 'eol', 'searching', 'for', 'something', 'to', 'say']\n",
      "['and', 'dreamless', 'nights', 'and', 'far', 'aways', 'eol', 'ooo', 'ooo', 'ooo']\n",
      "['goes', 'on', 'eol', 'i', 'realize', 'eol', 'just', 'what', 'you', 'mean']\n",
      "['with', 'you', 'eol', 'it', 'doesnt', 'matter', 'where', 'we', 'are', 'eol']\n",
      "['boys', 'are', 'eol', 'someone', 'waits', 'for', 'me', 'eol', 'a', 'smiling']\n",
      "['youre', 'a', 'real', 'mean', 'guy', 'eol', 'id', 'like', 'to', 'clip']\n",
      "['gotta', 'say', 'goodbye', 'for', 'the', 'summer', 'eol', 'baby', 'i', 'promise']\n",
      "['blue', 'velvet', 'eol', 'bluer', 'than', 'velvet', 'was', 'the', 'night', 'eol']\n",
      "['like', 'to', 'ride', 'in', 'my', 'beautiful', 'balloon', 'eol', 'would', 'you']\n",
      "['du', 'du', 'dudududududuud', 'eol', 'dudududuudu', 'eol', 'dudududududududududuududududududududuud', 'eol', 'dudududuu', 'eol']\n",
      "['to', 'say', 'it', 'and', 'its', 'hard', 'for', 'me', 'eol', 'you']\n",
      "['i', 'wake', 'up', 'eol', 'before', 'i', 'put', 'on', 'my', 'makeup']\n",
      "['know', 'the', 'way', 'to', 'san', 'jose', 'eol', 'ive', 'been', 'away']\n",
      "['see', 'me', 'walking', 'down', 'the', 'street', 'eol', 'and', 'i', 'start']\n",
      "['was', 'on', 'fire', 'and', 'no', 'one', 'could', 'save', 'me', 'but']\n",
      "['boats', 'go', 'out', 'across', 'the', 'evening', 'water', 'eol', 'smuggling', 'guns']\n",
      "['late', 'in', 'december', 'the', 'sky', 'turned', 'to', 'snow', 'eol', 'all']\n",
      "['morning', 'from', 'a', 'bogart', 'movie', 'eol', 'in', 'a', 'country', 'where']\n",
      "['must', 'be', 'a', 'kind', 'of', 'blind', 'love', 'eol', 'i', 'cant']\n",
      "['you', 'walking', 'everyday', 'eol', 'with', 'a', 'smile', 'beneath', 'frown', 'eol']\n",
      "['the', 'right', 'time', 'once', 'in', 'a', 'lifetime', 'eol', 'so', 'i']\n",
      "['you', 'in', 'my', 'memory', 'eol', 'as', 'vivid', 'as', 'today', 'eol']\n",
      "['yeah', 'yeah', 'yeah', 'eol', 'last', 'night', 'before', 'you', 'fell', 'asleep']\n",
      "['yeah', 'yeah', 'yeah', 'eol', 'yeah', 'yeah', 'yeah', 'yeah', 'yeah', 'eol']\n",
      "['you', 'go', 'again', 'eol', 'you', 'say', 'you', 'want', 'your', 'freedom']\n",
      "['my', 'love', 'by', 'his', 'way', 'of', 'walking', 'eol', 'and', 'i']\n",
      "['me', 'with', 'your', 'stories', 'eol', 'i', 'cant', 'believe', 'that', 'i']\n",
      "['at', 'night', 'and', 'im', 'feeling', 'down', 'eol', 'there', 'are', 'couples']\n",
      "['love', 'to', 'love', 'you', 'like', 'you', 'do', 'me', 'eol', 'id']\n",
      "['staring', 'on', 'eol', 'watching', 'her', 'life', 'go', 'by', 'eol', 'when']\n",
      "['oh', 'oh', 'yeah', 'yeah', 'eol', 'she', 'drove', 'a', 'long', 'way']\n",
      "['our', 'love', 'thats', 'all', 'we', 'have', 'eol', 'what', 'we', 'had']\n",
      "['yeah', 'eol', 'just', 'when', 'i', 'thought', 'i', 'was', 'safe', 'eol']\n",
      "['eol', 'hiding', 'in', 'the', 'dark', 'eol', 'im', 'looking', 'for', 'a']\n",
      "['just', 'a', 'dream', 'boat', 'eol', 'sailing', 'in', 'my', 'head', 'eol']\n",
      "['true', 'theres', 'nothing', 'like', 'me', 'and', 'you', 'eol', 'not', 'alone']\n",
      "['is', 'a', 'river', 'and', 'your', 'heart', 'is', 'a', 'boat', 'eol']\n",
      "['slept', 'at', 'all', 'in', 'days', 'eol', 'its', 'been', 'so', 'long']\n",
      "['giant', 'to', 'supernova', 'eol', 'back', 'to', 'you', 'and', 'me', 'eol']\n",
      "['drops', 'and', 'no', 'one', 'stirs', 'eol', 'on', 'a', 'lazy', 'summers']\n",
      "['in', 'our', 'hearts', 'eol', 'strangers', 'apart', 'eol', 'oh', 'please', 'come']\n",
      "['a', 'home', 'in', 'a', 'quiet', 'place', 'eol', 'i', 'see', 'myself']\n",
      "['daylights', 'gone', 'and', 'youre', 'on', 'your', 'own', 'eol', 'and', 'you']\n",
      "['the', 'sunshine', 'baby', 'whenever', 'you', 'smiled', 'eol', 'but', 'i', 'call']\n",
      "['here', 'and', 'warm', 'eol', 'but', 'i', 'could', 'look', 'away', 'and']\n",
      "['those', 'big', 'dark', 'eyes', 'that', 'flash', 'at', 'me', 'baby', 'eol']\n",
      "['the', 'christmas', 'tree', 'eol', 'at', 'the', 'christmas', 'party', 'hop', 'eol']\n",
      "['is', 'this', 'who', 'laid', 'to', 'rest', 'eol', 'on', 'marys', 'lap']\n",
      "['watch', 'out', 'eol', 'you', 'better', 'not', 'cry', 'eol', 'better', 'not']\n",
      "['those', 'sleigh', 'bells', 'jingling', 'eol', 'ring', 'ting', 'tingling', 'too', 'eol']\n",
      "['of', 'a', 'white', 'christmas', 'eol', 'just', 'like', 'the', 'ones', 'i']\n",
      "['mommy', 'kissing', 'santa', 'claus', 'eol', 'underneath', 'the', 'mistletoe', 'last', 'night']\n",
      "['ring', 'eol', 'are', 'you', 'listening', 'eol', 'in', 'the', 'lane', 'eol']\n",
      "['night', 'wind', 'to', 'the', 'little', 'lamb', 'eol', 'do', 'you', 'see']\n",
      "['are', 'brightly', 'shining', 'eol', 'it', 'is', 'the', 'night', 'of', 'our']\n",
      "['the', 'world', 'the', 'lord', 'is', 'come', 'eol', 'let', 'earth', 'receive']\n",
      "['the', 'snow', 'eol', 'in', 'a', 'one', 'horse', 'open', 'sleigh', 'eol']\n",
      "['mmm', 'eol', 'have', 'yourself', 'a', 'merry', 'little', 'christmas', 'eol', 'let']\n",
      "['noel', 'the', 'angels', 'did', 'say', 'eol', 'was', 'to', 'certain', 'poor']\n",
      "['all', 'ye', 'faithful', 'joyful', 'and', 'triumphant', 'eol', 'oh', 'come', 'ye']\n",
      "['snowman', 'was', 'a', 'jolly', 'happy', 'soul', 'eol', 'with', 'a', 'corncob']\n",
      "['have', 'heard', 'on', 'high', 'eol', 'angels', 'we', 'have', 'heard', 'on']\n",
      "['weather', 'outside', 'is', 'frightful', 'eol', 'but', 'the', 'fire', 'is', 'so']\n",
      "['holly', 'jolly', 'christmas', 'eol', 'its', 'the', 'best', 'time', 'of', 'the']\n",
      "['dasher', 'and', 'dancer', 'and', 'prancer', 'and', 'vixen', 'eol', 'you', 'know']\n",
      "['told', 'me', 'eol', 'pa', 'rum', 'pum', 'pum', 'pum', 'eol', 'a']\n",
      "['kings', 'eol', 'we', 'three', 'kings', 'of', 'orient', 'are', 'eol', 'bearing']\n",
      "['busy', 'sidewalks', 'eol', 'dressed', 'in', 'holiday', 'style', 'eol', 'in', 'the']\n",
      "['halls', 'with', 'boughs', 'of', 'holly', 'eol', 'fa', 'la', 'la', 'la']\n",
      "['my', 'problems', 'and', 'i', 'see', 'the', 'light', 'eol', 'we', 'got']\n",
      "['seen', 'a', 'man', 'at', 'the', 'liquor', 'store', 'beggin', 'for', 'your']\n",
      "['i', 'do', 'now', 'eol', 'i', 'do', 'i', 'do', 'eol', 'all']\n",
      "['know', 'that', 'i', 'shouldnt', 'be', 'here', 'eol', 'this', 'is', 'wrong']\n",
      "['flipmode', 'eol', 'here', 'we', 'come', 'bout', 'to', 'bust', 'and', 'explode']\n",
      "['than', 'you', 'want', 'me', 'to', 'eol', 'cant', 'help', 'myself', 'when']\n",
      "['the', 'city', 'of', 'new', 'orleans', 'eol', 'illinois', 'central', 'monday', 'morning']\n",
      "['dee', 'do', 'day', 'eol', 'dee', 'do', 'dee', 'do', 'dee', 'do']\n",
      "['nothing', 'but', 'bad', 'luck', 'eol', 'since', 'the', 'day', 'i', 'saw']\n",
      "['the', 'woman', 'that', 'ive', 'always', 'dreamed', 'of', 'eol', 'i', 'knew']\n",
      "['all', 'goes', 'crazy', 'and', 'the', 'thrill', 'is', 'gone', 'eol', 'when']\n",
      "['now', 'eol', 'its', 'in', 'your', 'eyes', 'eol', 'theres', 'no', 'disguising']\n",
      "['ride', 'the', 'hounds', 'of', 'hell', 'eol', 'twist', 'my', 'foot', 'i']\n",
      "['distance', 'eol', 'the', 'world', 'looks', 'blue', 'and', 'green', 'eol', 'and']\n",
      "['love', 'it', 'is', 'a', 'river', 'eol', 'that', 'drowns', 'the', 'tender']\n",
      "['knew', 'there', 'was', 'a', 'eol', 'love', 'like', 'this', 'before', 'eol']\n",
      "['my', 'world', 'eol', 'wont', 'you', 'come', 'on', 'in', 'eol', 'miracles']\n",
      "['make', 'me', 'lose', 'my', 'mind', 'eol', 'up', 'in', 'here', 'up']\n",
      "['back', 'to', 'memphis', 'eol', 'gotta', 'find', 'my', 'daisy', 'jane', 'eol']\n",
      "['eol', 'when', 'things', 'are', 'real', 'eol', 'and', 'the', 'people', 'share']\n",
      "['foggy', 'outside', 'eol', 'all', 'the', 'planes', 'have', 'been', 'grounded', 'eol']\n",
      "['tried', 'to', 'make', 'it', 'sunday', 'but', 'i', 'got', 'so', 'damn']\n",
      "['for', 'all', 'the', 'lonely', 'people', 'eol', 'thinking', 'that', 'life', 'has']\n",
      "['a', 'piece', 'of', 'grass', 'eol', 'walking', 'down', 'the', 'road', 'eol']\n",
      "['to', 'laugh', 'we', 'used', 'to', 'cry', 'eol', 'we', 'used', 'to']\n",
      "['if', 'you', 'could', 'return', 'eol', 'dont', 'let', 'it', 'burn', 'eol']\n",
      "['hangs', 'lowly', 'eol', 'child', 'is', 'slowly', 'taken', 'eol', 'and', 'if']\n",
      "['you', 'ruled', 'my', 'mind', 'eol', 'i', 'thought', 'youd', 'always', 'be']\n",
      "['swallow', 'my', 'pride', 'eol', 'i', 'would', 'choke', 'on', 'the', 'rinds']\n",
      "['nights', 'id', 'sit', 'by', 'my', 'window', 'eol', 'waiting', 'for', 'someone']\n",
      "['think', 'youre', 'better', 'off', 'alone', 'eol', 'do', 'you', 'think', 'youre']\n",
      "['sail', 'let', 'me', 'sail', 'eol', 'let', 'the', 'orinoco', 'flow', 'eol']\n",
      "['girlfriend', 'takes', 'me', 'home', 'when', 'im', 'too', 'drunk', 'to', 'drive']\n",
      "['to', 'tell', 'me', 'what', 'you', 'think', 'about', 'me', 'eol', 'i']\n",
      "['i', 'said', 'it', 'many', 'ways', 'eol', 'too', 'scared', 'to', 'run']\n",
      "['the', 'crowd', 'and', 'plays', 'so', 'loud', 'eol', 'baby', 'its', 'the']\n",
      "['through', 'runnin', 'its', 'true', 'eol', 'id', 'be', 'a', 'fool', 'to']\n",
      "['her', 'diary', 'underneath', 'a', 'tree', 'eol', 'and', 'started', 'reading', 'about']\n",
      "['me', 'from', 'harm', 'eol', 'kept', 'me', 'warm', 'kept', 'me', 'warm']\n",
      "['you', 'ever', 'tried', 'eol', 'really', 'reaching', 'out', 'for', 'the', 'other']\n",
      "['picture', 'paints', 'a', 'thousand', 'words', 'eol', 'then', 'why', 'cant', 'i']\n",
      "['life', 'youve', 'waited', 'for', 'love', 'to', 'eol', 'come', 'and', 'stay']\n",
      "['was', 'her', 'name', 'eol', 'a', 'not', 'so', 'very', 'ordinary', 'girl']\n",
      "['the', 'neon', 'lights', 'are', 'bright', 'eol', 'on', 'broadway', 'on', 'broadway']\n",
      "['dance', 'eol', 'every', 'dance', 'with', 'the', 'guy', 'who', 'gives', 'you']\n",
      "['old', 'world', 'starts', 'getting', 'me', 'down', 'eol', 'and', 'people', 'are']\n",
      "['moment', 'so', 'different', 'and', 'so', 'new', 'eol', 'was', 'like', 'any']\n",
      "['the', 'sun', 'beats', 'down', 'and', 'burns', 'the', 'tar', 'up', 'on']\n",
      "['mad', 'about', 'saffron', 'eol', 'saffrons', 'mad', 'about', 'me', 'eol', 'im']\n",
      "['softly', 'through', 'my', 'a', 'window', 'today', 'eol', 'couldve', 'tripped', 'out']\n",
      "['down', 'eol', 'im', 'going', 'down', 'down', 'down', 'eol', 'down', 'down']\n",
      "['she', 'may', 'not', 'a', 'look', 'eol', 'like', 'one', 'of', 'those']\n",
      "['a', 'drag', 'eol', 'when', 'your', 'baby', 'dont', 'love', 'you', 'eol']\n",
      "['could', 'reach', 'the', 'stars', 'id', 'pull', 'one', 'down', 'for', 'you']\n",
      "['my', 'heart', 'beat', 'again', 'eol', 'when', 'does', 'the', 'pain', 'ever']\n",
      "['tight', 'clothing', 'and', 'high', 'heel', 'shoes', 'eol', 'it', 'doesnt', 'mean']\n",
      "['never', 'gonna', 'get', 'it', 'eol', 'never', 'ever', 'gonna', 'get', 'it']\n",
      "['legs', 'dont', 'work', 'like', 'they', 'used', 'to', 'before', 'eol', 'and']\n",
      "['i', 'hope', 'and', 'pray', 'eol', 'a', 'dream', 'lover', 'will', 'come']\n",
      "['shark', 'babe', 'has', 'such', 'teeth', 'dear', 'eol', 'and', 'it', 'shows']\n",
      "['walk', 'walk', 'walk', 'eol', 'oh', 'walk', 'walk', 'walk', 'man', 'eol']\n",
      "['on', 'living', 'and', 'keep', 'on', 'forgiving', 'because', 'eol', 'you', 'were']\n",
      "['baby', 'eol', 'sherry', 'sherry', 'baby', 'eol', 'sherry', 'baby', 'sherry', 'baby']\n",
      "['oh', 'rag', 'doll', 'ooh', 'eol', 'hand', 'me', 'down', 'eol', 'when']\n",
      "['a', 'midsummers', 'morn', 'eol', 'they', 'call', 'her', 'dawn', 'eol', 'dawn']\n",
      "['love', 'eol', 'when', 'i', 'think', 'about', 'those', 'nights', 'in', 'montreal']\n",
      "['is', 'on', 'on', 'the', 'street', 'eol', 'inside', 'your', 'head', 'on']\n",
      "['goes', 'down', 'eol', 'the', 'night', 'rolls', 'in', 'eol', 'you', 'can']\n",
      "['you', 'need', 'a', 'friend', 'eol', 'someone', 'you', 'can', 'talk', 'to']\n",
      "['down', 'baby', 'ooo', 'uh', 'boogie', 'baby', 'lets', 'boogie', 'down', 'im']\n",
      "['the', 'longest', 'night', 'wont', 'last', 'forever', 'eol', 'but', 'too', 'many']\n",
      "['like', 'a', 'river', 'eol', 'time', 'beckoning', 'me', 'eol', 'who', 'knows']\n",
      "['no', 'sign', 'of', 'light', 'as', 'we', 'stand', 'in', 'the', 'darkness']\n",
      "['we', 'go', 'from', 'here', 'now', 'that', 'all', 'other', 'children', 'are']\n",
      "['got', 'a', 'heart', 'of', 'stone', 'eol', 'im', 'hurtin', 'more', 'now']\n",
      "['sorrys', 'easily', 'said', 'eol', 'dont', 'try', 'turning', 'tables', 'instead', 'eol']\n",
      "['closed', 'my', 'eyes', 'again', 'eol', 'climbed', 'aboard', 'the', 'dream', 'weaver']\n",
      "['think', 'its', 'time', 'to', 'get', 'ready', 'eol', 'to', 'realize', 'just']\n",
      "['a', 'patient', 'boy', 'eol', 'i', 'wait', 'i', 'wait', 'i', 'wait']\n",
      "['want', 'to', 'be', 'the', 'kind', 'to', 'hesitate', 'eol', 'be', 'too']\n",
      "['is', 'unconditional', 'eol', 'we', 'knew', 'it', 'from', 'the', 'start', 'eol']\n",
      "['all', 'my', 'exs', 'live', 'in', 'texas', 'eol', 'and', 'texas', 'is']\n",
      "['mornin', 'eol', 'up', 'from', 'san', 'antone', 'eol', 'everything', 'that', 'i']\n",
      "['of', 'god', 'eol', 'youve', 'got', 'to', 'go', 'faster', 'than', 'that']\n",
      "['of', 'bed', 'wasnt', 'feeling', 'too', 'good', 'eol', 'with', 'my', 'wallet']\n",
      "['need', 'is', 'a', 'tv', 'show', 'that', 'and', 'the', 'radio', 'eol']\n",
      "['key', 'to', 'my', 'survival', 'eol', 'was', 'never', 'in', 'much', 'doubt']\n",
      "['asleep', 'they', 'may', 'show', 'you', 'eol', 'aerial', 'views', 'of', 'the']\n",
      "['been', 'waiting', 'waiting', 'here', 'so', 'long', 'eol', 'but', 'thinking', 'nothing']\n",
      "['dreamed', 'a', 'thousand', 'dreams', 'eol', 'been', 'haunted', 'by', 'a', 'million']\n",
      "['dust', 'that', 'settles', 'all', 'around', 'me', 'eol', 'i', 'must', 'find']\n",
      "['the', 'life', 'on', 'the', 'city', 'of', 'gold', 'eol', 'hed', 'left']\n",
      "['be', 'some', 'misunderstanding', 'eol', 'there', 'must', 'be', 'some', 'kind', 'of']\n",
      "['time', 'i', 'was', 'searching', 'nowhere', 'to', 'run', 'to', 'it', 'started']\n",
      "['see', 'the', 'face', 'on', 'the', 'tv', 'screen', 'eol', 'coming', 'at']\n",
      "['is', 'clear', 'eol', 'though', 'no', 'eyes', 'can', 'see', 'eol', 'the']\n",
      "['on', 'the', 'wall', 'there', 'on', 'the', 'floor', 'eol', 'under', 'the']\n",
      "['the', 'blind', 'side', 'shinning', 'up', 'the', 'wall', 'eol', 'stealing', 'through']\n",
      "['eol', 'paperlate', 'paperlate', 'eol', 'paperlate', 'oh', 'im', 'sorry', 'but', 'there']\n",
      "['stations', 'eol', 'can', 'anybody', 'tell', 'me', 'tell', 'me', 'exactly', 'where']\n",
      "['down', 'eol', 'coming', 'down', 'like', 'a', 'monkey', 'eol', 'but', 'its']\n",
      "['my', 'do', 'you', 'want', 'to', 'say', 'goodbye', 'eol', 'to', 'have']\n",
      "['are', 'to', 'me', 'all', 'that', 'a', 'woman', 'should', 'be', 'eol']\n",
      "['too', 'much', 'for', 'the', 'man', 'eol', 'too', 'much', 'for', 'the']\n",
      "['youre', 'home', 'eol', 'now', 'did', 'you', 'really', 'miss', 'me', 'eol']\n",
      "['day', 'im', 'more', 'confused', 'eol', 'so', 'i', 'look', 'for', 'the']\n",
      "['love', 'you', 'eol', 'is', 'not', 'the', 'words', 'i', 'want', 'to']\n",
      "['we', 'need', 'is', 'candlelight', 'eol', 'you', 'and', 'me', 'and', 'a']\n",
      "['the', 'earth', 'move', 'under', 'my', 'feet', 'eol', 'i', 'feel', 'the']\n",
      "['away', 'eol', 'doesnt', 'anybody', 'stay', 'in', 'one', 'place', 'any', 'more']\n",
      "['mine', 'completely', 'eol', 'you', 'give', 'your', 'love', 'so', 'sweety', 'eol']\n",
      "['every', 'now', 'and', 'then', 'i', 'get', 'a', 'little', 'bit', 'lonely']\n",
      "['all', 'the', 'good', 'men', 'gone', 'eol', 'and', 'where', 'are', 'all']\n",
      "['duke', 'duke', 'of', 'earl', 'eol', 'duke', 'duke', 'duke', 'of', 'earl']\n",
      "['doin', 'out', 'there', 'yever', 'seem', 'to', 'have', 'one', 'of', 'those']\n",
      "['a', 'lonely', 'life', 'eol', 'she', 'leads', 'a', 'lonely', 'life', 'eol']\n",
      "['always', 'will', 'eol', 'i', 'was', 'mesmerized', 'when', 'i', 'first', 'met']\n",
      "['do', 'what', 'you', 'want', 'just', 'seize', 'the', 'day', 'eol', 'what']\n",
      "['got', 'a', 'new', 'life', 'you', 'would', 'hardly', 'recognize', 'me', 'im']\n",
      "['want', 'to', 'leave', 'eol', 'i', 'wont', 'beg', 'you', 'to', 'stay']\n",
      "['could', 'right', 'the', 'wrongs', 'that', 'made', 'you', 'cry', 'eol', 'i']\n",
      "['rock', 'your', 'body', 'eol', 'everybody', 'eol', 'rock', 'your', 'body', 'right']\n",
      "['that', 'i', 'cant', 'live', 'without', 'you', 'eol', 'its', 'just', 'that']\n",
      "['know', 'youre', 'hurting', 'eol', 'right', 'now', 'you', 'feel', 'like', 'you']\n",
      "['run', 'and', 'hide', 'eol', 'when', 'youre', 'screamin', 'my', 'name', 'alright']\n",
      "['turn', 'out', 'the', 'lights', 'eol', 'the', 'two', 'of', 'us', 'alone']\n",
      "['heah', 'eol', 'you', 'are', 'my', 'fire', 'eol', 'the', 'one', 'desire']\n",
      "['as', 'you', 'love', 'eol', 'although', 'loneliness', 'has', 'always', 'been', 'a']\n",
      "['the', 'one', 'eol', 'i', 'guess', 'you', 'were', 'lost', 'when', 'i']\n",
      "['try', 'to', 'forgive', 'me', 'eol', 'stay', 'here', 'dont', 'put', 'out']\n",
      "['your', 'heart', 'to', 'me', 'eol', 'and', 'say', 'whats', 'on', 'your']\n",
      "['the', 'meaning', 'of', 'being', 'lonely', 'eol', 'so', 'many', 'words', 'for']\n",
      "['sometimes', 'being', 'eol', 'on', 'the', 'road', 'is', 'rough', 'especially', 'the']\n",
      "['eol', 'even', 'in', 'my', 'heart', 'i', 'see', 'eol', 'youre', 'not']\n",
      "['someone', 'writes', 'a', 'song', 'with', 'a', 'eol', 'simple', 'rhyme', 'touch']\n",
      "['that', 'we', 'should', 'be', 'together', 'eol', 'its', 'unbelievable', 'how', 'i']\n",
      "['remember', 'why', 'we', 'fell', 'apart', 'eol', 'from', 'something', 'that', 'was']\n",
      "['to', 'hurry', 'a', 'lot', 'i', 'used', 'to', 'worry', 'a', 'lot']\n",
      "['a', 'runnin', 'down', 'the', 'road', 'tryn', 'to', 'loosen', 'my', 'load']\n",
      "['on', 'the', 'street', 'it', 'sounds', 'so', 'familiar', 'eol', 'great', 'expectations']\n",
      "['the', 'shiny', 'night', 'eol', 'the', 'rain', 'was', 'softly', 'falling', 'eol']\n",
      "['heard', 'some', 'people', 'talking', 'just', 'the', 'other', 'day', 'eol', 'and']\n",
      "['these', 'nights', 'one', 'of', 'these', 'crazy', 'old', 'nights', 'eol', 'were']\n",
      "['tequila', 'sunrise', 'eol', 'starin', 'slowly', 'cross', 'the', 'sky', 'said', 'goodbye']\n",
      "['of', 'love', 'have', 'you', 'got', 'eol', 'you', 'should', 'be', 'home']\n",
      "['at', 'the', 'end', 'of', 'the', 'evening', 'eol', 'and', 'the', 'bright']\n",
      "['out', 'there', 'on', 'your', 'own', 'eol', 'where', 'your', 'memories', 'can']\n",
      "['you', 'come', 'to', 'your', 'senses', 'eol', 'you', 'been', 'out', 'ridin']\n",
      "['stars', 'eol', 'in', 'the', 'southern', 'sky', 'eol', 'southward', 'as', 'you']\n",
      "['eol', 'im', 'lying', 'in', 'bed', 'eol', 'holdin', 'you', 'close', 'in']\n",
      "['just', 'seem', 'to', 'find', 'out', 'early', 'eol', 'how', 'to', 'open']\n",
      "['dark', 'desert', 'highway', 'cool', 'wind', 'in', 'my', 'hair', 'eol', 'warm']\n",
      "['standing', 'eol', 'all', 'alone', 'against', 'the', 'world', 'outside', 'eol', 'you']\n",
      "['a', 'hard', 'headed', 'man', 'he', 'was', 'brutally', 'handsome', 'eol', 'and']\n",
      "['from', 'providence', 'the', 'one', 'in', 'rhode', 'island', 'eol', 'where', 'the']\n",
      "['be', 'ringing', 'this', 'sad', 'sad', 'new', 'years', 'eol', 'oh', 'what']\n",
      "['the', 'way', 'sparkling', 'earrings', 'lay', 'eol', 'against', 'your', 'skin', 'so']\n",
      "['there', 'you', 'stand', 'eol', 'with', 'your', 'little', 'head', 'down', 'in']\n",
      "['how', 'are', 'ya', 'eol', 'its', 'been', 'a', 'long', 'time', 'eol']\n",
      "['hurt', 'someone', 'before', 'the', 'night', 'is', 'through', 'eol', 'somebodys', 'gonna']\n",
      "['and', 'ruby', 'lips', 'eol', 'sparks', 'fly', 'from', 'her', 'finger', 'tips']\n",
      "['long', 'time', 'ago', 'eol', 'i', 'can', 'still', 'remember', 'how', 'eol']\n",
      "['starry', 'night', 'eol', 'paint', 'your', 'palette', 'blue', 'and', 'grey', 'eol']\n",
      "['floors', 'of', 'tokyo', 'eol', 'a', 'down', 'in', 'london', 'towns', 'a']\n",
      "['sister', 'what', 'have', 'you', 'done', 'eol', 'hey', 'little', 'sister', 'whos']\n",
      "['comes', 'now', 'sayin', 'mony', 'mony', 'eol', 'shoot', 'em', 'down', 'turn']\n",
      "['the', 'time', 'so', 'i', 'will', 'sing', 'yeah', 'eol', 'im', 'just']\n",
      "['a', 'little', 'dancer', 'came', 'dancin', 'to', 'my', 'door', 'eol', 'last']\n",
      "['change', 'in', 'pace', 'eol', 'of', 'fantasy', 'and', 'taste', 'eol', 'do']\n",
      "['out', 'of', 'hope', 'eol', 'one', 'more', 'bad', 'dream', 'could', 'bring']\n",
      "['stranger', 'stranger', 'eol', 'its', 'hot', 'here', 'at', 'night', 'lonely', 'black']\n",
      "['a', 'clown', 'eol', 'gary', 'lewis', 'and', 'the', 'playboys', 'eol', 'written']\n",
      "['well', 'now', 'eol', 'we', 'call', 'this', 'the', 'act', 'of', 'mating']\n",
      "['the', 'roof', 'the', 'roof', 'is', 'on', 'fire', 'eol', 'we', 'dont']\n",
      "['you', 'wrapped', 'up', 'in', 'her', 'satin', 'and', 'lace', 'eol', 'tied']\n",
      "['and', 'wanda', 'were', 'the', 'best', 'of', 'friends', 'eol', 'all', 'through']\n",
      "['know', 'what', 'youre', 'looking', 'for', 'eol', 'you', 'havent', 'found', 'it']\n",
      "['did', 'you', 'hear', 'me', 'say', 'eol', 'you', 'know', 'the', 'difference']\n",
      "['kung', 'fu', 'fighting', 'eol', 'those', 'kicks', 'were', 'fast', 'as', 'lightning']\n",
      "['you', 'hold', 'me', 'how', 'you', 'control', 'me', 'eol', 'you', 'bend']\n",
      "['what', 'they', 'tell', 'us', 'eol', 'no', 'matter', 'what', 'they', 'do']\n",
      "['is', 'all', 'that', 'you', 'cant', 'say', 'eol', 'years', 'gone', 'by']\n",
      "['a', 'tear', 'eol', 'you', 'wiped', 'it', 'dry', 'eol', 'i', 'was']\n",
      "['pinch', 'of', 'one', 'man', 'eol', 'wrap', 'him', 'up', 'in', 'black']\n",
      "['lost', 'and', 'alone', 'eol', 'trying', 'to', 'grow', 'making', 'my', 'way']\n",
      "['the', 'feel', 'of', 'your', 'name', 'on', 'my', 'lips', 'eol', 'and']\n",
      "['is', 'all', 'that', 'you', 'cant', 'say', 'eol', 'years', 'gone', 'by']\n",
      "['home', 'eol', 'the', 'thoughts', 'racing', 'through', 'my', 'crazy', 'mind', 'eol']\n",
      "['our', 'lives', 'we', 'all', 'have', 'pain', 'eol', 'we', 'all', 'have']\n",
      "['the', 'crystal', 'raindrops', 'fall', 'eol', 'and', 'the', 'beauty', 'of', 'it']\n",
      "['got', 'a', 'problem', 'i', 'dont', 'care', 'what', 'it', 'is', 'eol']\n",
      "['theres', 'got', 'to', 'be', 'a', 'better', 'way', 'eol', 'say', 'it']\n",
      "['when', 'we', 'used', 'to', 'sit', 'in', 'the', 'government', 'yard', 'in']\n",
      "['not', 'here', 'i', 'come', 'you', 'cant', 'hide', 'eol', 'gonna', 'find']\n",
      "['he', 'sang', 'a', 'good', 'song', 'i', 'heard', 'he', 'had', 'a']\n",
      "['on', 'a', 'sunday', 'morning', 'eol', 'outside', 'i', 'see', 'the', 'rain']\n",
      "['you', 'do', 'what', 'you', 'do', 'to', 'me', 'i', 'wish', 'i']\n",
      "['need', 'to', 'be', 'a', 'global', 'citizen', 'eol', 'because', 'im', 'blessed']\n",
      "['for', 'a', 'walk', 'eol', 'not', 'the', 'after', 'dinner', 'kind', 'eol']\n",
      "['sign', 'said', 'long', 'haired', 'freaky', 'people', 'need', 'not', 'apply', 'eol']\n",
      "['the', 'eyes', 'that', 'never', 'knew', 'how', 'to', 'smile', 'eol', 'till']\n",
      "['the', 'ground', 'eol', 'there', 'is', 'movement', 'all', 'around', 'eol', 'there']\n",
      "['goin', 'back', 'to', 'massachusetts', 'eol', 'somethings', 'telling', 'me', 'i', 'must']\n",
      "['light', 'eol', 'a', 'certain', 'kind', 'of', 'light', 'eol', 'that', 'never']\n",
      "['your', 'jive', 'talkin', 'eol', 'youre', 'telling', 'me', 'lies', 'yeah', 'eol']\n",
      "['long', 'eol', 'you', 'and', 'me', 'been', 'finding', 'each', 'other', 'for']\n",
      "['known', 'you', 'very', 'well', 'eol', 'ive', 'seen', 'you', 'growing', 'every']\n",
      "['lie', 'eol', 'in', 'a', 'lost', 'and', 'lonely', 'part', 'of', 'town']\n",
      "['can', 'tell', 'by', 'the', 'way', 'i', 'use', 'my', 'walk', 'eol']\n",
      "['think', 'of', 'younger', 'days', 'eol', 'when', 'living', 'for', 'my', 'life']\n",
      "['everlasting', 'smile', 'eol', 'a', 'smile', 'can', 'bring', 'you', 'near', 'to']\n",
      "['too', 'much', 'heaven', 'no', 'more', 'eol', 'its', 'much', 'harder', 'to']\n",
      "['your', 'eyes', 'in', 'the', 'morning', 'sun', 'eol', 'i', 'feel', 'you']\n",
      "['a', 'joke', 'which', 'started', 'the', 'whole', 'world', 'crying', 'eol', 'but']\n",
      "['are', 'eol', 'in', 'a', 'room', 'full', 'of', 'strangers', 'eol', 'standing']\n",
      "['cant', 'figure', 'it', 'out', 'eol', 'your', 'kisses', 'taste', 'like', 'honey']\n",
      "['at', 'daddys', 'baby', 'girl', 'eol', 'thats', 'daddy', 'baby', 'eol', 'little']\n",
      "['name', 'is', 'what', 'eol', 'my', 'name', 'is', 'who', 'eol', 'my']\n",
      "['dre', 'just', 'let', 'it', 'run', 'eol', 'ey', 'yo', 'turn', 'the']\n",
      "['you', 'aint', 'familiar', 'with', 'these', 'here', 'parts', 'eol', 'you', 'know']\n",
      "['have', 'your', 'attention', 'please', 'eol', 'may', 'i', 'have', 'your', 'attention']\n",
      "['i', 'am', 'going', 'to', 'attempt', 'to', 'drown', 'myself', 'eol', 'you']\n",
      "['gone', 'cold', 'im', 'wondering', 'why', 'i', 'eol', 'got', 'out', 'of']\n",
      "['can', 'suck', 'my', 'dick', 'if', 'you', 'dont', 'like', 'my', 'shit']\n",
      "['twenty', 'three', 'years', 'old', 'eol', 'fed', 'up', 'with', 'life', 'and']\n",
      "['you', 'high', 'baby', 'eol', 'yeah', 'eol', 'yeah', 'eol', 'ha', 'ha']\n",
      "['get', 'home', 'babe', 'gonna', 'light', 'your', 'fire', 'eol', 'all', 'day']\n",
      "['stop', 'this', 'feelin', 'eol', 'deep', 'inside', 'of', 'me', 'eol', 'girl']\n",
      "['rocks', 'in', 'the', 'hot', 'sun', 'eol', 'i', 'fought', 'the', 'law']\n",
      "['an', 'obsession', 'eol', 'i', 'cannot', 'sleep', 'eol', 'i', 'am', 'your']\n",
      "['the', 'days', 'were', 'long', 'eol', 'and', 'rolled', 'beneath', 'a', 'deep']\n",
      "['my', 'living', 'off', 'the', 'evening', 'news', 'eol', 'just', 'give', 'me']\n",
      "['up', 'eol', 'dressed', 'all', 'in', 'black', 'eol', 'went', 'down', 'to']\n",
      "['the', 'basement', 'eol', 'mixing', 'up', 'the', 'medicine', 'eol', 'im', 'on']\n",
      "['her', 'sunday', 'that', 'was', 'yesterday', 'eol', 'the', 'girl', 'i', 'knew']\n",
      "['me', 'still', 'the', 'same', 'og', 'but', 'i', 'been', 'low', 'key']\n",
      "Dataset has: 176046 sequences and 176046 targets\n"
     ]
    }
   ],
   "source": [
    "songdata_train_dataset = SongDataset(train_midi_data, unified_embeddings, artist_to_index)\n",
    "songdata_test_dataset = SongDataset(train_midi_data, unified_embeddings, artist_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee962c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set = train_test_split(songdata_train_dataset, test_size=VALIDATION_SPLIT, random_state=RANDOM_LOADER_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = data.DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_data_loader = data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_data_loader = data.DataLoader(songdata_test_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {word_in_vocab: index_of_word for index_of_word, word_in_vocab in enumerate(songdata_train_dataset.word_embeddings.keys())} \n",
    "id_to_word = {index_of_word: word_in_vocab for word_in_vocab, index_of_word in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bfa020",
   "metadata": {},
   "source": [
    "<font size=6>Model 1: Simple concatenation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration Method 1: Simple Concatenation - Melody features are concatenated to each word embedding\n",
    "class LyricsGenerator_Concatenation(nn.Module):\n",
    "  def __init__(self, vocab_size, \n",
    "               input_size: int, \n",
    "               hidden_layer_dim: int, \n",
    "               num_layers: int = LSTM_LAYERS, \n",
    "               dropout_rate: float = DROPOUT\n",
    "               ):\n",
    "    super(LyricsGenerator_Concatenation, self).__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    # Input dimension is embedding + melody features\n",
    "\n",
    "    # LSTM processes concatenated features\n",
    "    self.lstm = nn.LSTM(input_size, hidden_layer_dim, num_layers,\n",
    "                        batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "\n",
    "    # Dropout and output layers\n",
    "    self.dropout = nn.Dropout(dropout_rate)\n",
    "    self.fc = nn.Linear(hidden_layer_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x is already concatenated [batch_size, seq_len, embedding_dim + melody_dim]\n",
    "\n",
    "    # LSTM forward pass\n",
    "    lstm_out, (hidden, cell) = self.lstm(x)\n",
    "\n",
    "    # Use the last output for prediction\n",
    "    last_output = lstm_out[:, -1, :]  # [batch_size, hidden_dim]\n",
    "\n",
    "    # Apply dropout and final linear layer\n",
    "    output = self.dropout(last_output)\n",
    "    output = self.fc(output)  # [batch_size, vocab_size]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02346871",
   "metadata": {},
   "source": [
    "<font size=6>Running the models</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f210285",
   "metadata": {},
   "source": [
    "<font size=5>Running model 1: Concatenation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb53ce",
   "metadata": {},
   "source": [
    "Add a custom loss to enforce the creation of words that look like actual lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyric_generation_loss(outputs, \n",
    "                          targets, \n",
    "                          base_criterion,\n",
    "                          word_to_id,\n",
    "                          min_line_length=MIN_LINE_LENGTH, \n",
    "                          max_line_length=MAX_LINE_LENGTH,\n",
    "                          current_line_length=None, \n",
    "                          penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    outputs: logits from model [batch_size, vocab_size]\n",
    "    targets: tensor of target indices [batch_size]\n",
    "    current_line_length: list[int] or tensor [batch_size] (length of current line for each sample)\n",
    "    current_num_lines: list[int] or tensor [batch_size] (number of lines so far for each sample)\n",
    "    \"\"\"\n",
    "    base_loss = base_criterion(outputs, targets)\n",
    "    eol_id = word_to_id.get(f\"{EOL_STRING}\")\n",
    "    penalty = 0.0\n",
    "    if eol_id is not None and current_line_length is not None:\n",
    "        # Penalize if eol is selected and line is too short\n",
    "        eol_selected = (targets == eol_id)\n",
    "        short_line = (torch.tensor(current_line_length, device=device) < min_line_length).detach()\n",
    "        penalty += penalty_weight * torch.sum(eol_selected & short_line).float() / outputs.size(0)\n",
    "\n",
    "        # Penalize if eol is NOT selected and line is too long\n",
    "        not_eol_selected = (targets != eol_id)\n",
    "        long_line = (torch.tensor(current_line_length, device=device) > max_line_length)\n",
    "        penalty += penalty_weight * torch.sum(not_eol_selected & long_line).float() / outputs.size(0)\n",
    "    \n",
    "\n",
    "    return base_loss + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02754ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[277], line 113\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# def train_model(model: nn.Module, \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#                 train_loader: data.DataLoader, \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#                 val_loader: data.DataLoader, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#     model.eval()\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m#     return model, train_losses, val_losses, test_losses\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model: nn\u001b[38;5;241m.\u001b[39mModule, \n\u001b[0;32m    116\u001b[0m                 train_loader: data\u001b[38;5;241m.\u001b[39mDataLoader, \n\u001b[0;32m    117\u001b[0m                 val_loader: data\u001b[38;5;241m.\u001b[39mDataLoader, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m                 patiance_factor: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m PATIANCE_FACTOR,\n\u001b[0;32m    123\u001b[0m                 patiance_epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m PATIANCE_EPOCHS):\n\u001b[0;32m    124\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_vendor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensorboard, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m Version(\n\u001b[0;32m      5\u001b[0m     tensorboard\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m      6\u001b[0m ) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.15\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "# def train_model(model: nn.Module, \n",
    "#                 train_loader: data.DataLoader, \n",
    "#                 val_loader: data.DataLoader, \n",
    "#                 test_loader: data.DataLoader,\n",
    "#                 word_to_id_dict: dict[str, int],\n",
    "#                 num_epochs: int = MAX_EPOCHS, \n",
    "#                 learning_rate: float = LEARNING_RATE,\n",
    "#                 patiance_factor: float = PATIANCE_FACTOR,\n",
    "#                 patiance_epochs: int = PATIANCE_EPOCHS):\n",
    "#     model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "#     best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "#     unk_id = word_to_id_dict.get(\"<unk>\", 0)\n",
    "#     train_losses: list[float] = list()\n",
    "#     val_losses: list[float] = list()\n",
    "#     test_losses: list[float] = list()\n",
    "#     best_validation_loss: float = 10000.0\n",
    "#     epochs_with_no_improvements: int = 0\n",
    "#     for epoch in range(num_epochs):\n",
    "#         current_time = time.time()\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, targets, line_lengths in train_loader:\n",
    "#             inputs = inputs.to(dtype=torch.float32)\n",
    "#             inputs = inputs.to(device)\n",
    "#             targets_indices = torch.tensor(\n",
    "#                 [word_to_id.get(t, unk_id) for t in targets],\n",
    "#                 dtype=torch.long, device=device\n",
    "#             )\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = lyric_generation_loss(\n",
    "#                 outputs,\n",
    "#                 targets_indices,\n",
    "#                 base_criterion=criterion,\n",
    "#                 word_to_id=word_to_id,\n",
    "#                 min_line_length=MIN_LINE_LENGTH,\n",
    "#                 current_line_length=line_lengths,\n",
    "#                 penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "#             )\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#         epoch_loss = running_loss / len(train_loader.dataset)\n",
    "#         train_losses.append(epoch_loss)\n",
    "\n",
    "#         model.eval()\n",
    "#         validation_running_loss = 0.0\n",
    "#         test_running_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for val_inputs, val_targets, val_line_length in val_loader:\n",
    "#                 val_inputs = val_inputs.to(dtype=torch.float32)\n",
    "#                 val_inputs = val_inputs.to(device)\n",
    "#                 val_targets_indices = torch.tensor(\n",
    "#                     [word_to_id.get(t, unk_id) for t in val_targets],\n",
    "#                     dtype=torch.long, device=device\n",
    "#                 )\n",
    "#                 validation_outputs = model(val_inputs)\n",
    "#                 validation_loss = lyric_generation_loss(\n",
    "#                     validation_outputs,\n",
    "#                     val_targets_indices,\n",
    "#                     base_criterion=criterion,\n",
    "#                     word_to_id=word_to_id,\n",
    "#                     min_line_length=MIN_LINE_LENGTH,\n",
    "#                     current_line_length=val_line_length,\n",
    "#                     penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "#                 )\n",
    "#                 validation_running_loss += validation_loss.item() * val_inputs.size(0)\n",
    "\n",
    "#             for test_inputs, test_targets, test_line_length in test_loader:\n",
    "#                 test_inputs = test_inputs.to(dtype=torch.float32)\n",
    "#                 test_inputs = test_inputs.to(device)\n",
    "#                 test_targets_indices = torch.tensor(\n",
    "#                     [word_to_id.get(t, unk_id) for t in test_targets],\n",
    "#                     dtype=torch.long, device=device\n",
    "#                 )\n",
    "#                 test_outputs = model(test_inputs)                  # logits (B, |V|)\n",
    "#                 test_loss = criterion(test_outputs, test_targets_indices)\n",
    "#                 test_loss = lyric_generation_loss(\n",
    "#                     test_outputs,\n",
    "#                     test_targets_indices,\n",
    "#                     base_criterion=criterion,\n",
    "#                     word_to_id=word_to_id,\n",
    "#                     min_line_length=MIN_LINE_LENGTH,\n",
    "#                     current_line_length=test_line_length,\n",
    "#                     penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "#                 )\n",
    "#                 test_running_loss += test_loss.item() * test_inputs.size(0)\n",
    "\n",
    "#         val_epoch_loss = validation_running_loss / len(val_loader.dataset)\n",
    "#         val_losses.append(val_epoch_loss)\n",
    "#         test_epoch_loss = test_running_loss / len(test_loader.dataset)\n",
    "#         test_losses.append(test_epoch_loss)\n",
    "#         if  best_validation_loss - val_epoch_loss >= patiance_factor:\n",
    "#             epochs_with_no_improvements = 0\n",
    "#             best_validation_loss = val_epoch_loss\n",
    "#             best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "#         else:\n",
    "#             epochs_with_no_improvements += 1\n",
    "#             print(f'No improvement in epoch. Patiance: {epochs_with_no_improvements}\\\\{patiance_epochs}')\n",
    "#         scheduler.step()\n",
    "#         finish_time = time.time() - current_time\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, Test Loss: {test_epoch_loss:.4f}, time: {finish_time}')\n",
    "#         if epochs_with_no_improvements >= patiance_epochs:\n",
    "#             print(f'Training ended prematurely due to lack of improvement.')\n",
    "#             break\n",
    "#     model.load_state_dict(best_model_state_dict)\n",
    "#     model.eval()\n",
    "#     return model, train_losses, val_losses, test_losses\n",
    "def train_model(model: nn.Module, \n",
    "                train_loader: data.DataLoader, \n",
    "                val_loader: data.DataLoader, \n",
    "                test_loader: data.DataLoader,\n",
    "                word_to_id_dict: dict[str, int],\n",
    "                num_epochs: int = MAX_EPOCHS, \n",
    "                learning_rate: float = LEARNING_RATE,\n",
    "                patiance_factor: float = PATIANCE_FACTOR,\n",
    "                patiance_epochs: int = PATIANCE_EPOCHS):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "    unk_id = word_to_id_dict.get(\"<unk>\", 0)\n",
    "    train_losses: list[float] = list()\n",
    "    val_losses: list[float] = list()\n",
    "    test_losses: list[float] = list()\n",
    "    best_validation_loss: float = 10000.0\n",
    "    epochs_with_no_improvements: int = 0\n",
    "\n",
    "    writer = SummaryWriter()  # TensorBoard writer\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets, line_lengths in train_loader:\n",
    "            inputs = inputs.to(dtype=torch.float32)\n",
    "            inputs = inputs.to(device)\n",
    "            targets_indices = torch.tensor(\n",
    "                [word_to_id.get(t, unk_id) for t in targets],\n",
    "                dtype=torch.long, device=device\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = lyric_generation_loss(\n",
    "                outputs,\n",
    "                targets_indices,\n",
    "                base_criterion=criterion,\n",
    "                word_to_id=word_to_id,\n",
    "                min_line_length=MIN_LINE_LENGTH,\n",
    "                current_line_length=line_lengths,\n",
    "                penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)  # TensorBoard\n",
    "\n",
    "        model.eval()\n",
    "        validation_running_loss = 0.0\n",
    "        test_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_line_length in val_loader:\n",
    "                val_inputs = val_inputs.to(dtype=torch.float32)\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets_indices = torch.tensor(\n",
    "                    [word_to_id.get(t, unk_id) for t in val_targets],\n",
    "                    dtype=torch.long, device=device\n",
    "                )\n",
    "                validation_outputs = model(val_inputs)\n",
    "                validation_loss = lyric_generation_loss(\n",
    "                    validation_outputs,\n",
    "                    val_targets_indices,\n",
    "                    base_criterion=criterion,\n",
    "                    word_to_id=word_to_id,\n",
    "                    min_line_length=MIN_LINE_LENGTH,\n",
    "                    current_line_length=val_line_length,\n",
    "                    penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "                )\n",
    "                validation_running_loss += validation_loss.item() * val_inputs.size(0)\n",
    "\n",
    "            val_epoch_loss = validation_running_loss / len(val_loader.dataset)\n",
    "            val_losses.append(val_epoch_loss)\n",
    "            writer.add_scalar(\"Loss/val\", val_epoch_loss, epoch)  # TensorBoard\n",
    "\n",
    "            for test_inputs, test_targets, test_line_length in test_loader:\n",
    "                test_inputs = test_inputs.to(dtype=torch.float32)\n",
    "                test_inputs = test_inputs.to(device)\n",
    "                test_targets_indices = torch.tensor(\n",
    "                    [word_to_id.get(t, unk_id) for t in test_targets],\n",
    "                    dtype=torch.long, device=device\n",
    "                )\n",
    "                test_outputs = model(test_inputs)                  # logits (B, |V|)\n",
    "                test_loss = lyric_generation_loss(\n",
    "                    test_outputs,\n",
    "                    test_targets_indices,\n",
    "                    base_criterion=criterion,\n",
    "                    word_to_id=word_to_id,\n",
    "                    min_line_length=MIN_LINE_LENGTH,\n",
    "                    current_line_length=test_line_length,\n",
    "                    penalty_weight=SONG_STRUCTURE_PENALTY\n",
    "                )\n",
    "                test_running_loss += test_loss.item() * test_inputs.size(0)\n",
    "\n",
    "        test_epoch_loss = test_running_loss / len(test_loader.dataset)\n",
    "        test_losses.append(test_epoch_loss)\n",
    "        if  best_validation_loss - val_epoch_loss >= patiance_factor:\n",
    "            epochs_with_no_improvements = 0\n",
    "            best_validation_loss = val_epoch_loss\n",
    "            best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            epochs_with_no_improvements += 1\n",
    "            print(f'No improvement in epoch. Patiance: {epochs_with_no_improvements}\\\\{patiance_epochs}')\n",
    "        scheduler.step()\n",
    "        finish_time = time.time() - current_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}, Test Loss: {test_epoch_loss:.4f}, time: {finish_time}')\n",
    "        if epochs_with_no_improvements >= patiance_epochs:\n",
    "            print(f'Training ended prematurely due to lack of improvement.')\n",
    "            break\n",
    "    writer.close()  # Close TensorBoard writer\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    model.eval()\n",
    "    return model, train_losses, val_losses, test_losses\n",
    "\n",
    "# After training, run in terminal to view TensorBoard:\n",
    "# !tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b319f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LyricsGenerator_Concatenation(\n",
    "    vocab_size=len(songdata_train_dataset.word_embeddings),\n",
    "    input_size=pretrained_word2vec.vector_size + NUMBER_OF_EXTRACT_MIDI_FEATURES + 1, # word embedding + melody features + artist index\n",
    "    hidden_layer_dim=256,\n",
    "    num_layers=LSTM_LAYERS,\n",
    "    dropout_rate=DROPOUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_loss, validation_loss, test_loss = train_model(model, \n",
    "            training_data_loader, \n",
    "            validation_data_loader, \n",
    "            test_data_loader,\n",
    "            word_to_id_dict=word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27710111",
   "metadata": {},
   "source": [
    "Displaying tensorboard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e702053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd43df",
   "metadata": {},
   "source": [
    "<font size=6>Generating Lyrics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6c6c5",
   "metadata": {},
   "source": [
    "A function that returns the k most likely words given the input, used for coherent lyric generatin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf15b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_top_k_next_words(\n",
    "    model,\n",
    "    top_words_to_predict: int,\n",
    "    word_sequence: list[str],\n",
    "    artist_index: int,\n",
    "    melody_vec,\n",
    "    word_to_id: dict[str, int],\n",
    "    id_to_word: dict[int, str],\n",
    "    embedding_weight: torch.Tensor,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    model.to(device).eval()\n",
    "    if embedding_weight.device.type != device:\n",
    "        embedding_weight = embedding_weight.to(device)\n",
    "\n",
    "    # melody -> torch tensor on device\n",
    "    if not torch.is_tensor(melody_vec):\n",
    "        melody_vec = torch.as_tensor(melody_vec, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        melody_vec = melody_vec.to(device, dtype=torch.float32)\n",
    "\n",
    "    # Prepare sequence embeddings\n",
    "    unk_id = word_to_id.get(\"<unk>\", UNK_ID)\n",
    "    seq_ids = [word_to_id.get(w, unk_id) for w in word_sequence]\n",
    "    seq_embs = embedding_weight[torch.tensor(seq_ids, device=device)]  # [seq_len, emb_dim]\n",
    "\n",
    "    # Broadcast melody and artist features\n",
    "    melody_broadcast = melody_vec.expand(len(word_sequence), -1)  # [seq_len, melody_dim]\n",
    "    artist_broadcast = torch.full((len(word_sequence), 1), artist_index, dtype=torch.float32, device=device)  # [seq_len, 1]\n",
    "\n",
    "    # Concatenate features\n",
    "    x = torch.cat([seq_embs, melody_broadcast, artist_broadcast], dim=1).unsqueeze(0)  # [1, seq_len, input_size]\n",
    "\n",
    "    logits = model(x)  # [1, vocab]\n",
    "    probs = F.softmax(logits, dim=-1).squeeze(0)  # [vocab]\n",
    "\n",
    "    vals, idxs = probs.topk(top_words_to_predict)\n",
    "    top5 = [(id_to_word[i], float(v)) for i, v in zip(idxs.tolist(), vals.tolist())]\n",
    "    return top5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fcbb1",
   "metadata": {},
   "source": [
    "Printing the generated text and handling tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "45e527c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generated_lyrics(generated_lyrics: list[str]):\n",
    "    capitalize = True\n",
    "    for word in generated_lyrics:\n",
    "        if word == EOL_STRING:\n",
    "            capitalize = True\n",
    "            print()\n",
    "        if word == EOS_STRING:\n",
    "            break\n",
    "        if word != EOL_STRING:\n",
    "            if capitalize:\n",
    "                capitalize = False\n",
    "                print(word.title(), end=' ')\n",
    "            else:\n",
    "                print(word, end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5239b529",
   "metadata": {},
   "source": [
    "Generating the lyrics and maintaining the lyrics generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3392cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(\n",
    "        model_to_use: nn.Module,\n",
    "        initial_word: str,\n",
    "        melody_features: np.ndarray,\n",
    "        melody_title: str,\n",
    "        artist_to_use: str,\n",
    "        word_to_id: dict[str, int],\n",
    "        id_to_word: dict[int, str],\n",
    "        artist_to_index: dict[str, int],\n",
    "        word_embeddings: dict[str, np.ndarray],\n",
    "        max_song_length: int = MAX_SONG_LENGTH_WORDS,\n",
    "        sequence_length: int = SEQUENCE_LENGTH,\n",
    "        top_k: int = TOP_K_WORDS_TO_PREDICT,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates lyrics word by word using the model, melody, and artist.\n",
    "    Picks next word randomly from top_k candidates according to their normalized probabilities.\n",
    "    Artificially increases probability of EOS_STRING after half of max_song_length.\n",
    "    Prints the generated lyrics with line breaks at <eol>.\n",
    "    Enforces some more grammatical rules.\n",
    "    Returns: generated_lyrics (list of str), artist, melody_title.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating song from initial word: '{initial_word}', melody: '{melody_title}', artist: '{artist_to_use}', max length: {max_song_length}\")\n",
    "    melody_vec = torch.as_tensor(melody_features['vector'], dtype=torch.float32, device=device)\n",
    "    artist_idx = artist_to_index.get(artist_to_use, -1)\n",
    "    if artist_idx == -1:\n",
    "        print(f\"Warning: Artist '{artist_to_use}' not found, using index -1.\")\n",
    "\n",
    "    embedding_weight = torch.from_numpy(\n",
    "        np.stack([word_embeddings[w].astype(np.float32) for w in word_to_id], axis=0)\n",
    "    ).to(device)\n",
    "    context: deque = deque()\n",
    "    context.extendleft([UNK_STRING for _ in range(sequence_length - 1)])\n",
    "    context.appendleft(initial_word)\n",
    "    unk_index: int = 1\n",
    "    generated_lyrics = [initial_word]\n",
    "    words_in_song: int = 0\n",
    "    current_word: str = initial_word\n",
    "    minimum_song_length = int(max_song_length / 2)\n",
    "\n",
    "    while True:\n",
    "        top_words = predict_top_k_next_words(\n",
    "            model=model_to_use,\n",
    "            top_words_to_predict=top_k,\n",
    "            word_sequence=context,\n",
    "            artist_index=artist_idx,\n",
    "            melody_vec=melody_vec,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            embedding_weight=embedding_weight,\n",
    "            device=device\n",
    "        )\n",
    "        words, probs = zip(*top_words)\n",
    "        probs = np.array(probs, dtype=np.float32)\n",
    "        probs = probs / probs.sum()  # Normalize to sum to 1\n",
    "\n",
    "        # Artificially increase EOS probability after reaching half of max_song_length.\n",
    "        words, probs = zip(*top_words)\n",
    "        probs = np.array(probs, dtype=np.float32)\n",
    "        probs = probs / probs.sum()  # Normalize to sum to 1\n",
    "\n",
    "        # If EOS should be considered, add it to the candidates and replace the lowest probability\n",
    "        if words_in_song >= minimum_song_length and EOS_STRING not in words:\n",
    "            # Find the index of the lowest probability\n",
    "            min_prob_idx = np.argmin(probs)\n",
    "            # Add EOS to words and replace the lowest probability with eos_boost\n",
    "            eos_boost = (words_in_song - minimum_song_length + 1) / minimum_song_length\n",
    "            eos_boost = min(max(eos_boost, 0.0), 1.0)\n",
    "            words = list(words)\n",
    "            probs = list(probs)\n",
    "            words[min_prob_idx] = EOS_STRING\n",
    "            probs[min_prob_idx] = eos_boost\n",
    "            # Renormalize\n",
    "            probs = np.array(probs, dtype=np.float32)\n",
    "            probs = probs / probs.sum()\n",
    "\n",
    "        # Don't allow end of song before minimum amount of lines.\n",
    "        # Don't repeat the same word twice\n",
    "        while True:\n",
    "            next_word = np.random.choice(words, p=probs)     \n",
    "            if next_word == current_word:\n",
    "                continue\n",
    "            if next_word == EOS_STRING and words_in_song < int(minimum_song_length):\n",
    "                continue\n",
    "            break\n",
    "        if next_word == EOS_STRING and current_word == EOL_STRING:\n",
    "            generated_lyrics[-1] = next_word\n",
    "        else:\n",
    "            generated_lyrics.append(next_word)\n",
    "        current_word = next_word\n",
    "        if unk_index < sequence_length:\n",
    "            context[unk_index] = next_word\n",
    "            unk_index += 1\n",
    "        else:\n",
    "            context.popleft()\n",
    "            context.append(next_word)\n",
    "        if next_word == EOS_STRING:\n",
    "            break\n",
    "        if words_in_song >= max_song_length:\n",
    "            generated_lyrics.append(EOS_STRING)\n",
    "            break\n",
    "        if next_word != EOL_STRING:\n",
    "            words_in_song += 1\n",
    "    print_generated_lyrics(generated_lyrics=generated_lyrics)\n",
    "    print(f'Number of words in lyrics: {words_in_song}')\n",
    "    print(\"\\n--- End of generated lyrics ---\")\n",
    "    return generated_lyrics, artist_to_use, melody_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0acea986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating song from initial word: 'eyes', melody: 'karma chameleon', artist: 'billy joel', max length: 80\n",
      "-------------\n",
      "context is: deque(['eyes', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'and', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'and', 'eol', 'unk', 'unk', 'unk', 'unk', 'unk', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'and', 'eol', 'me', 'unk', 'unk', 'unk', 'unk', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'and', 'eol', 'me', 'eol', 'unk', 'unk', 'unk', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'and', 'eol', 'me', 'eol', 'and', 'unk', 'unk', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'and', 'eol', 'me', 'eol', 'and', 'the', 'unk', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'and', 'eol', 'me', 'eol', 'and', 'the', 'a', 'unk'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eyes', 'eol', 'and', 'eol', 'me', 'eol', 'and', 'the', 'a', 'eol'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eol', 'and', 'eol', 'me', 'eol', 'and', 'the', 'a', 'eol', 'who'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['and', 'eol', 'me', 'eol', 'and', 'the', 'a', 'eol', 'who', 'oh'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eol', 'me', 'eol', 'and', 'the', 'a', 'eol', 'who', 'oh', 'i'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['me', 'eol', 'and', 'the', 'a', 'eol', 'who', 'oh', 'i', 'be'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eol', 'and', 'the', 'a', 'eol', 'who', 'oh', 'i', 'be', 'to'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['and', 'the', 'a', 'eol', 'who', 'oh', 'i', 'be', 'to', 'eol'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['the', 'a', 'eol', 'who', 'oh', 'i', 'be', 'to', 'eol', 'i'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['a', 'eol', 'who', 'oh', 'i', 'be', 'to', 'eol', 'i', 'be'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eol', 'who', 'oh', 'i', 'be', 'to', 'eol', 'i', 'be', 'to'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['who', 'oh', 'i', 'be', 'to', 'eol', 'i', 'be', 'to', 'you'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['oh', 'i', 'be', 'to', 'eol', 'i', 'be', 'to', 'you', 'be'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['i', 'be', 'to', 'eol', 'i', 'be', 'to', 'you', 'be', 'my'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['be', 'to', 'eol', 'i', 'be', 'to', 'you', 'be', 'my', 'way'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['to', 'eol', 'i', 'be', 'to', 'you', 'be', 'my', 'way', 'eol'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eol', 'i', 'be', 'to', 'you', 'be', 'my', 'way', 'eol', 'but'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['i', 'be', 'to', 'you', 'be', 'my', 'way', 'eol', 'but', 'i'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['be', 'to', 'you', 'be', 'my', 'way', 'eol', 'but', 'i', 'be'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['to', 'you', 'be', 'my', 'way', 'eol', 'but', 'i', 'be', 'a'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['you', 'be', 'my', 'way', 'eol', 'but', 'i', 'be', 'a', 'friend'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['be', 'my', 'way', 'eol', 'but', 'i', 'be', 'a', 'friend', 'of'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['my', 'way', 'eol', 'but', 'i', 'be', 'a', 'friend', 'of', 'me'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['way', 'eol', 'but', 'i', 'be', 'a', 'friend', 'of', 'me', 'to'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eol', 'but', 'i', 'be', 'a', 'friend', 'of', 'me', 'to', 'see'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['but', 'i', 'be', 'a', 'friend', 'of', 'me', 'to', 'see', 'eol'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['i', 'be', 'a', 'friend', 'of', 'me', 'to', 'see', 'eol', 'i'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['be', 'a', 'friend', 'of', 'me', 'to', 'see', 'eol', 'i', 'want'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['a', 'friend', 'of', 'me', 'to', 'see', 'eol', 'i', 'want', 'to'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['friend', 'of', 'me', 'to', 'see', 'eol', 'i', 'want', 'to', 'love'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['of', 'me', 'to', 'see', 'eol', 'i', 'want', 'to', 'love', 'your'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['me', 'to', 'see', 'eol', 'i', 'want', 'to', 'love', 'your', 'love'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['to', 'see', 'eol', 'i', 'want', 'to', 'love', 'your', 'love', 'eol'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['see', 'eol', 'i', 'want', 'to', 'love', 'your', 'love', 'eol', 'i'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eol', 'i', 'want', 'to', 'love', 'your', 'love', 'eol', 'i', 'cant'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['i', 'want', 'to', 'love', 'your', 'love', 'eol', 'i', 'cant', 'do'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['want', 'to', 'love', 'your', 'love', 'eol', 'i', 'cant', 'do', 'to'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['to', 'love', 'your', 'love', 'eol', 'i', 'cant', 'do', 'to', 'be'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['love', 'your', 'love', 'eol', 'i', 'cant', 'do', 'to', 'be', 'a'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['your', 'love', 'eol', 'i', 'cant', 'do', 'to', 'be', 'a', 'chance'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['love', 'eol', 'i', 'cant', 'do', 'to', 'be', 'a', 'chance', 'of'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['eol', 'i', 'cant', 'do', 'to', 'be', 'a', 'chance', 'of', 'the'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['i', 'cant', 'do', 'to', 'be', 'a', 'chance', 'of', 'the', 'eol'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['cant', 'do', 'to', 'be', 'a', 'chance', 'of', 'the', 'eol', 'but'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['do', 'to', 'be', 'a', 'chance', 'of', 'the', 'eol', 'but', 'i'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['to', 'be', 'a', 'chance', 'of', 'the', 'eol', 'but', 'i', 'cant'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['be', 'a', 'chance', 'of', 'the', 'eol', 'but', 'i', 'cant', 'be'])\n",
      "-------------\n",
      "-------------\n",
      "context is: deque(['a', 'chance', 'of', 'the', 'eol', 'but', 'i', 'cant', 'be', 'the'])\n",
      "-------------\n",
      "Eyes \n",
      "And \n",
      "Me \n",
      "And the a \n",
      "Who oh i be to \n",
      "I be to you be my way \n",
      "But i be a friend of me to see \n",
      "I want to love your love \n",
      "I cant do to be a chance of the \n",
      "But i cant be the \n",
      "Number of words in lyrics: 46\n",
      "\n",
      "--- End of generated lyrics ---\n"
     ]
    }
   ],
   "source": [
    "song_to_use = train_midi_data[63]\n",
    "\n",
    "lyrics, artist, melody = generate_lyrics(\n",
    "    model_to_use=model,\n",
    "    initial_word=\"eyes\",\n",
    "    melody_features=song_to_use.midi_features,\n",
    "    melody_title=song_to_use.title,\n",
    "    artist_to_use='billy joel',\n",
    "    word_to_id=word_to_id,\n",
    "    id_to_word=id_to_word,\n",
    "    artist_to_index=artist_to_index,\n",
    "    word_embeddings=unified_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b6897",
   "metadata": {},
   "source": [
    "<font size=6>Section 7, testing with the testing set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949fa72",
   "metadata": {},
   "source": [
    "For each melody, the output of the architecture given the melody and the initial word of the real lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "87e489d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Generating song from initial word: 'close', melody: 'eternal flame', artist: 'the bangles', max length: 95\n",
      "Close me \n",
      "I dont know the time to say it \n",
      "Its the race \n",
      "Oh you know it the love \n",
      "Its love the a place \n",
      "I can be \n",
      "I dont do \n",
      "I can be your superman i do you be to \n",
      "You can want it you know i do to have \n",
      "And im want to \n",
      "Number of words in lyrics: 53\n",
      "\n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'if', melody: 'honesty', artist: 'billy joel', max length: 211\n",
      "If \n",
      "You want be the life \n",
      "And i need the way you make it me \n",
      "Oh \n",
      "I know i love to me you be \n",
      "And my love for the heart \n",
      "Oh a bunch a little of world \n",
      "And my world the mind \n",
      "Im can to see it you \n",
      "I just it \n",
      "If the time and a bottle of the world \n",
      "I be love \n",
      "I be \n",
      "But to love it \n",
      "You dont see a fool \n",
      "I know to have it for you to do \n",
      "And you need to love you me \n",
      "I be it you to do \n",
      "Cause if that oh and im not you \n",
      "Im you do you know you \n",
      "Im you have to be \n",
      "But i \n",
      "Number of words in lyrics: 116\n",
      "\n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'dear', melody: 'lovefool', artist: 'cardigans', max length: 292\n",
      "Dear \n",
      "And i know that what i know \n",
      "You cant want me to do \n",
      "I never want the way is \n",
      "Its my way of my mind \n",
      "You can never be you be you \n",
      "I want to want me your heart \n",
      "I can want the way \n",
      "Im love the world \n",
      "The a york minute \n",
      "Oh i need you \n",
      "I can get \n",
      "And you dont need i know \n",
      "You want the one \n",
      "I do to do \n",
      "You want you say to you \n",
      "And you be a chance \n",
      "I got me \n",
      "When you have me you \n",
      "I know the love that i say \n",
      "I know to know i be \n",
      "Cause i really get \n",
      "You dont do it to can \n",
      "But \n",
      "When \n",
      "I dont know you go \n",
      "And i love \n",
      "And i know to know you feel you say you be it \n",
      "I know that to dont get \n",
      "And i be your way to know \n",
      "But my name is my heart \n",
      "Number of words in lyrics: 156\n",
      "\n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'hiya', melody: 'barbie girl', artist: 'aqua', max length: 390\n",
      "Hiya \n",
      "You be a little day \n",
      "I need you \n",
      "I got you \n",
      "You want \n",
      "Its love is to \n",
      "No i get you to be \n",
      "I can want \n",
      "But im dont do you be my heart \n",
      "But i really get \n",
      "I want the love you know \n",
      "I need you say you be my \n",
      "You know you say \n",
      "I can have a chance \n",
      "I want to make \n",
      "Cause i got to be me you \n",
      "But you do to do you \n",
      "Im can be a dream \n",
      "You cant know me you be your \n",
      "I can see it in \n",
      "And i know you feel me to say me \n",
      "Cause i feel a chance of my heart \n",
      "And i love that my eyes \n",
      "You got your way that and a friend of mine \n",
      "And my time to can \n",
      "It you know you be me to you be \n",
      "To got me you i say \n",
      "I dont know you \n",
      "I do me my love \n",
      "If i need the way that a new york \n",
      "I \n",
      "I get a little \n",
      "And you know to make and you can \n",
      "It i say it you know and do \n",
      "I want you say i do and i say me \n",
      "But i love me \n",
      "But i know \n",
      "And a woman to know you know \n",
      "And the love in to be \n",
      "So it i want me to be \n",
      "To do \n",
      "Number of words in lyrics: 225\n",
      "\n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'all', melody: 'all the small things', artist: 'blink 182', max length: 176\n",
      "All on your heart \n",
      "And a bunch \n",
      "Of \n",
      "I be \n",
      "Its the time \n",
      "Its bop of a ling york \n",
      "And i know you \n",
      "Im you to have my heart \n",
      "If the love you a fool \n",
      "Im know to say \n",
      "But the love is my heart \n",
      "Im got to say \n",
      "And the way in you \n",
      "I dont be \n",
      "I never know \n",
      "Its the love \n",
      "Im know it to can want a friend \n",
      "Its love is \n",
      "I be to say \n",
      "I dont do you \n",
      "If you can want the heart of you be \n",
      "Its time \n",
      "A little bit \n",
      "Im can never \n",
      "Number of words in lyrics: 98\n",
      "\n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for test_song in test_midi_data:\n",
    "    print('--------------------------------')\n",
    "    test_song: SongData\n",
    "    lyrics, artist, melody = generate_lyrics(\n",
    "        model_to_use=model,\n",
    "        initial_word=test_song.lyrics[0],\n",
    "        melody_features=test_song.midi_features,\n",
    "        melody_title=test_song.title,\n",
    "        artist_to_use=test_song.artist,\n",
    "        word_to_id=word_to_id,\n",
    "        id_to_word=id_to_word,\n",
    "        artist_to_index=artist_to_index,\n",
    "        word_embeddings=unified_embeddings,\n",
    "        max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "    )\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92433b27",
   "metadata": {},
   "source": [
    "For each melody, the output of the architecture given the melody and different starting words. The same word should be used for all melodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0dea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Generating song from initial word: 'love', melody: 'eternal flame', artist: 'the bangles', max length: 95\n",
      "Love my own \n",
      "And the way is you \n",
      "Oh you got me you \n",
      "I just be i you to be \n",
      "You do to be to know \n",
      "I know you i want you \n",
      "So i dont get \n",
      "I cant want the way you be \n",
      "I dont be you \n",
      "I never be a little to \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'love', melody: 'honesty', artist: 'billy joel', max length: 211\n",
      "Love \n",
      "But i see \n",
      "Im just you be \n",
      "So you want it \n",
      "Cause \n",
      "Its a genie \n",
      "Oh i dont make to love a place \n",
      "And its the man and the \n",
      "And \n",
      "And a place that my own \n",
      "And a dream is a chance \n",
      "Oh you know you love you \n",
      "Im dont make to you say you know \n",
      "You can want to be love \n",
      "But it \n",
      "I got \n",
      "To love i go \n",
      "But i really be you say you \n",
      "And youre you to be your \n",
      "And i do i do \n",
      "I can want \n",
      "Its way in you say you \n",
      "I got the love of my own \n",
      "And you cant get my world of the world \n",
      "But the time \n",
      "The a bootie \n",
      "I be your love \n",
      "Im can \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'love', melody: 'lovefool', artist: 'cardigans', max length: 292\n",
      "Love me \n",
      "I be the love \n",
      "But i dont get to love my way \n",
      "I want to be \n",
      "I know i do \n",
      "And i want to go \n",
      "I know \n",
      "I want to say to know \n",
      "I can want to make a lot day \n",
      "Im a place \n",
      "Its clock \n",
      "And if i have my mind \n",
      "I dont make \n",
      "And i know my way you \n",
      "But that the love is me \n",
      "Its not will i be \n",
      "I cant see me \n",
      "I never know you to be \n",
      "I could want you \n",
      "If you be the world that to be you know \n",
      "Its love and my time to \n",
      "So i can \n",
      "It my love that to be you \n",
      "You got a fool for the love \n",
      "Its the heart \n",
      "And im a la york \n",
      "La \n",
      "Oh \n",
      "Im bop is your eyes \n",
      "I need i be a heartache \n",
      "You want me you to get \n",
      "You dont make it \n",
      "You can have \n",
      "Im the la life \n",
      "Ill never know you i \n",
      "You could want \n",
      "I dont \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'love', melody: 'barbie girl', artist: 'aqua', max length: 390\n",
      "Love \n",
      "They know to make you be your heart \n",
      "I know you \n",
      "But i can have you \n",
      "If i love my heart \n",
      "You dont need your way \n",
      "When you can know what \n",
      "And i know to make \n",
      "And i want you \n",
      "I can be you go and you can say \n",
      "Its you be to have you \n",
      "I dont have your dream \n",
      "And you be to get my way \n",
      "I want a way on my mind \n",
      "I know you \n",
      "If you do you say \n",
      "I want you be a way \n",
      "If the world \n",
      "You never see it \n",
      "I want you love \n",
      "If youre i want a heartache \n",
      "I want be you love \n",
      "I know to love you \n",
      "I can never do \n",
      "And we know me i \n",
      "You be a way \n",
      "Oh i got you \n",
      "I never be to love you \n",
      "I can want to say it \n",
      "I dont never make the love \n",
      "I know i go my eyes \n",
      "But you want i be you \n",
      "But \n",
      "I never want you say to go \n",
      "But you know it i know you \n",
      "And its way i say \n",
      "I want you be the love \n",
      "You can make your life \n",
      "Oh im dont be a heartache of my eyes \n",
      "I never be the love \n",
      "I be your love \n",
      "I never have the way of mine to see \n",
      "I can want to love you \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'love', melody: 'all the small things', artist: 'blink 182', max length: 176\n",
      "Love \n",
      "Oh it the eyes \n",
      "You know my heart \n",
      "Oh i need me my body \n",
      "But the love that in the heart \n",
      "You can want you love me \n",
      "And i cant know \n",
      "You know to be to \n",
      "And my just is you \n",
      "Im the radio is me \n",
      "I can make your bell \n",
      "And i need you say to love \n",
      "I can have my love \n",
      "And a new la minute \n",
      "I want \n",
      "Its world to be \n",
      "I can see your eyes \n",
      "I can get the heart to make to \n",
      "But it you can know \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'baby', melody: 'eternal flame', artist: 'the bangles', max length: 95\n",
      "Baby \n",
      "And we know you go \n",
      "I need i feel \n",
      "But i want i see me \n",
      "You dont say to love \n",
      "And i got to you \n",
      "But to be a love to the \n",
      "And a new la minute \n",
      "I can need me \n",
      "But you do me you be \n",
      "So you really \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'baby', melody: 'honesty', artist: 'billy joel', max length: 211\n",
      "Baby \n",
      "But on my heart is the way of \n",
      "I got to love me my time \n",
      "I can want your mind you \n",
      "But i can see you i be to \n",
      "And i know my way and we know \n",
      "Im gonna need \n",
      "If i really do \n",
      "I can want i love \n",
      "You cant see to love a lot \n",
      "I can be you get \n",
      "Im can see the way \n",
      "Im the limit in all \n",
      "Oh you got a dream \n",
      "Its not \n",
      "I can be my heart \n",
      "Oh you can be my world you be me \n",
      "I can have the world \n",
      "You dont do you do you \n",
      "I want a chance \n",
      "Oh me i see \n",
      "But you \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'baby', melody: 'lovefool', artist: 'cardigans', max length: 292\n",
      "Baby \n",
      "The a man that a song \n",
      "Oh i want to do \n",
      "If you dont see \n",
      "You can see to say \n",
      "I dont make the world \n",
      "I will to know to be it \n",
      "But i need it i can to be \n",
      "So you dont do \n",
      "I know to love \n",
      "Cause youre i want \n",
      "Its only is \n",
      "Its love \n",
      "And the world and i want \n",
      "Im a la minute \n",
      "But oh i dont do \n",
      "When i know \n",
      "And the love for the way \n",
      "You never see your heart \n",
      "And you want the world you \n",
      "I cant see you know you \n",
      "And i love i want you be me \n",
      "You be the world you can \n",
      "I can need you say \n",
      "And i got you love \n",
      "Its just is in a new york \n",
      "Oh a new bit in summer \n",
      "Im got a friend of a man of your mind \n",
      "You dont know a dream \n",
      "I dont make to love \n",
      "I can want \n",
      "But you can \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'baby', melody: 'barbie girl', artist: 'aqua', max length: 390\n",
      "Baby \n",
      "And i can get to make \n",
      "And im dont have to go \n",
      "Its the time of my mind of the \n",
      "And i know me what you dont do \n",
      "But the all is a new \n",
      "Who can have the way \n",
      "I know me you \n",
      "I know to have the same \n",
      "And i cant do you to \n",
      "And we dont have you \n",
      "I know me i be me \n",
      "I know to love you \n",
      "I know you know \n",
      "Oh you can do \n",
      "I dont have you \n",
      "I love \n",
      "I can be your mind and your own \n",
      "I need to be a place on a new way \n",
      "And the a cruel \n",
      "And the la beautiful \n",
      "I know you do to be \n",
      "I want you want it \n",
      "And its the one to know you to get to say \n",
      "And if it to be me to be \n",
      "I dont be the superman \n",
      "Im know you \n",
      "To know a lot to know \n",
      "I never want that \n",
      "And you can want i be my mind \n",
      "But i can need \n",
      "When you know \n",
      "Youre the time i know \n",
      "Im love a place of your mind \n",
      "I know you i know you be \n",
      "I do it to say \n",
      "And you be you be \n",
      "If you can be to know you \n",
      "But i want \n",
      "I can see me a way \n",
      "I \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'baby', melody: 'all the small things', artist: 'blink 182', max length: 176\n",
      "Baby \n",
      "And you be to go and the love in a new york \n",
      "And im got to be \n",
      "Its way \n",
      "When my name \n",
      "The clock you make me \n",
      "I know i feel to go \n",
      "You can know i \n",
      "The love is the mind and my way \n",
      "The only \n",
      "And we know you can do \n",
      "You dont make the life for the way \n",
      "And i have want to do it a time \n",
      "Its love that i can get \n",
      "I never be your heart for the way \n",
      "Its time of in \n",
      "I can want \n",
      "Im \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'time', melody: 'eternal flame', artist: 'the bangles', max length: 95\n",
      "Time \n",
      "Oh i got the same day \n",
      "Im love me a way \n",
      "I got a dream of my heart \n",
      "You need a place in \n",
      "And youre a place \n",
      "I cant know you \n",
      "I never see my superman to be me to love \n",
      "I never get that \n",
      "I can do you \n",
      "\n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'time', melody: 'honesty', artist: 'billy joel', max length: 211\n",
      "Time \n",
      "Oh you have a hero for \n",
      "And i got it you know \n",
      "If i be the heart \n",
      "I dont want \n",
      "But i need it \n",
      "I cant never be to \n",
      "And i can see it you be you \n",
      "I never be \n",
      "Im the love of me the a new world \n",
      "Oh i can need you \n",
      "You i have to be \n",
      "Cause you be me to be \n",
      "You i know the heart \n",
      "I can get to \n",
      "I love you to \n",
      "So you say i say me \n",
      "I got a dream \n",
      "And i can get it to dont \n",
      "You i need me \n",
      "Im you be to love \n",
      "I can want \n",
      "And i never make the love to be \n",
      "And youre you be the time to \n",
      "And the one of \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'time', melody: 'lovefool', artist: 'cardigans', max length: 292\n",
      "Time \n",
      "And the time to be my heart \n",
      "You cant have the same \n",
      "You cant want to say me the time of the heart \n",
      "Its only race \n",
      "The name \n",
      "The time to be \n",
      "I never do \n",
      "I love to love you say \n",
      "I just to be a love \n",
      "The not to love you \n",
      "You can be to you be i \n",
      "I be the love that and its love i know \n",
      "Its the la minute \n",
      "Ill can want you do \n",
      "Im a little day and a place \n",
      "And the time is me \n",
      "I got the way \n",
      "You dont do me \n",
      "You know it \n",
      "I need i be \n",
      "Cause it me you do and youre \n",
      "Its time \n",
      "Its time the world \n",
      "I dont see you do the way \n",
      "I cant want you to \n",
      "You know my time and i be to \n",
      "No a new day on \n",
      "Oh you know to go and the way of a heart \n",
      "I can need you love \n",
      "I want \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'time', melody: 'barbie girl', artist: 'aqua', max length: 390\n",
      "Time me \n",
      "I know i feel my time \n",
      "You got it \n",
      "I dont want to be the love \n",
      "I got it to know a way \n",
      "I can know a friend \n",
      "Ill can want \n",
      "Its love of my life \n",
      "I can know to go you \n",
      "If you want it the world \n",
      "You got to be it \n",
      "Its not \n",
      "I never want you say that you be your \n",
      "That you never want \n",
      "But i be a place in me \n",
      "I dont have i have my heart \n",
      "You can be it \n",
      "I can make me you can be you \n",
      "I never get to \n",
      "I know you \n",
      "I cant know you be my heart \n",
      "I need i feel your eyes \n",
      "You can never know you be \n",
      "You do to love you to love \n",
      "I cant know me \n",
      "I want it to go you \n",
      "I cant want a superman and \n",
      "Im know to make \n",
      "You can be the world of the eyes \n",
      "You got to the \n",
      "But \n",
      "Oh it i feel me \n",
      "I know my mind \n",
      "I got it \n",
      "I never be you do you \n",
      "I know you love you be my love \n",
      "And its way is you be it \n",
      "You dont know you \n",
      "You could know to say \n",
      "You got a dream \n",
      "When i need you \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "Generating song from initial word: 'time', melody: 'all the small things', artist: 'blink 182', max length: 176\n",
      "Time the \n",
      "And a place of in to \n",
      "So \n",
      "The a place and the a place \n",
      "I will have a place of time \n",
      "The time a lot \n",
      "And i want be my mind on a chance \n",
      "But you can have to have to \n",
      "So your eyes \n",
      "I want to get \n",
      "I know to have to be \n",
      "And i be you know that all it \n",
      "I dont make that my heart i you \n",
      "I do to say to go you \n",
      "So my love that i be you say \n",
      "Oh if me \n",
      "Its time of the heart \n",
      "And i can get to make \n",
      "I know the love \n",
      "Oh me \n",
      "And \n",
      "--- End of generated lyrics ---\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "starting_words: list[str] = ['love', 'baby', 'time']\n",
    "\n",
    "for word in starting_words:\n",
    "    for test_song in test_midi_data:\n",
    "        print('--------------------------------')\n",
    "        test_song: SongData\n",
    "        lyrics, artist, melody = generate_lyrics(\n",
    "            model_to_use=model,\n",
    "            initial_word=word,\n",
    "            melody_features=test_song.midi_features,\n",
    "            melody_title=test_song.title,\n",
    "            artist_to_use=test_song.artist,\n",
    "            word_to_id=word_to_id,\n",
    "            id_to_word=id_to_word,\n",
    "            artist_to_index=artist_to_index,\n",
    "            word_embeddings=unified_embeddings,\n",
    "            max_song_length=len([word for word in test_song.lyrics if word != EOL_STRING])\n",
    "        )\n",
    "        print('--------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
